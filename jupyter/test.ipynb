{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# specify which GPU will be used\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pymysql\n",
    "from warnings import filterwarnings\n",
    "\n",
    "_connection = None\n",
    "\n",
    "def get_connection(db_config):\n",
    "    \"\"\"\n",
    "    get db connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        _connection = pymysql.connect(host=db_config['host'], user=db_config['username'],\n",
    "                                      password=db_config['password'],\n",
    "                                      db=db_config['db'], charset=\"utf8\")\n",
    "        filterwarnings('ignore', category=pymysql.Warning)\n",
    "\n",
    "    return _connection\n",
    "\n",
    "\n",
    "def close():\n",
    "    \"\"\"\n",
    "    close DB connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is not None:\n",
    "        _connection.close()\n",
    "    _connection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {\n",
    "    'host': '172.26.187.242',\n",
    "    'username': 'malware_r',\n",
    "    'password': 'GEg22v2O7jbfWhb3',\n",
    "    'db': 'malware'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields\n",
    "\n",
    "- mw_file_suffix: file name after hash value\n",
    "- mw_file_prefix: directory\n",
    "- mw_em_f: features of ember, splitted by \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# the base function which can query sql and return dict data\n",
    "def get_specific_data(table_suffix, sql=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        raise Exception(\"please init db connect first\")\n",
    "\n",
    "    cursor = _connection.cursor()\n",
    "    cursor.execute(\"SET NAMES utf8mb4\")\n",
    "\n",
    "    ret = []\n",
    "        \n",
    "    cursor.execute(sql)\n",
    "\n",
    "    field_names = [i[0] for i in cursor.description]\n",
    "\n",
    "    for row in cursor:\n",
    "        temp = {}\n",
    "        for key in range(len(row)):\n",
    "            temp[field_names[key]] = row[key]\n",
    "        ret.append(temp)\n",
    "     \n",
    "    cursor.close()\n",
    "    # _connection.close()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5.231847524642944 seconds ---\n",
      "--- 4.660666227340698 seconds ---\n",
      "--- 3.9280669689178467 seconds ---\n",
      "--- 2.8787410259246826 seconds ---\n",
      "--- 3.6401829719543457 seconds ---\n",
      "--- 3.8073761463165283 seconds ---\n",
      "--- 3.7070224285125732 seconds ---\n",
      "--- 3.7544069290161133 seconds ---\n",
      "--- 3.7250242233276367 seconds ---\n",
      "--- 3.0903587341308594 seconds ---\n",
      "--- 3.022162675857544 seconds ---\n",
      "--- 3.583305835723877 seconds ---\n",
      "--- 3.7613255977630615 seconds ---\n",
      "--- 3.684119462966919 seconds ---\n",
      "--- 3.906905174255371 seconds ---\n",
      "--- 3.4077231884002686 seconds ---\n",
      "162542\n"
     ]
    }
   ],
   "source": [
    "close()\n",
    "res1 = []\n",
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "# table_suffix = [\"0\",\"3\",\"9\",\"A\",\"F\"]\n",
    "# Iterate all partitions of databases\n",
    "for suffix in table_suffix:\n",
    "    sql = \"\"\" \n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  a.section_name,\n",
    "  c.mw_file_suffix as mw_file_size,\n",
    "  c.mw_file_prefix as mw_file_directory,\n",
    "  c.mw_num_engines,\n",
    "  a.pointerto_raw_data,\n",
    "  a.virtual_size,\n",
    "  d.mw_em_f\n",
    "from mw_index_2017_section_%s as a\n",
    "  inner join mw_index_2017_%s c on a.mw_file_hash = c.mw_file_hash\n",
    "  inner join mw_index_2017_feature_%s d on a.mw_file_hash = d.mw_file_hash\n",
    "where (CNT_CODE = 1 or MEM_EXECUTE = 1) and c.mw_num_engines <> -1 and (c.mw_num_engines >= 4 or c.mw_num_engines = 0) and\n",
    "      c.mw_file_prefix in ('201704')\n",
    "    \"\"\" % (suffix, suffix, suffix)\n",
    "    res1.extend(get_specific_data(suffix, sql))\n",
    "close()\n",
    "print(len(res1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.953291416168213 seconds ---\n",
      "--- 4.301469802856445 seconds ---\n",
      "--- 4.776055097579956 seconds ---\n",
      "--- 4.221483469009399 seconds ---\n",
      "--- 4.561415433883667 seconds ---\n",
      "--- 4.487716913223267 seconds ---\n",
      "--- 4.43102765083313 seconds ---\n",
      "--- 4.561986207962036 seconds ---\n",
      "--- 4.231433868408203 seconds ---\n",
      "--- 4.492696046829224 seconds ---\n",
      "--- 4.55763053894043 seconds ---\n",
      "--- 4.370911359786987 seconds ---\n",
      "--- 3.9771170616149902 seconds ---\n",
      "--- 4.1825315952301025 seconds ---\n",
      "--- 4.036640882492065 seconds ---\n",
      "--- 4.036452054977417 seconds ---\n",
      "187577\n"
     ]
    }
   ],
   "source": [
    "close()\n",
    "res2 = []\n",
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "# table_suffix = [\"0\",\"3\",\"9\",\"A\",\"F\"]\n",
    "# Iterate all partitions of databases\n",
    "for suffix in table_suffix:\n",
    "    sql = \"\"\" \n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  a.section_name,\n",
    "  c.mw_file_suffix as mw_file_size,\n",
    "  c.mw_file_prefix as mw_file_directory,\n",
    "  c.mw_num_engines,\n",
    "  a.pointerto_raw_data,\n",
    "  a.virtual_size,\n",
    "  d.mw_em_f\n",
    "from mw_index_2017_section_%s as a\n",
    "  inner join mw_index_2017_%s c on a.mw_file_hash = c.mw_file_hash\n",
    "  inner join mw_index_2017_feature_%s d on a.mw_file_hash = d.mw_file_hash\n",
    "where (CNT_CODE = 1 or MEM_EXECUTE = 1) and c.mw_num_engines <> -1 and (c.mw_num_engines >= 4 or c.mw_num_engines = 0) and\n",
    "      c.mw_file_prefix in ('201705')\n",
    "    \"\"\" % (suffix, suffix, suffix)\n",
    "    res2.extend(get_specific_data(suffix, sql))\n",
    "close()\n",
    "print(len(res2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_length = 10000\n",
    "\n",
    "train_data = pd.DataFrame(res1)\n",
    "# train_data = train_data.loc[train_data.virtual_size <= max_length]\n",
    "# train_data = train_data.reset_index(drop=True)\n",
    "train_data.mw_num_engines[train_data.mw_num_engines == 0 ] = 0\n",
    "train_data.mw_num_engines[train_data.mw_num_engines >= 4 ] = 1\n",
    "train_label = train_data.mw_num_engines.ravel()\n",
    "\n",
    "test_data = pd.DataFrame(res2)\n",
    "# test_data = test_data.loc[test_data.virtual_size <= max_length]\n",
    "# test_data = test_data.reset_index(drop=True)\n",
    "test_data.mw_num_engines[test_data.mw_num_engines == 0 ] = 0\n",
    "test_data.mw_num_engines[test_data.mw_num_engines >= 4 ] = 1\n",
    "test_label = test_data.mw_num_engines.ravel()\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data, train_label, test_size=0.1, random_state=2345)\n",
    "x_test = test_data\n",
    "y_test = test_label\n",
    "\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "x_val = x_val.reset_index(drop=True)\n",
    "x_test = x_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model(y_pred, test_y):\n",
    "    \n",
    "    loss = log_loss(test_y, y_pred)\n",
    "    auc = roc_auc_score(test_y, y_pred)\n",
    "    acc = accuracy_score(test_y, (y_pred > 0.5).astype(int))\n",
    "    print(\"loss : %.5f\" % loss)\n",
    "    print(\"auc score : %.5f\" % auc)\n",
    "    print(\"accuracy score : %.5f\" % acc)\n",
    "\n",
    "    fp_np_index = np.where(test_y == 0)\n",
    "    fp_np = y_pred[fp_np_index].shape[0]\n",
    "    thre_index = int(np.ceil(fp_np - fp_np * 0.001))\n",
    "\n",
    "    sorted_pred_prob = np.sort(y_pred[fp_np_index], axis=0)\n",
    "    thre = sorted_pred_prob[thre_index]\n",
    "    if thre == 1:\n",
    "        thre = max(sorted_pred_prob[np.where(sorted_pred_prob != 1)])\n",
    "\n",
    "    y_pred_prob = np.vstack((y_pred.transpose(), (1 - y_pred).transpose())).transpose()\n",
    "    y_pred_prob[:, 1] = thre\n",
    "    y_pred_label = np.argmin(y_pred_prob, axis=-1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(test_y, y_pred_label).ravel()\n",
    "    fp_rate = fp / (fp + tn)\n",
    "    recall_rate = tp / (tp + fn)\n",
    "\n",
    "    print(\"thre: %.10f\"%  thre)\n",
    "    print(\"fp:  %.10f\"%  fp_rate)\n",
    "    print(\"recall:  %.10f\"%  recall_rate)\n",
    "    \n",
    "    return auc, loss, recall_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "\n",
    "    def __init__(self, list_IDs, datasets, batch_size=32, dim=10000, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.datasets = datasets\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, Y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size, self.dim), dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            base_path = \"/ssd/2017/{0}/{1}{2}\"\n",
    "            item = self.datasets.loc[ID]\n",
    "            file_path = base_path.format(item[\"mw_file_directory\"], item[\"mw_file_hash\"], item[\"mw_file_size\"])\n",
    "            in_file = open(file_path, 'rb')\n",
    "            in_file.seek(item['pointerto_raw_data'])\n",
    "            if item['virtual_size'] > self.dim:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(self.dim)]\n",
    "            else:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(item['virtual_size'])]\n",
    "            X[i, 0:len(bytes_data)] = bytes_data\n",
    "\n",
    "#         X = X.reshape((-1, 100, 100, 1)) / 255.0\n",
    "        T = X.reshape((-1, 10000)) / 255.0\n",
    "        Y = X.reshape((-1, 10000, 1)) / 255.0\n",
    "        return T, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_23 (Embedding)     (None, 10000, 8)          2048      \n",
      "_________________________________________________________________\n",
      "activation_241 (Activation)  (None, 10000, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_509 (Conv1D)          (None, 10000, 32)         1312      \n",
      "_________________________________________________________________\n",
      "conv1d_510 (Conv1D)          (None, 10000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "activation_242 (Activation)  (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_511 (Conv1D)          (None, 2000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_512 (Conv1D)          (None, 2000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "activation_243 (Activation)  (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_513 (Conv1D)          (None, 1000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_514 (Conv1D)          (None, 1000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_244 (Activation)  (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_515 (Conv1D)          (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "conv1d_516 (Conv1D)          (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_245 (Activation)  (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_517 (Conv1D)          (None, 250, 32)           3104      \n",
      "_________________________________________________________________\n",
      "conv1d_518 (Conv1D)          (None, 250, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 50, 2)             66        \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 50, 2)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_519 (Conv1D)          (None, 50, 32)            224       \n",
      "_________________________________________________________________\n",
      "conv1d_520 (Conv1D)          (None, 50, 32)            3104      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_113 (UpSamplin (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_247 (Activation)  (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_521 (Conv1D)          (None, 250, 32)           3104      \n",
      "_________________________________________________________________\n",
      "conv1d_522 (Conv1D)          (None, 250, 32)           3104      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_114 (UpSamplin (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_523 (Conv1D)          (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "conv1d_524 (Conv1D)          (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_115 (UpSamplin (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "activation_249 (Activation)  (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_525 (Conv1D)          (None, 1000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_526 (Conv1D)          (None, 1000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_116 (UpSamplin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "activation_250 (Activation)  (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_527 (Conv1D)          (None, 2000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "conv1d_528 (Conv1D)          (None, 2000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_117 (UpSamplin (None, 10000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_529 (Conv1D)          (None, 10000, 1)          97        \n",
      "=================================================================\n",
      "Total params: 65,763\n",
      "Trainable params: 65,763\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Length of the train:  138972\n",
      "Length of the validation:  7315\n",
      "Epoch 1/64\n",
      "8685/8685 [==============================] - 852s 98ms/step - loss: 0.0840 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08031, saving model to /home/zhaoqi/autoencoder/models/1533489116.5710807-0001-0.08031_12_3_a_0.h5\n",
      "Epoch 2/64\n",
      "8685/8685 [==============================] - 873s 100ms/step - loss: 0.0791 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08031 to 0.07885, saving model to /home/zhaoqi/autoencoder/models/1533489116.5710807-0002-0.07885_12_3_a_0.h5\n",
      "Epoch 3/64\n",
      "8685/8685 [==============================] - 880s 101ms/step - loss: 0.0778 - val_loss: 0.0777\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.07885 to 0.07769, saving model to /home/zhaoqi/autoencoder/models/1533489116.5710807-0003-0.07769_12_3_a_0.h5\n",
      "Epoch 4/64\n",
      "8685/8685 [==============================] - 874s 101ms/step - loss: 0.0772 - val_loss: 0.0771\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.07769 to 0.07708, saving model to /home/zhaoqi/autoencoder/models/1533489116.5710807-0004-0.07708_12_3_a_0.h5\n",
      "Epoch 5/64\n",
      "8685/8685 [==============================] - 869s 100ms/step - loss: 0.0767 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07708\n",
      "Epoch 6/64\n",
      "8685/8685 [==============================] - 876s 101ms/step - loss: 0.0764 - val_loss: 0.0771\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.07708 to 0.07707, saving model to /home/zhaoqi/autoencoder/models/1533489116.5710807-0006-0.07707_12_3_a_0.h5\n",
      "Epoch 7/64\n",
      "8685/8685 [==============================] - 865s 100ms/step - loss: 0.0761 - val_loss: 0.0762\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07707 to 0.07620, saving model to /home/zhaoqi/autoencoder/models/1533489116.5710807-0007-0.07620_12_3_a_0.h5\n",
      "Epoch 8/64\n",
      "8685/8685 [==============================] - 885s 102ms/step - loss: 0.0759 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07620\n",
      "Epoch 9/64\n",
      "8685/8685 [==============================] - 874s 101ms/step - loss: 0.0757 - val_loss: 0.0763\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07620\n",
      "Epoch 10/64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8685/8685 [==============================] - 882s 102ms/step - loss: 0.0756 - val_loss: 0.0762\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07620\n",
      "Epoch 11/64\n",
      "8685/8685 [==============================] - 877s 101ms/step - loss: 0.0755 - val_loss: 0.0758\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.07620 to 0.07584, saving model to /home/zhaoqi/autoencoder/models/1533489116.5710807-0011-0.07584_12_3_a_0.h5\n",
      "Epoch 12/64\n",
      "8685/8685 [==============================] - 880s 101ms/step - loss: 0.0754 - val_loss: 0.0761\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07584\n",
      "Epoch 13/64\n",
      "8685/8685 [==============================] - 876s 101ms/step - loss: 0.0753 - val_loss: 0.0765\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.07584\n",
      "Epoch 14/64\n",
      "8685/8685 [==============================] - 865s 100ms/step - loss: 0.0752 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.07584\n",
      "Epoch 15/64\n",
      "8685/8685 [==============================] - 866s 100ms/step - loss: 0.0751 - val_loss: 0.0759\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.07584\n",
      "Epoch 16/64\n",
      "8685/8685 [==============================] - 889s 102ms/step - loss: 0.0751 - val_loss: 0.0759\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.07584\n",
      "Epoch 17/64\n",
      "8685/8685 [==============================] - 872s 100ms/step - loss: 0.0750 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.07584\n",
      "Epoch 18/64\n",
      "8685/8685 [==============================] - 874s 101ms/step - loss: 0.0750 - val_loss: 0.0764\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.07584\n",
      "Epoch 19/64\n",
      "8685/8685 [==============================] - 870s 100ms/step - loss: 0.0749 - val_loss: 0.0758\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.07584 to 0.07576, saving model to /home/zhaoqi/autoencoder/models/1533489116.5710807-0019-0.07576_12_3_a_0.h5\n",
      "Epoch 20/64\n",
      "8685/8685 [==============================] - 877s 101ms/step - loss: 0.0749 - val_loss: 0.0754\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.07576 to 0.07536, saving model to /home/zhaoqi/autoencoder/models/1533489116.5710807-0020-0.07536_12_3_a_0.h5\n",
      "Epoch 21/64\n",
      "8685/8685 [==============================] - 852s 98ms/step - loss: 0.0749 - val_loss: 0.0757\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.07536\n",
      "Epoch 22/64\n",
      "8685/8685 [==============================] - 889s 102ms/step - loss: 0.0749 - val_loss: 0.0755\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.07536\n",
      "Epoch 23/64\n",
      "8685/8685 [==============================] - 874s 101ms/step - loss: 0.0749 - val_loss: 0.0758\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.07536\n",
      "Epoch 24/64\n",
      "8685/8685 [==============================] - 871s 100ms/step - loss: 0.0749 - val_loss: 0.0772\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.07536\n",
      "Epoch 25/64\n",
      "8685/8685 [==============================] - 871s 100ms/step - loss: 0.0749 - val_loss: 0.0754\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.07536\n",
      "Epoch 26/64\n",
      "8685/8685 [==============================] - 864s 99ms/step - loss: 0.0748 - val_loss: 0.0755\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.07536\n",
      "Epoch 27/64\n",
      "8685/8685 [==============================] - 867s 100ms/step - loss: 0.0748 - val_loss: 0.0754\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.07536\n",
      "Epoch 28/64\n",
      "8685/8685 [==============================] - 877s 101ms/step - loss: 0.0748 - val_loss: 0.0756\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.07536\n",
      "Epoch 29/64\n",
      "8685/8685 [==============================] - 875s 101ms/step - loss: 0.0747 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.07536\n",
      "Epoch 30/64\n",
      "8685/8685 [==============================] - 876s 101ms/step - loss: 0.0747 - val_loss: 0.0758\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.07536\n",
      "Epoch 31/64\n",
      "8685/8685 [==============================] - 873s 101ms/step - loss: 0.0747 - val_loss: 0.0755\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.07536\n",
      "Epoch 32/64\n",
      "8685/8685 [==============================] - 881s 101ms/step - loss: 0.0747 - val_loss: 0.0750\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.07536 to 0.07503, saving model to /home/zhaoqi/autoencoder/models/1533489116.5710807-0032-0.07503_12_3_a_0.h5\n",
      "Epoch 33/64\n",
      "8685/8685 [==============================] - 905s 104ms/step - loss: 0.0747 - val_loss: 0.0759\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.07503\n",
      "Epoch 34/64\n",
      "8685/8685 [==============================] - 903s 104ms/step - loss: 0.0747 - val_loss: 0.0754\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.07503\n",
      "Epoch 35/64\n",
      "8685/8685 [==============================] - 908s 105ms/step - loss: 0.0747 - val_loss: 0.0754\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.07503\n",
      "Epoch 36/64\n",
      "6412/8685 [=====================>........] - ETA: 3:30 - loss: 0.0746"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-879:\n",
      "Process ForkPoolWorker-882:\n",
      "Process ForkPoolWorker-880:\n",
      "Process ForkPoolWorker-878:\n",
      "Process ForkPoolWorker-883:\n",
      "Process ForkPoolWorker-881:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-63-78187bda1bf7>\", line 28, in __getitem__\n",
      "    X, Y = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-63-78187bda1bf7>\", line 51, in __data_generation\n",
      "    bytes_data = [int(single_byte) for single_byte in in_file.read(self.dim)]\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-e74c4444cb9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoencoder.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoder.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-e74c4444cb9c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_df, max_epoch, batch_size)\u001b[0m\n\u001b[1;32m    129\u001b[0m                                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                                        \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                                        callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoencoder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2667\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2649\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras import Input\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, Conv1D, Conv2D, Multiply, GlobalMaxPooling1D, Dropout, Activation\n",
    "from keras.layers import UpSampling2D, Flatten, merge, MaxPooling2D, MaxPooling1D, UpSampling1D\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import merge, Dropout, BatchNormalization, Maximum, Add\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Autoencoder():\n",
    "    def __init__(self, autoencoder_name, encoder_name):\n",
    "        self.autoencoder_name = autoencoder_name\n",
    "        self.encoder_name = encoder_name\n",
    "        self.autoencoder = None\n",
    "        self.encoder = None\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def get_model(self):\n",
    "#         input_img = Input(shape=(100, 100, 1))\n",
    "# Deep\n",
    "        input_sequence = Input(shape=(10000,));\n",
    "    \n",
    "        x = Embedding(256, 8, input_length=10000)(input_sequence)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv1D(32, 5, padding='same')(x)\n",
    "        x = Conv1D(32, 5, padding='causal', dilation_rate=1)(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv1D(32, 3, padding='same')(x)\n",
    "        x = Conv1D(32, 3, padding='causal', dilation_rate=2)(x)\n",
    "        x = MaxPooling1D(2, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv1D(32, 3, padding='same')(x)\n",
    "        x = Conv1D(32, 3, padding='causal', dilation_rate=4)(x)\n",
    "        x = MaxPooling1D(2, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv1D(32, 3, padding='same')(x)\n",
    "        x = Conv1D(32, 3, padding='causal', dilation_rate=8)(x)\n",
    "        x = MaxPooling1D(2, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv1D(32, 3, padding='same')(x)\n",
    "        x = Conv1D(32, 3, padding='causal', dilation_rate=16)(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        encoded = Dense(2, activation='sigmoid')(x)\n",
    "        \n",
    "        # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "        x = Activation('relu')(encoded)\n",
    "        x = Conv1D(32, 3, padding='same')(x)\n",
    "        x = Conv1D(32, 3, padding='causal', dilation_rate=16)(x)\n",
    "        x = UpSampling1D(5)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv1D(32, 3, padding='same')(x)\n",
    "        x = Conv1D(32, 3, padding='causal', dilation_rate=8)(x)\n",
    "        x = UpSampling1D(2)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv1D(32, 3, padding='same')(x)\n",
    "        x = Conv1D(32, 3, padding='causal', dilation_rate=4)(x)\n",
    "        x = UpSampling1D(2)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv1D(32, 3, padding='same')(x)\n",
    "        x = Conv1D(32, 3, padding='causal', dilation_rate=2)(x)\n",
    "        x = UpSampling1D(2)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv1D(32, 5, padding='same')(x)\n",
    "        x = Conv1D(32, 5, padding='causal', dilation_rate=1)(x)\n",
    "        x = UpSampling1D(5)(x)\n",
    "        decoded = Conv1D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "        \n",
    "## Deep\n",
    "#         x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "#         x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "#         x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "#         x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "#         x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "#         x = MaxPooling2D((5, 5), padding='same')(x)\n",
    "#         encoded = Dense(2, activation='sigmoid')(x)\n",
    "        \n",
    "#         # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "#         x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
    "#         x = UpSampling2D((5, 5))(x)\n",
    "#         x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "#         x = UpSampling2D((2, 2))(x)\n",
    "#         x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "#         x = UpSampling2D((2, 2))(x)\n",
    "#         decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "## Base\n",
    "#         x = Conv2D(32, 4, activation='relu', padding='same')(input_img)\n",
    "#         x = MaxPooling2D(25, padding='same')(x)\n",
    "#         encoded = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "#         x = Conv2D(32, 4, activation='relu', padding='same')(encoded)\n",
    "#         x = UpSampling2D(25)(x)\n",
    "#         decoded = Conv2D(1, 4, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "        self.autoencoder = Model(inputs=input_sequence, outputs=decoded)\n",
    "        self.encoder = Model(inputs=input_sequence, outputs=encoded)\n",
    "\n",
    "        self.autoencoder.compile(loss='mean_squared_error', optimizer=RMSprop())\n",
    "#         self.autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "        self.autoencoder.summary()\n",
    "\n",
    "    def train(self, train_df, max_epoch, batch_size=32):\n",
    "        self.get_model()\n",
    "        partition_train, partition_validation = train_test_split(range(len(train_df)), test_size=0.05,\n",
    "                                                                 random_state=1234)\n",
    "        print('Length of the train: ', len(partition_train))\n",
    "        print('Length of the validation: ', len(partition_validation))\n",
    "\n",
    "        #         tensor_board = TensorBoard(log_dir='./logs/', batch_size=batch_size)\n",
    "        file_path = \"/home/zhaoqi/autoencoder/models/\"+ str(self.start_time) +\"-{epoch:04d}-{val_loss:.5f}_12_3_a_0.h5\"\n",
    "        #         early_stopping = EarlyStopping(\"val_loss\", patience=2, verbose=0, mode='auto')\n",
    "        check_point = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "        callbacks_list = [check_point]\n",
    "        \n",
    "        # Generators\n",
    "        training_generator = DataGenerator(partition_train, train_df, batch_size)\n",
    "        validation_generator = DataGenerator(partition_validation, train_df, batch_size)\n",
    "\n",
    "        self.autoencoder.fit_generator(generator=training_generator,\n",
    "                                       validation_data=validation_generator,\n",
    "                                       use_multiprocessing=True,\n",
    "                                       epochs=max_epoch,\n",
    "                                       workers=6,\n",
    "                                       callbacks=callbacks_list)\n",
    "        self.autoencoder.save(self.autoencoder_name)\n",
    "        self.encoder.save(self.encoder_name)\n",
    "        \n",
    "        \n",
    "autoencoder = Autoencoder('autoencoder.h5', 'encoder.h5')\n",
    "autoencoder.train(x_train, max_epoch=64, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model_dir = '/home/zhaoqi/autoencoder/models/'\n",
    "f_name = '1533522697.911087-0057-0.06419_12_3_a_0.h5'\n",
    "c_model = load_model(model_dir + f_name)\n",
    "\n",
    "model_f = Model(c_model.input, c_model.layers[-21].output)\n",
    "\n",
    "model_f.summary()\n",
    "\n",
    "train_generator = DataGenerator(range(len(x_train)), x_train, 16, max_length, False)\n",
    "malcon_train_x = model_f.predict_generator(generator=train_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)\n",
    "\n",
    "val_generator = DataGenerator(range(len(x_val)), x_val, 16, max_length, False)\n",
    "malcon_val_x = model_f.predict_generator(generator=val_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)\n",
    "\n",
    "test_generator = DataGenerator(range(len(x_test)), x_test, 16, max_length, False)\n",
    "malcon_test_x = model_f.predict_generator(generator=test_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "malcon_train_x = malcon_train_x.reshape(len(malcon_train_x), -1)\n",
    "malcon_val_x = malcon_val_x.reshape(len(malcon_val_x), -1)\n",
    "malcon_test_x = malcon_test_x.reshape(len(malcon_test_x), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ember_feature(data):\n",
    "    ember_f = np.zeros((len(data.mw_em_f), 2351), dtype=float)\n",
    "    for index, item in data.iterrows():\n",
    "        ember_f[index, :] = item['mw_em_f'].split(';')\n",
    "    return ember_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_feature(origin_data, feature_data):\n",
    "    feature_data = pd.DataFrame(feature_data)\n",
    "    feature_data['mw_file_hash'] = origin_data.iloc[0:len(feature_data)][['mw_file_hash']]\n",
    "    feature_data = feature_data.groupby('mw_file_hash').max().merge(origin_data[['mw_file_hash','mw_em_f','mw_num_engines']], how='inner', on='mw_file_hash')\n",
    "    tmp_ember = get_ember_feature(feature_data[['mw_em_f']])\n",
    "    tmp_label = feature_data[['mw_num_engines']].mw_num_engines\n",
    "    tmp_data = feature_data.drop(['mw_num_engines', 'mw_em_f', 'mw_file_hash'], axis=1)\n",
    "    return tmp_data, tmp_label, tmp_ember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-cc5a3365d5af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_train_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_train_ember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_max_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmalcon_train_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_val_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val_ember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_max_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmalcon_val_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmax_test_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_test_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_test_ember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_max_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmalcon_test_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-e1af605def11>\u001b[0m in \u001b[0;36mget_max_feature\u001b[0;34m(origin_data, feature_data)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfeature_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfeature_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mw_file_hash'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morigin_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mw_file_hash'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfeature_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mw_file_hash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mw_file_hash'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mw_em_f'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mw_num_engines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mw_file_hash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtmp_ember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ember_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mw_em_f'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtmp_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mw_num_engines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmw_num_engines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2727\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take\u001b[0;34m(self, indices, axis, is_copy)\u001b[0m\n\u001b[1;32m   2787\u001b[0m         new_data = self._data.take(indices,\n\u001b[1;32m   2788\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2789\u001b[0;31m                                    verify=True)\n\u001b[0m\u001b[1;32m   2790\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   4537\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4538\u001b[0m         return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\n\u001b[0;32m-> 4539\u001b[0;31m                                     axis=axis, allow_dups=True)\n\u001b[0m\u001b[1;32m   4540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   4419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4420\u001b[0m             new_blocks = self._slice_take_blocks_ax0(indexer,\n\u001b[0;32m-> 4421\u001b[0;31m                                                      fill_tuple=(fill_value,))\n\u001b[0m\u001b[1;32m   4422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4423\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_tuple)\u001b[0m\n\u001b[1;32m   4499\u001b[0m                     blocks.append(blk.take_nd(blklocs[mgr_locs.indexer],\n\u001b[1;32m   4500\u001b[0m                                               \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4501\u001b[0;31m                                               fill_tuple=None))\n\u001b[0m\u001b[1;32m   4502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4503\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             new_values = algos.take_nd(values, indexer, axis=axis,\n\u001b[0;32m-> 1254\u001b[0;31m                                        allow_fill=False)\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n\u001b[1;32m   1658\u001b[0m                                  mask_info=mask_info)\n\u001b[0;32m-> 1659\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_train_data, max_train_label, max_train_ember = get_max_feature(x_train, malcon_train_x)\n",
    "max_val_data, max_val_label, max_val_ember = get_max_feature(x_val, malcon_val_x)\n",
    "max_test_data, max_test_label, max_test_ember = get_max_feature(x_test, malcon_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_feature(m_data, e_data):\n",
    "    num = len(m_data)\n",
    "    m_x = np.zeros((num, 50+2351), dtype=float)\n",
    "    \n",
    "    for index in range(num):\n",
    "        m_x[index, 0:50] = m_data.iloc[index]\n",
    "        m_x[index, 50:50+2351] = e_data[index]  \n",
    "    return m_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_train_x = merge_feature(max_train_data, max_train_ember)\n",
    "merge_val_x = merge_feature(max_val_data, max_val_ember)\n",
    "merge_test_x = merge_feature(max_test_data, max_test_ember)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(x_train, y_train, x_val, y_val):\n",
    "    params = {'application': 'binary'}\n",
    "    lgbm_dataset = lgb.Dataset(x_train, y_train.ravel())\n",
    "    valid_sets = lgb.Dataset(x_val, y_val.ravel())\n",
    "\n",
    "    model = lgb.train(params, lgbm_dataset, valid_sets=valid_sets, num_boost_round=10000, early_stopping_rounds=10)\n",
    "    y_pred = model.predict(x_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    loss = log_loss(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    acc = accuracy_score(y_val, (y_pred > 0.5).astype(int))\n",
    "#     model.save_model(file_path + \"-%04d-%.5f-%.5f.h5\" % (model.best_iteration, loss, acc),\n",
    "#                      num_iteration=model.best_iteration)\n",
    "    print(\"val loss : %.5f\" % loss)\n",
    "    print(\"auc score : %.5f\" % auc)\n",
    "    print(\"accuracy score : %.5f\" % acc)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.581733\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.518383\n",
      "[3]\tvalid_0's binary_logloss: 0.467749\n",
      "[4]\tvalid_0's binary_logloss: 0.422848\n",
      "[5]\tvalid_0's binary_logloss: 0.385547\n",
      "[6]\tvalid_0's binary_logloss: 0.352819\n",
      "[7]\tvalid_0's binary_logloss: 0.323956\n",
      "[8]\tvalid_0's binary_logloss: 0.298528\n",
      "[9]\tvalid_0's binary_logloss: 0.276523\n",
      "[10]\tvalid_0's binary_logloss: 0.256983\n",
      "[11]\tvalid_0's binary_logloss: 0.238856\n",
      "[12]\tvalid_0's binary_logloss: 0.222173\n",
      "[13]\tvalid_0's binary_logloss: 0.207248\n",
      "[14]\tvalid_0's binary_logloss: 0.194787\n",
      "[15]\tvalid_0's binary_logloss: 0.18286\n",
      "[16]\tvalid_0's binary_logloss: 0.173031\n",
      "[17]\tvalid_0's binary_logloss: 0.162902\n",
      "[18]\tvalid_0's binary_logloss: 0.154537\n",
      "[19]\tvalid_0's binary_logloss: 0.146415\n",
      "[20]\tvalid_0's binary_logloss: 0.139501\n",
      "[21]\tvalid_0's binary_logloss: 0.13338\n",
      "[22]\tvalid_0's binary_logloss: 0.127167\n",
      "[23]\tvalid_0's binary_logloss: 0.121614\n",
      "[24]\tvalid_0's binary_logloss: 0.11661\n",
      "[25]\tvalid_0's binary_logloss: 0.111962\n",
      "[26]\tvalid_0's binary_logloss: 0.107996\n",
      "[27]\tvalid_0's binary_logloss: 0.104339\n",
      "[28]\tvalid_0's binary_logloss: 0.100802\n",
      "[29]\tvalid_0's binary_logloss: 0.0974463\n",
      "[30]\tvalid_0's binary_logloss: 0.0941432\n",
      "[31]\tvalid_0's binary_logloss: 0.0913011\n",
      "[32]\tvalid_0's binary_logloss: 0.0885969\n",
      "[33]\tvalid_0's binary_logloss: 0.0858998\n",
      "[34]\tvalid_0's binary_logloss: 0.0828781\n",
      "[35]\tvalid_0's binary_logloss: 0.0804512\n",
      "[36]\tvalid_0's binary_logloss: 0.0783238\n",
      "[37]\tvalid_0's binary_logloss: 0.0762941\n",
      "[38]\tvalid_0's binary_logloss: 0.0746251\n",
      "[39]\tvalid_0's binary_logloss: 0.0727337\n",
      "[40]\tvalid_0's binary_logloss: 0.0707625\n",
      "[41]\tvalid_0's binary_logloss: 0.0693965\n",
      "[42]\tvalid_0's binary_logloss: 0.0678231\n",
      "[43]\tvalid_0's binary_logloss: 0.0662631\n",
      "[44]\tvalid_0's binary_logloss: 0.0649246\n",
      "[45]\tvalid_0's binary_logloss: 0.0635847\n",
      "[46]\tvalid_0's binary_logloss: 0.0621167\n",
      "[47]\tvalid_0's binary_logloss: 0.0610405\n",
      "[48]\tvalid_0's binary_logloss: 0.0600173\n",
      "[49]\tvalid_0's binary_logloss: 0.0589063\n",
      "[50]\tvalid_0's binary_logloss: 0.0577078\n",
      "[51]\tvalid_0's binary_logloss: 0.0566384\n",
      "[52]\tvalid_0's binary_logloss: 0.0556298\n",
      "[53]\tvalid_0's binary_logloss: 0.0548401\n",
      "[54]\tvalid_0's binary_logloss: 0.0539521\n",
      "[55]\tvalid_0's binary_logloss: 0.0529678\n",
      "[56]\tvalid_0's binary_logloss: 0.0522548\n",
      "[57]\tvalid_0's binary_logloss: 0.0515894\n",
      "[58]\tvalid_0's binary_logloss: 0.0509252\n",
      "[59]\tvalid_0's binary_logloss: 0.0503181\n",
      "[60]\tvalid_0's binary_logloss: 0.0497855\n",
      "[61]\tvalid_0's binary_logloss: 0.0491481\n",
      "[62]\tvalid_0's binary_logloss: 0.0484857\n",
      "[63]\tvalid_0's binary_logloss: 0.0478124\n",
      "[64]\tvalid_0's binary_logloss: 0.0472074\n",
      "[65]\tvalid_0's binary_logloss: 0.0466371\n",
      "[66]\tvalid_0's binary_logloss: 0.0461647\n",
      "[67]\tvalid_0's binary_logloss: 0.0458418\n",
      "[68]\tvalid_0's binary_logloss: 0.0452753\n",
      "[69]\tvalid_0's binary_logloss: 0.0448492\n",
      "[70]\tvalid_0's binary_logloss: 0.0442809\n",
      "[71]\tvalid_0's binary_logloss: 0.0436877\n",
      "[72]\tvalid_0's binary_logloss: 0.0431509\n",
      "[73]\tvalid_0's binary_logloss: 0.0427395\n",
      "[74]\tvalid_0's binary_logloss: 0.0422983\n",
      "[75]\tvalid_0's binary_logloss: 0.0417787\n",
      "[76]\tvalid_0's binary_logloss: 0.0412807\n",
      "[77]\tvalid_0's binary_logloss: 0.0408346\n",
      "[78]\tvalid_0's binary_logloss: 0.0403915\n",
      "[79]\tvalid_0's binary_logloss: 0.0400725\n",
      "[80]\tvalid_0's binary_logloss: 0.0396768\n",
      "[81]\tvalid_0's binary_logloss: 0.039286\n",
      "[82]\tvalid_0's binary_logloss: 0.0388157\n",
      "[83]\tvalid_0's binary_logloss: 0.0383647\n",
      "[84]\tvalid_0's binary_logloss: 0.0380724\n",
      "[85]\tvalid_0's binary_logloss: 0.0377412\n",
      "[86]\tvalid_0's binary_logloss: 0.0374243\n",
      "[87]\tvalid_0's binary_logloss: 0.0370402\n",
      "[88]\tvalid_0's binary_logloss: 0.0365799\n",
      "[89]\tvalid_0's binary_logloss: 0.0363963\n",
      "[90]\tvalid_0's binary_logloss: 0.0360838\n",
      "[91]\tvalid_0's binary_logloss: 0.0357426\n",
      "[92]\tvalid_0's binary_logloss: 0.0353618\n",
      "[93]\tvalid_0's binary_logloss: 0.0351087\n",
      "[94]\tvalid_0's binary_logloss: 0.0348898\n",
      "[95]\tvalid_0's binary_logloss: 0.0346858\n",
      "[96]\tvalid_0's binary_logloss: 0.0344286\n",
      "[97]\tvalid_0's binary_logloss: 0.0341512\n",
      "[98]\tvalid_0's binary_logloss: 0.0340179\n",
      "[99]\tvalid_0's binary_logloss: 0.0338406\n",
      "[100]\tvalid_0's binary_logloss: 0.0336108\n",
      "[101]\tvalid_0's binary_logloss: 0.0334959\n",
      "[102]\tvalid_0's binary_logloss: 0.0332099\n",
      "[103]\tvalid_0's binary_logloss: 0.0331183\n",
      "[104]\tvalid_0's binary_logloss: 0.0329043\n",
      "[105]\tvalid_0's binary_logloss: 0.0326698\n",
      "[106]\tvalid_0's binary_logloss: 0.032459\n",
      "[107]\tvalid_0's binary_logloss: 0.0322319\n",
      "[108]\tvalid_0's binary_logloss: 0.0320158\n",
      "[109]\tvalid_0's binary_logloss: 0.0317818\n",
      "[110]\tvalid_0's binary_logloss: 0.0316733\n",
      "[111]\tvalid_0's binary_logloss: 0.0314232\n",
      "[112]\tvalid_0's binary_logloss: 0.0311553\n",
      "[113]\tvalid_0's binary_logloss: 0.0309374\n",
      "[114]\tvalid_0's binary_logloss: 0.0307662\n",
      "[115]\tvalid_0's binary_logloss: 0.0305991\n",
      "[116]\tvalid_0's binary_logloss: 0.0304779\n",
      "[117]\tvalid_0's binary_logloss: 0.0302713\n",
      "[118]\tvalid_0's binary_logloss: 0.0299347\n",
      "[119]\tvalid_0's binary_logloss: 0.0298814\n",
      "[120]\tvalid_0's binary_logloss: 0.0297168\n",
      "[121]\tvalid_0's binary_logloss: 0.0295488\n",
      "[122]\tvalid_0's binary_logloss: 0.0294125\n",
      "[123]\tvalid_0's binary_logloss: 0.029398\n",
      "[124]\tvalid_0's binary_logloss: 0.0291458\n",
      "[125]\tvalid_0's binary_logloss: 0.0289868\n",
      "[126]\tvalid_0's binary_logloss: 0.02888\n",
      "[127]\tvalid_0's binary_logloss: 0.0286765\n",
      "[128]\tvalid_0's binary_logloss: 0.0283878\n",
      "[129]\tvalid_0's binary_logloss: 0.0283389\n",
      "[130]\tvalid_0's binary_logloss: 0.0282781\n",
      "[131]\tvalid_0's binary_logloss: 0.0280294\n",
      "[132]\tvalid_0's binary_logloss: 0.0278401\n",
      "[133]\tvalid_0's binary_logloss: 0.0276589\n",
      "[134]\tvalid_0's binary_logloss: 0.0277685\n",
      "[135]\tvalid_0's binary_logloss: 0.0276395\n",
      "[136]\tvalid_0's binary_logloss: 0.0275321\n",
      "[137]\tvalid_0's binary_logloss: 0.0273631\n",
      "[138]\tvalid_0's binary_logloss: 0.0271689\n",
      "[139]\tvalid_0's binary_logloss: 0.0270241\n",
      "[140]\tvalid_0's binary_logloss: 0.0269137\n",
      "[141]\tvalid_0's binary_logloss: 0.0267791\n",
      "[142]\tvalid_0's binary_logloss: 0.0267024\n",
      "[143]\tvalid_0's binary_logloss: 0.026579\n",
      "[144]\tvalid_0's binary_logloss: 0.0263126\n",
      "[145]\tvalid_0's binary_logloss: 0.0261546\n",
      "[146]\tvalid_0's binary_logloss: 0.0260261\n",
      "[147]\tvalid_0's binary_logloss: 0.0259529\n",
      "[148]\tvalid_0's binary_logloss: 0.0258142\n",
      "[149]\tvalid_0's binary_logloss: 0.0258317\n",
      "[150]\tvalid_0's binary_logloss: 0.0257577\n",
      "[151]\tvalid_0's binary_logloss: 0.0257805\n",
      "[152]\tvalid_0's binary_logloss: 0.0257128\n",
      "[153]\tvalid_0's binary_logloss: 0.0256239\n",
      "[154]\tvalid_0's binary_logloss: 0.0256007\n",
      "[155]\tvalid_0's binary_logloss: 0.0254972\n",
      "[156]\tvalid_0's binary_logloss: 0.0254068\n",
      "[157]\tvalid_0's binary_logloss: 0.0253262\n",
      "[158]\tvalid_0's binary_logloss: 0.0252362\n",
      "[159]\tvalid_0's binary_logloss: 0.0250787\n",
      "[160]\tvalid_0's binary_logloss: 0.0250701\n",
      "[161]\tvalid_0's binary_logloss: 0.0249518\n",
      "[162]\tvalid_0's binary_logloss: 0.0248546\n",
      "[163]\tvalid_0's binary_logloss: 0.0247072\n",
      "[164]\tvalid_0's binary_logloss: 0.0245863\n",
      "[165]\tvalid_0's binary_logloss: 0.0245186\n",
      "[166]\tvalid_0's binary_logloss: 0.0244226\n",
      "[167]\tvalid_0's binary_logloss: 0.0244295\n",
      "[168]\tvalid_0's binary_logloss: 0.0243413\n",
      "[169]\tvalid_0's binary_logloss: 0.024284\n",
      "[170]\tvalid_0's binary_logloss: 0.0242269\n",
      "[171]\tvalid_0's binary_logloss: 0.024096\n",
      "[172]\tvalid_0's binary_logloss: 0.0241666\n",
      "[173]\tvalid_0's binary_logloss: 0.0241135\n",
      "[174]\tvalid_0's binary_logloss: 0.0239405\n",
      "[175]\tvalid_0's binary_logloss: 0.0238292\n",
      "[176]\tvalid_0's binary_logloss: 0.0238381\n",
      "[177]\tvalid_0's binary_logloss: 0.0238672\n",
      "[178]\tvalid_0's binary_logloss: 0.0237786\n",
      "[179]\tvalid_0's binary_logloss: 0.0236974\n",
      "[180]\tvalid_0's binary_logloss: 0.0236273\n",
      "[181]\tvalid_0's binary_logloss: 0.0235119\n",
      "[182]\tvalid_0's binary_logloss: 0.0234384\n",
      "[183]\tvalid_0's binary_logloss: 0.0233318\n",
      "[184]\tvalid_0's binary_logloss: 0.0232531\n",
      "[185]\tvalid_0's binary_logloss: 0.0232164\n",
      "[186]\tvalid_0's binary_logloss: 0.0231342\n",
      "[187]\tvalid_0's binary_logloss: 0.0230902\n",
      "[188]\tvalid_0's binary_logloss: 0.0230687\n",
      "[189]\tvalid_0's binary_logloss: 0.0229518\n",
      "[190]\tvalid_0's binary_logloss: 0.0229617\n",
      "[191]\tvalid_0's binary_logloss: 0.0228765\n",
      "[192]\tvalid_0's binary_logloss: 0.0228318\n",
      "[193]\tvalid_0's binary_logloss: 0.022747\n",
      "[194]\tvalid_0's binary_logloss: 0.0226668\n",
      "[195]\tvalid_0's binary_logloss: 0.0226326\n",
      "[196]\tvalid_0's binary_logloss: 0.022609\n",
      "[197]\tvalid_0's binary_logloss: 0.0225248\n",
      "[198]\tvalid_0's binary_logloss: 0.0224553\n",
      "[199]\tvalid_0's binary_logloss: 0.022371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's binary_logloss: 0.0223153\n",
      "[201]\tvalid_0's binary_logloss: 0.0222235\n",
      "[202]\tvalid_0's binary_logloss: 0.0222071\n",
      "[203]\tvalid_0's binary_logloss: 0.0222045\n",
      "[204]\tvalid_0's binary_logloss: 0.0222346\n",
      "[205]\tvalid_0's binary_logloss: 0.0221861\n",
      "[206]\tvalid_0's binary_logloss: 0.0222075\n",
      "[207]\tvalid_0's binary_logloss: 0.0221068\n",
      "[208]\tvalid_0's binary_logloss: 0.0221132\n",
      "[209]\tvalid_0's binary_logloss: 0.0220833\n",
      "[210]\tvalid_0's binary_logloss: 0.0220365\n",
      "[211]\tvalid_0's binary_logloss: 0.0219935\n",
      "[212]\tvalid_0's binary_logloss: 0.0219894\n",
      "[213]\tvalid_0's binary_logloss: 0.0220137\n",
      "[214]\tvalid_0's binary_logloss: 0.0220177\n",
      "[215]\tvalid_0's binary_logloss: 0.0219722\n",
      "[216]\tvalid_0's binary_logloss: 0.0220012\n",
      "[217]\tvalid_0's binary_logloss: 0.0219874\n",
      "[218]\tvalid_0's binary_logloss: 0.0218965\n",
      "[219]\tvalid_0's binary_logloss: 0.0218642\n",
      "[220]\tvalid_0's binary_logloss: 0.0219216\n",
      "[221]\tvalid_0's binary_logloss: 0.0219158\n",
      "[222]\tvalid_0's binary_logloss: 0.0218828\n",
      "[223]\tvalid_0's binary_logloss: 0.0218674\n",
      "[224]\tvalid_0's binary_logloss: 0.0218196\n",
      "[225]\tvalid_0's binary_logloss: 0.0218165\n",
      "[226]\tvalid_0's binary_logloss: 0.0217971\n",
      "[227]\tvalid_0's binary_logloss: 0.0217871\n",
      "[228]\tvalid_0's binary_logloss: 0.021686\n",
      "[229]\tvalid_0's binary_logloss: 0.0217138\n",
      "[230]\tvalid_0's binary_logloss: 0.0216693\n",
      "[231]\tvalid_0's binary_logloss: 0.0216779\n",
      "[232]\tvalid_0's binary_logloss: 0.0216864\n",
      "[233]\tvalid_0's binary_logloss: 0.0216669\n",
      "[234]\tvalid_0's binary_logloss: 0.0216453\n",
      "[235]\tvalid_0's binary_logloss: 0.0216082\n",
      "[236]\tvalid_0's binary_logloss: 0.0215962\n",
      "[237]\tvalid_0's binary_logloss: 0.0215851\n",
      "[238]\tvalid_0's binary_logloss: 0.0215212\n",
      "[239]\tvalid_0's binary_logloss: 0.0215039\n",
      "[240]\tvalid_0's binary_logloss: 0.0214091\n",
      "[241]\tvalid_0's binary_logloss: 0.0213793\n",
      "[242]\tvalid_0's binary_logloss: 0.021365\n",
      "[243]\tvalid_0's binary_logloss: 0.0213623\n",
      "[244]\tvalid_0's binary_logloss: 0.0213598\n",
      "[245]\tvalid_0's binary_logloss: 0.0213769\n",
      "[246]\tvalid_0's binary_logloss: 0.0213262\n",
      "[247]\tvalid_0's binary_logloss: 0.0212978\n",
      "[248]\tvalid_0's binary_logloss: 0.0212895\n",
      "[249]\tvalid_0's binary_logloss: 0.0212494\n",
      "[250]\tvalid_0's binary_logloss: 0.0212315\n",
      "[251]\tvalid_0's binary_logloss: 0.0212043\n",
      "[252]\tvalid_0's binary_logloss: 0.0211953\n",
      "[253]\tvalid_0's binary_logloss: 0.0211857\n",
      "[254]\tvalid_0's binary_logloss: 0.0211298\n",
      "[255]\tvalid_0's binary_logloss: 0.0211043\n",
      "[256]\tvalid_0's binary_logloss: 0.0211411\n",
      "[257]\tvalid_0's binary_logloss: 0.0211661\n",
      "[258]\tvalid_0's binary_logloss: 0.0211364\n",
      "[259]\tvalid_0's binary_logloss: 0.0211694\n",
      "[260]\tvalid_0's binary_logloss: 0.0211494\n",
      "[261]\tvalid_0's binary_logloss: 0.0211385\n",
      "[262]\tvalid_0's binary_logloss: 0.0211278\n",
      "[263]\tvalid_0's binary_logloss: 0.0210731\n",
      "[264]\tvalid_0's binary_logloss: 0.0210711\n",
      "[265]\tvalid_0's binary_logloss: 0.0210005\n",
      "[266]\tvalid_0's binary_logloss: 0.0209506\n",
      "[267]\tvalid_0's binary_logloss: 0.0209593\n",
      "[268]\tvalid_0's binary_logloss: 0.0209104\n",
      "[269]\tvalid_0's binary_logloss: 0.0209126\n",
      "[270]\tvalid_0's binary_logloss: 0.0209131\n",
      "[271]\tvalid_0's binary_logloss: 0.0209531\n",
      "[272]\tvalid_0's binary_logloss: 0.020967\n",
      "[273]\tvalid_0's binary_logloss: 0.0209514\n",
      "[274]\tvalid_0's binary_logloss: 0.0208797\n",
      "[275]\tvalid_0's binary_logloss: 0.0208619\n",
      "[276]\tvalid_0's binary_logloss: 0.0208713\n",
      "[277]\tvalid_0's binary_logloss: 0.0208703\n",
      "[278]\tvalid_0's binary_logloss: 0.0208362\n",
      "[279]\tvalid_0's binary_logloss: 0.0208505\n",
      "[280]\tvalid_0's binary_logloss: 0.0208008\n",
      "[281]\tvalid_0's binary_logloss: 0.0207338\n",
      "[282]\tvalid_0's binary_logloss: 0.0207368\n",
      "[283]\tvalid_0's binary_logloss: 0.0206852\n",
      "[284]\tvalid_0's binary_logloss: 0.0207031\n",
      "[285]\tvalid_0's binary_logloss: 0.0207403\n",
      "[286]\tvalid_0's binary_logloss: 0.0207998\n",
      "[287]\tvalid_0's binary_logloss: 0.0207931\n",
      "[288]\tvalid_0's binary_logloss: 0.0208396\n",
      "[289]\tvalid_0's binary_logloss: 0.0208646\n",
      "[290]\tvalid_0's binary_logloss: 0.0208267\n",
      "[291]\tvalid_0's binary_logloss: 0.0208773\n",
      "[292]\tvalid_0's binary_logloss: 0.0208213\n",
      "[293]\tvalid_0's binary_logloss: 0.0207851\n",
      "Early stopping, best iteration is:\n",
      "[283]\tvalid_0's binary_logloss: 0.0206852\n",
      "val loss : 0.02069\n",
      "auc score : 0.99961\n",
      "accuracy score : 0.99363\n"
     ]
    }
   ],
   "source": [
    "model_m = get_model(merge_train_x[:,:], max_train_label, merge_val_x[:,:], max_val_label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.06704\n",
      "auc score : 0.99724\n",
      "accuracy score : 0.97945\n",
      "thre: 0.9968107390\n",
      "fp:  0.0009359047\n",
      "recall:  0.8009765855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.997240157779064, 0.06703657879012284, 0.8009765854806448)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p = model_m.predict(merge_test_x[:,:])\n",
    "y_pred = np.zeros((len(y_p), 1))\n",
    "for i in range(len(y_p)):\n",
    "    y_pred[i, 0] = y_p[i]\n",
    "\n",
    "estimate_model(y_pred, max_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-26975918d1e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_train_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_train_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_val_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val_label\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_m = get_model(merge_train_x[:,100:], max_train_label, merge_val_x[:,100:], max_val_label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_m' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-c44e269062b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_test_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_m' is not defined"
     ]
    }
   ],
   "source": [
    "y_p = model_m.predict(merge_test_x[:,100:])\n",
    "y_pred = np.zeros((len(y_p), 1))\n",
    "for i in range(len(y_p)):\n",
    "    y_pred[i, 0] = y_p[i]\n",
    "\n",
    "estimate_model(y_pred, max_test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
