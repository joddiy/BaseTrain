{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# specify which GPU will be used\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pymysql\n",
    "from warnings import filterwarnings\n",
    "\n",
    "_connection = None\n",
    "\n",
    "def get_connection(db_config):\n",
    "    \"\"\"\n",
    "    get db connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        _connection = pymysql.connect(host=db_config['host'], user=db_config['username'],\n",
    "                                      password=db_config['password'],\n",
    "                                      db=db_config['db'], charset=\"utf8\")\n",
    "        filterwarnings('ignore', category=pymysql.Warning)\n",
    "\n",
    "    return _connection\n",
    "\n",
    "\n",
    "def close():\n",
    "    \"\"\"\n",
    "    close DB connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is not None:\n",
    "        _connection.close()\n",
    "    _connection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {\n",
    "    'host': '172.26.187.242',\n",
    "    'username': 'malware_r',\n",
    "    'password': 'GEg22v2O7jbfWhb3',\n",
    "    'db': 'malware'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields\n",
    "\n",
    "- mw_file_suffix: file name after hash value\n",
    "- mw_file_prefix: directory\n",
    "- mw_em_f: features of ember, splitted by \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# the base function which can query sql and return dict data\n",
    "def get_specific_data(table_suffix, sql=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        raise Exception(\"please init db connect first\")\n",
    "\n",
    "    cursor = _connection.cursor()\n",
    "    cursor.execute(\"SET NAMES utf8mb4\")\n",
    "\n",
    "    ret = []\n",
    "        \n",
    "    cursor.execute(sql)\n",
    "\n",
    "    field_names = [i[0] for i in cursor.description]\n",
    "\n",
    "    for row in cursor:\n",
    "        temp = {}\n",
    "        for key in range(len(row)):\n",
    "            temp[field_names[key]] = row[key]\n",
    "        ret.append(temp)\n",
    "     \n",
    "    cursor.close()\n",
    "    # _connection.close()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5.226050138473511 seconds ---\n",
      "--- 7.007890701293945 seconds ---\n",
      "--- 5.362804174423218 seconds ---\n",
      "58589\n"
     ]
    }
   ],
   "source": [
    "close()\n",
    "res1 = []\n",
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "table_suffix = [\"0\",\"8\",\"F\"]\n",
    "# Iterate all partitions of databases\n",
    "for suffix in table_suffix:\n",
    "    sql = \"\"\" \n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  a.section_name,\n",
    "  c.mw_file_suffix as mw_file_size,\n",
    "  c.mw_file_prefix as mw_file_directory,\n",
    "  c.mw_num_engines,\n",
    "  a.pointerto_raw_data,\n",
    "  a.virtual_size,\n",
    "  d.mw_em_f\n",
    "from mw_index_2017_section_%s as a\n",
    "  inner join mw_index_2017_%s c on a.mw_file_hash = c.mw_file_hash\n",
    "  inner join mw_index_2017_feature_%s d on a.mw_file_hash = d.mw_file_hash\n",
    "where CNT_CODE = 1 and MEM_EXECUTE = 1 and c.mw_num_engines <> -1 and (c.mw_num_engines >= 4 or c.mw_num_engines = 0) and\n",
    "      c.mw_file_prefix in ('201702')\n",
    "    \"\"\" % (suffix, suffix, suffix)\n",
    "    res1.extend(get_specific_data(suffix, sql))\n",
    "close()\n",
    "print(len(res1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.4539613723754883 seconds ---\n",
      "--- 2.776331901550293 seconds ---\n",
      "--- 1.7263424396514893 seconds ---\n",
      "24465\n"
     ]
    }
   ],
   "source": [
    "close()\n",
    "res2 = []\n",
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "table_suffix = [\"0\",\"8\",\"F\"]\n",
    "# Iterate all partitions of databases\n",
    "for suffix in table_suffix:\n",
    "    sql = \"\"\" \n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  a.section_name,\n",
    "  c.mw_file_suffix as mw_file_size,\n",
    "  c.mw_file_prefix as mw_file_directory,\n",
    "  c.mw_num_engines,\n",
    "  a.pointerto_raw_data,\n",
    "  a.virtual_size,\n",
    "  d.mw_em_f\n",
    "from mw_index_2017_section_%s as a\n",
    "  inner join mw_index_2017_%s c on a.mw_file_hash = c.mw_file_hash\n",
    "  inner join mw_index_2017_feature_%s d on a.mw_file_hash = d.mw_file_hash\n",
    "where CNT_CODE = 1 and MEM_EXECUTE = 1 and c.mw_num_engines <> -1 and (c.mw_num_engines >= 4 or c.mw_num_engines = 0) and\n",
    "      c.mw_file_prefix in ('201705')\n",
    "    \"\"\" % (suffix, suffix, suffix)\n",
    "    res2.extend(get_specific_data(suffix, sql))\n",
    "close()\n",
    "print(len(res2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_length = 300000\n",
    "\n",
    "train_data = pd.DataFrame(res1)\n",
    "# train_data = train_data.loc[train_data.virtual_size <= max_length]\n",
    "# train_data = train_data.reset_index(drop=True)\n",
    "train_data.mw_num_engines[train_data.mw_num_engines == 0 ] = 0\n",
    "train_data.mw_num_engines[train_data.mw_num_engines >= 4 ] = 1\n",
    "train_label = train_data.mw_num_engines.ravel()\n",
    "\n",
    "test_data = pd.DataFrame(res2)\n",
    "# test_data = test_data.loc[test_data.virtual_size <= max_length]\n",
    "# test_data = test_data.reset_index(drop=True)\n",
    "test_data.mw_num_engines[test_data.mw_num_engines == 0 ] = 0\n",
    "test_data.mw_num_engines[test_data.mw_num_engines >= 4 ] = 1\n",
    "test_label = test_data.mw_num_engines.ravel()\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data, train_label, test_size=0.1, random_state=2345)\n",
    "x_test = test_data\n",
    "y_test = test_label\n",
    "\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "x_val = x_val.reset_index(drop=True)\n",
    "x_test = x_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ember_feature(data):\n",
    "    ember_f = np.zeros((len(data.mw_em_f), 2351), dtype=float)\n",
    "    for index, item in data.iterrows():\n",
    "        ember_f[index, :] = item['mw_em_f'].split(';')\n",
    "    return ember_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(x_train, y_train, x_val, y_val):\n",
    "    params = {'application': 'binary'}\n",
    "    lgbm_dataset = lgb.Dataset(x_train, y_train.ravel())\n",
    "    valid_sets = lgb.Dataset(x_val, y_val.ravel())\n",
    "\n",
    "    model = lgb.train(params, lgbm_dataset, valid_sets=valid_sets, early_stopping_rounds=10)\n",
    "    y_pred = model.predict(x_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    loss = log_loss(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    acc = accuracy_score(y_val, (y_pred > 0.5).astype(int))\n",
    "#     model.save_model(file_path + \"-%04d-%.5f-%.5f.h5\" % (model.best_iteration, loss, acc),\n",
    "#                      num_iteration=model.best_iteration)\n",
    "    print(\"val loss : %.5f\" % loss)\n",
    "    print(\"auc score : %.5f\" % auc)\n",
    "    print(\"accuracy score : %.5f\" % acc)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_etrain = get_ember_feature(x_train)\n",
    "x_eval = get_ember_feature(x_val)\n",
    "x_etest = get_ember_feature(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.520668\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.463745\n",
      "[3]\tvalid_0's binary_logloss: 0.417665\n",
      "[4]\tvalid_0's binary_logloss: 0.379067\n",
      "[5]\tvalid_0's binary_logloss: 0.346971\n",
      "[6]\tvalid_0's binary_logloss: 0.319679\n",
      "[7]\tvalid_0's binary_logloss: 0.29645\n",
      "[8]\tvalid_0's binary_logloss: 0.276105\n",
      "[9]\tvalid_0's binary_logloss: 0.25832\n",
      "[10]\tvalid_0's binary_logloss: 0.2428\n",
      "[11]\tvalid_0's binary_logloss: 0.22914\n",
      "[12]\tvalid_0's binary_logloss: 0.216891\n",
      "[13]\tvalid_0's binary_logloss: 0.206268\n",
      "[14]\tvalid_0's binary_logloss: 0.197075\n",
      "[15]\tvalid_0's binary_logloss: 0.188283\n",
      "[16]\tvalid_0's binary_logloss: 0.180711\n",
      "[17]\tvalid_0's binary_logloss: 0.173922\n",
      "[18]\tvalid_0's binary_logloss: 0.167557\n",
      "[19]\tvalid_0's binary_logloss: 0.161919\n",
      "[20]\tvalid_0's binary_logloss: 0.156859\n",
      "[21]\tvalid_0's binary_logloss: 0.152155\n",
      "[22]\tvalid_0's binary_logloss: 0.147893\n",
      "[23]\tvalid_0's binary_logloss: 0.14401\n",
      "[24]\tvalid_0's binary_logloss: 0.139654\n",
      "[25]\tvalid_0's binary_logloss: 0.135149\n",
      "[26]\tvalid_0's binary_logloss: 0.131103\n",
      "[27]\tvalid_0's binary_logloss: 0.128388\n",
      "[28]\tvalid_0's binary_logloss: 0.12548\n",
      "[29]\tvalid_0's binary_logloss: 0.1225\n",
      "[30]\tvalid_0's binary_logloss: 0.120296\n",
      "[31]\tvalid_0's binary_logloss: 0.117704\n",
      "[32]\tvalid_0's binary_logloss: 0.115642\n",
      "[33]\tvalid_0's binary_logloss: 0.113874\n",
      "[34]\tvalid_0's binary_logloss: 0.112038\n",
      "[35]\tvalid_0's binary_logloss: 0.109785\n",
      "[36]\tvalid_0's binary_logloss: 0.107633\n",
      "[37]\tvalid_0's binary_logloss: 0.105885\n",
      "[38]\tvalid_0's binary_logloss: 0.104389\n",
      "[39]\tvalid_0's binary_logloss: 0.103223\n",
      "[40]\tvalid_0's binary_logloss: 0.102132\n",
      "[41]\tvalid_0's binary_logloss: 0.10129\n",
      "[42]\tvalid_0's binary_logloss: 0.100249\n",
      "[43]\tvalid_0's binary_logloss: 0.0988189\n",
      "[44]\tvalid_0's binary_logloss: 0.097573\n",
      "[45]\tvalid_0's binary_logloss: 0.0964129\n",
      "[46]\tvalid_0's binary_logloss: 0.0948155\n",
      "[47]\tvalid_0's binary_logloss: 0.0939284\n",
      "[48]\tvalid_0's binary_logloss: 0.0932148\n",
      "[49]\tvalid_0's binary_logloss: 0.0924781\n",
      "[50]\tvalid_0's binary_logloss: 0.091804\n",
      "[51]\tvalid_0's binary_logloss: 0.0911303\n",
      "[52]\tvalid_0's binary_logloss: 0.090447\n",
      "[53]\tvalid_0's binary_logloss: 0.0897948\n",
      "[54]\tvalid_0's binary_logloss: 0.0890294\n",
      "[55]\tvalid_0's binary_logloss: 0.0885173\n",
      "[56]\tvalid_0's binary_logloss: 0.0877313\n",
      "[57]\tvalid_0's binary_logloss: 0.0870969\n",
      "[58]\tvalid_0's binary_logloss: 0.0865042\n",
      "[59]\tvalid_0's binary_logloss: 0.0860212\n",
      "[60]\tvalid_0's binary_logloss: 0.0855318\n",
      "[61]\tvalid_0's binary_logloss: 0.084949\n",
      "[62]\tvalid_0's binary_logloss: 0.0845397\n",
      "[63]\tvalid_0's binary_logloss: 0.0840117\n",
      "[64]\tvalid_0's binary_logloss: 0.0834051\n",
      "[65]\tvalid_0's binary_logloss: 0.0828751\n",
      "[66]\tvalid_0's binary_logloss: 0.0823945\n",
      "[67]\tvalid_0's binary_logloss: 0.0819706\n",
      "[68]\tvalid_0's binary_logloss: 0.0815015\n",
      "[69]\tvalid_0's binary_logloss: 0.0810083\n",
      "[70]\tvalid_0's binary_logloss: 0.0805532\n",
      "[71]\tvalid_0's binary_logloss: 0.0800595\n",
      "[72]\tvalid_0's binary_logloss: 0.0796726\n",
      "[73]\tvalid_0's binary_logloss: 0.0794329\n",
      "[74]\tvalid_0's binary_logloss: 0.0788532\n",
      "[75]\tvalid_0's binary_logloss: 0.0782598\n",
      "[76]\tvalid_0's binary_logloss: 0.0779626\n",
      "[77]\tvalid_0's binary_logloss: 0.0776405\n",
      "[78]\tvalid_0's binary_logloss: 0.0774521\n",
      "[79]\tvalid_0's binary_logloss: 0.0769652\n",
      "[80]\tvalid_0's binary_logloss: 0.0766269\n",
      "[81]\tvalid_0's binary_logloss: 0.0763586\n",
      "[82]\tvalid_0's binary_logloss: 0.0760157\n",
      "[83]\tvalid_0's binary_logloss: 0.0756768\n",
      "[84]\tvalid_0's binary_logloss: 0.0754227\n",
      "[85]\tvalid_0's binary_logloss: 0.0750477\n",
      "[86]\tvalid_0's binary_logloss: 0.07472\n",
      "[87]\tvalid_0's binary_logloss: 0.0746028\n",
      "[88]\tvalid_0's binary_logloss: 0.0742674\n",
      "[89]\tvalid_0's binary_logloss: 0.0740229\n",
      "[90]\tvalid_0's binary_logloss: 0.07355\n",
      "[91]\tvalid_0's binary_logloss: 0.0732051\n",
      "[92]\tvalid_0's binary_logloss: 0.0730091\n",
      "[93]\tvalid_0's binary_logloss: 0.0728631\n",
      "[94]\tvalid_0's binary_logloss: 0.0726652\n",
      "[95]\tvalid_0's binary_logloss: 0.0722222\n",
      "[96]\tvalid_0's binary_logloss: 0.072068\n",
      "[97]\tvalid_0's binary_logloss: 0.0719033\n",
      "[98]\tvalid_0's binary_logloss: 0.0717373\n",
      "[99]\tvalid_0's binary_logloss: 0.0715188\n",
      "[100]\tvalid_0's binary_logloss: 0.0711477\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.0711477\n",
      "val loss : 0.07115\n",
      "auc score : 0.99489\n",
      "accuracy score : 0.97713\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = get_model(x_etrain, y_train, x_eval, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model(y_pred, test_y):\n",
    "    \n",
    "    loss = log_loss(test_y, y_pred)\n",
    "    auc = roc_auc_score(test_y, y_pred)\n",
    "    acc = accuracy_score(test_y, (y_pred > 0.5).astype(int))\n",
    "    print(\"loss : %.5f\" % loss)\n",
    "    print(\"auc score : %.5f\" % auc)\n",
    "    print(\"accuracy score : %.5f\" % acc)\n",
    "\n",
    "    fp_np_index = np.where(test_y == 0)\n",
    "    fp_np = y_pred[fp_np_index].shape[0]\n",
    "    thre_index = int(np.ceil(fp_np - fp_np * 0.001))\n",
    "\n",
    "    sorted_pred_prob = np.sort(y_pred[fp_np_index], axis=0)\n",
    "    thre = sorted_pred_prob[thre_index]\n",
    "    if thre == 1:\n",
    "        thre = max(sorted_pred_prob[np.where(sorted_pred_prob != 1)])\n",
    "\n",
    "    y_pred_prob = np.vstack((y_pred.transpose(), (1 - y_pred).transpose())).transpose()\n",
    "    y_pred_prob[:, 1] = thre\n",
    "    y_pred_label = np.argmin(y_pred_prob, axis=-1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(test_y, y_pred_label).ravel()\n",
    "    fp_rate = fp / (fp + tn)\n",
    "    recall_rate = tp / (tp + fn)\n",
    "\n",
    "    print(\"thre: %.10f\"%  thre)\n",
    "    print(\"fp:  %.10f\"%  fp_rate)\n",
    "    print(\"recall:  %.10f\"%  recall_rate)\n",
    "    \n",
    "    return auc, loss, recall_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.10468\n",
      "auc score : 0.99249\n",
      "accuracy score : 0.96354\n",
      "thre: 0.9677302462\n",
      "fp:  0.0008908686\n",
      "recall:  0.5603382014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9924880381546489, 0.10467744166450532, 0.5603382013835511)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p = model.predict(x_etest)\n",
    "y_pred_e = np.zeros((len(y_p), 1))\n",
    "for i in range(len(y_p)):\n",
    "    y_pred_e[i, 0] = y_p[i]\n",
    "\n",
    "estimate_model(y_pred_e, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "\n",
    "    def __init__(self, list_IDs, datasets, labels, batch_size=32, dim=8192, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.datasets = datasets\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size, self.dim), dtype=float)\n",
    "        y = np.zeros(self.batch_size, dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            base_path = \"/ssd/2017/{0}/{1}{2}\"\n",
    "            item = self.datasets.loc[ID]\n",
    "            file_path = base_path.format(item[\"mw_file_directory\"], item[\"mw_file_hash\"], item[\"mw_file_size\"])\n",
    "            in_file = open(file_path, 'rb')\n",
    "            in_file.seek(item['pointerto_raw_data'])\n",
    "            if item['virtual_size'] > max_length:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(max_length)]\n",
    "            else:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(item['virtual_size'])]\n",
    "            X[i, 0:len(bytes_data)] = bytes_data\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras import Input\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, Conv1D, Multiply, GlobalMaxPooling1D, Dropout, Activation\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TMalConv(object):\n",
    "    \"\"\"\n",
    "    train of mal conv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.max_len = max_length\n",
    "        self.history = None\n",
    "        self.model = None\n",
    "        self.p_md5 = None\n",
    "        self.time = time.time()\n",
    "        self.summary = {\n",
    "            'time':time.time(),\n",
    "            'batch_size': 32,\n",
    "            'epochs': 64,\n",
    "            'g_c_filter': 128,\n",
    "            'g_c_kernel_size': 500,\n",
    "            'g_c_stride': 500,\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        \n",
    "    def get_p(self, key):\n",
    "        \"\"\"\n",
    "        get the parameter from the summary\n",
    "        :param key:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.summary[key]\n",
    "\n",
    "    def gate_cnn(self, gate_cnn_input):\n",
    "        \"\"\"\n",
    "        construct a gated cnn by the specific kernel size\n",
    "        :param gate_cnn_input:\n",
    "        :param kernel_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        gate_cnn_input = Activation('relu')(gate_cnn_input)\n",
    "        \n",
    "        conv1_out = Conv1D(self.get_p(\"g_c_filter\"), self.get_p(\"g_c_kernel_size\"), strides=self.get_p(\"g_c_stride\"))(\n",
    "            gate_cnn_input)\n",
    "        conv2_out = Conv1D(self.get_p(\"g_c_filter\"), self.get_p(\"g_c_kernel_size\"), strides=self.get_p(\"g_c_stride\"),\n",
    "                           activation=\"sigmoid\")(gate_cnn_input)\n",
    "        merged = Multiply()([conv1_out, conv2_out])\n",
    "        gate_cnn_output = GlobalMaxPooling1D()(merged)\n",
    "        return gate_cnn_output\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        get a model\n",
    "        :param max_len:\n",
    "        :param kernel_sizes:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        net_input = Input(shape=(self.max_len,))\n",
    "\n",
    "        embedding_out = Embedding(256, 8, input_length=self.max_len)(net_input)\n",
    "        merged = self.gate_cnn(embedding_out)\n",
    "\n",
    "        dense_out = Dense(128)(merged)\n",
    "        \n",
    "        net_output = Dense(1, activation='sigmoid')(dense_out)\n",
    "\n",
    "        model = keras.models.Model(inputs=net_input, outputs=net_output)\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        batch_size = self.get_p(\"batch_size\")\n",
    "        epochs = self.get_p(\"epochs\")\n",
    "\n",
    "        self.model = self.get_model()\n",
    "\n",
    "        print('Length of the train: ', len(x_train))\n",
    "        print('Length of the validation: ', len(x_val))\n",
    "        \n",
    "#         tensor_board = TensorBoard(log_dir='./logs/', batch_size=batch_size)\n",
    "        file_path = \"/home/zhaoqi/BaseTrain/models/\"+ str(self.time) +\"-{epoch:04d}-{val_loss:.5f}-{val_acc:.5f}.h5\"\n",
    "        early_stopping = EarlyStopping(\"val_loss\", patience=3, verbose=0, mode='auto')\n",
    "        check_point = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "        callbacks_list = [check_point, early_stopping]\n",
    "\n",
    "        # Generators\n",
    "        training_generator = DataGenerator(range(len(x_train)), x_train, y_train, batch_size, self.max_len)\n",
    "        validation_generator = DataGenerator(range(len(x_val)), x_val, y_val, batch_size, self.max_len)\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                           optimizer='adam',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        self.model.fit_generator(generator=training_generator,\n",
    "                                 validation_data=validation_generator,\n",
    "                                 use_multiprocessing=True,\n",
    "                                 epochs=epochs,\n",
    "                                 workers=6,\n",
    "                                 callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 300000)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300000, 8)    2048        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300000, 8)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 600, 128)     512128      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 600, 128)     512128      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 600, 128)     0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,042,945\n",
      "Trainable params: 1,042,945\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Length of the train:  52730\n",
      "Length of the validation:  5859\n",
      "Epoch 1/64\n",
      "1500/1647 [==========================>...] - ETA: 1:11 - loss: 0.2314 - acc: 0.9142"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.188627). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647/1647 [==============================] - 936s 568ms/step - loss: 0.2285 - acc: 0.9158 - val_loss: 0.1919 - val_acc: 0.9310\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19186, saving model to /home/zhaoqi/BaseTrain/models/1533004410.4653583-0001-0.19186-0.93101.h5\n",
      "Epoch 2/64\n",
      " 332/1647 [=====>........................] - ETA: 12:39 - loss: 0.1601 - acc: 0.9490"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.193544). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1321/1647 [=======================>......] - ETA: 3:10 - loss: 0.1593 - acc: 0.9470"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.202953). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647/1647 [==============================] - 1078s 654ms/step - loss: 0.1584 - acc: 0.9475 - val_loss: 0.1747 - val_acc: 0.9404\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19186 to 0.17470, saving model to /home/zhaoqi/BaseTrain/models/1533004410.4653583-0002-0.17470-0.94040.h5\n",
      "Epoch 3/64\n",
      " 293/1647 [====>.........................] - ETA: 13:03 - loss: 0.1148 - acc: 0.9676"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.178186). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 402/1647 [======>.......................] - ETA: 12:02 - loss: 0.1124 - acc: 0.9690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.185371). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1121/1647 [===================>..........] - ETA: 5:06 - loss: 0.1179 - acc: 0.9673"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.168669). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1292/1647 [======================>.......] - ETA: 3:27 - loss: 0.1172 - acc: 0.9673"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.222327). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1430/1647 [=========================>....] - ETA: 2:07 - loss: 0.1173 - acc: 0.9671"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.195958). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647/1647 [==============================] - 1064s 646ms/step - loss: 0.1168 - acc: 0.9671 - val_loss: 0.1954 - val_acc: 0.9348\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.17470\n",
      "Epoch 4/64\n",
      " 182/1647 [==>...........................] - ETA: 14:25 - loss: 0.0943 - acc: 0.9772"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.181150). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1051/1647 [==================>...........] - ETA: 5:57 - loss: 0.0894 - acc: 0.9799"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.192335). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647/1647 [==============================] - 1107s 672ms/step - loss: 0.0933 - acc: 0.9784 - val_loss: 0.1866 - val_acc: 0.9421\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.17470\n",
      "Epoch 5/64\n",
      " 671/1647 [===========>..................] - ETA: 9:32 - loss: 0.0815 - acc: 0.9828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.188902). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647/1647 [==============================] - 1116s 677ms/step - loss: 0.0852 - acc: 0.9816 - val_loss: 0.2047 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.17470\n"
     ]
    }
   ],
   "source": [
    "t_instance = TMalConv()\n",
    "t_instance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 412s 539ms/step\n"
     ]
    }
   ],
   "source": [
    "model_dir = '/home/zhaoqi/BaseTrain/models/'\n",
    "f_name = '1533004410.4653583-0002-0.17470-0.94040.h5'\n",
    "c_model = load_model(model_dir + f_name)\n",
    "\n",
    "test_generator = DataGenerator(range(len(x_test)), x_test, y_test, 32, max_length, False)\n",
    "y_pred = c_model.predict_generator(generator=test_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.25448\n",
      "auc score : 0.94617\n",
      "accuracy score : 0.89508\n",
      "thre: 0.9772065878\n",
      "fp:  0.0008914642\n",
      "recall:  0.2730769231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9461725969038761, 0.25447557655325487, 0.27307692307692305)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_model(y_pred, y_test[0:len(y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Malconv and Ember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.25529\n",
      "auc score : 0.93668\n",
      "accuracy score : 0.89484\n",
      "thre: 0.9787095785\n",
      "fp:  0.0009366219\n",
      "recall:  0.3027277155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9366812637141965, 0.255294795930398, 0.30272771553823674)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_x_test = x_test.iloc[0:len(y_pred)]\n",
    "f_x_test['pred'] = y_pred\n",
    "f_x_test = f_x_test[['mw_file_hash','mw_num_engines','pred']]\n",
    "f_x_test = f_x_test.groupby('mw_file_hash').max().reset_index(drop=True)\n",
    "estimate_model(f_x_test['pred'].values, f_x_test['mw_num_engines'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647/1647 [==============================] - 368s 223ms/step\n",
      "183/183 [==============================] - 29s 157ms/step\n",
      "764/764 [==============================] - 156s 204ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model_f = Model(c_model.input, c_model.layers[-3].output)\n",
    "\n",
    "train_generator = DataGenerator(range(len(x_train)), x_train, y_train, 32, max_length, False)\n",
    "malcon_train_x = model_f.predict_generator(generator=train_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)\n",
    "\n",
    "val_generator = DataGenerator(range(len(x_val)), x_val, y_val, 32, max_length, False)\n",
    "malcon_val_x = model_f.predict_generator(generator=val_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)\n",
    "\n",
    "test_generator = DataGenerator(range(len(x_test)), x_test, y_test, 32, max_length, False)\n",
    "malcon_test_x = model_f.predict_generator(generator=test_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_feature(origin_data, feature_data):\n",
    "    feature_data = pd.DataFrame(feature_data)\n",
    "    feature_data['mw_file_hash'] = origin_data.iloc[0:len(feature_data)][['mw_file_hash']]\n",
    "    feature_data = feature_data.groupby('mw_file_hash').first().merge(origin_data[['mw_file_hash','mw_em_f','mw_num_engines']], how='inner', on='mw_file_hash')\n",
    "    tmp_ember = get_ember_feature(feature_data[['mw_em_f']])\n",
    "    tmp_label = feature_data[['mw_num_engines']].mw_num_engines\n",
    "    tmp_data = feature_data.drop(['mw_num_engines', 'mw_em_f', 'mw_file_hash'], axis=1)\n",
    "    return tmp_data, tmp_label, tmp_ember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_data, max_train_label, max_train_ember = get_max_feature(x_train, malcon_train_x)\n",
    "max_val_data, max_val_label, max_val_ember = get_max_feature(x_val, malcon_val_x)\n",
    "max_test_data, max_test_label, max_test_ember = get_max_feature(x_test, malcon_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_feature(m_data, e_data):\n",
    "    num = len(m_data)\n",
    "    m_x = np.zeros((num, 128+2351), dtype=float)\n",
    "    \n",
    "    for index in range(num):\n",
    "        m_x[index, 0:128] = m_data.iloc[index]\n",
    "        m_x[index, 128:128+2351] = e_data[index]  \n",
    "    return m_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_train_x = merge_feature(max_train_data, max_train_ember)\n",
    "merge_val_x = merge_feature(max_val_data, max_val_ember)\n",
    "merge_test_x = merge_feature(max_test_data, max_test_ember)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.524031\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.46987\n",
      "[3]\tvalid_0's binary_logloss: 0.426163\n",
      "[4]\tvalid_0's binary_logloss: 0.389946\n",
      "[5]\tvalid_0's binary_logloss: 0.360472\n",
      "[6]\tvalid_0's binary_logloss: 0.335289\n",
      "[7]\tvalid_0's binary_logloss: 0.313761\n",
      "[8]\tvalid_0's binary_logloss: 0.295727\n",
      "[9]\tvalid_0's binary_logloss: 0.280041\n",
      "[10]\tvalid_0's binary_logloss: 0.266274\n",
      "[11]\tvalid_0's binary_logloss: 0.254547\n",
      "[12]\tvalid_0's binary_logloss: 0.244384\n",
      "[13]\tvalid_0's binary_logloss: 0.234946\n",
      "[14]\tvalid_0's binary_logloss: 0.227119\n",
      "[15]\tvalid_0's binary_logloss: 0.220544\n",
      "[16]\tvalid_0's binary_logloss: 0.214707\n",
      "[17]\tvalid_0's binary_logloss: 0.209596\n",
      "[18]\tvalid_0's binary_logloss: 0.204982\n",
      "[19]\tvalid_0's binary_logloss: 0.200993\n",
      "[20]\tvalid_0's binary_logloss: 0.19756\n",
      "[21]\tvalid_0's binary_logloss: 0.194767\n",
      "[22]\tvalid_0's binary_logloss: 0.192512\n",
      "[23]\tvalid_0's binary_logloss: 0.190222\n",
      "[24]\tvalid_0's binary_logloss: 0.188582\n",
      "[25]\tvalid_0's binary_logloss: 0.187159\n",
      "[26]\tvalid_0's binary_logloss: 0.186067\n",
      "[27]\tvalid_0's binary_logloss: 0.184801\n",
      "[28]\tvalid_0's binary_logloss: 0.184018\n",
      "[29]\tvalid_0's binary_logloss: 0.183379\n",
      "[30]\tvalid_0's binary_logloss: 0.183195\n",
      "[31]\tvalid_0's binary_logloss: 0.182656\n",
      "[32]\tvalid_0's binary_logloss: 0.182212\n",
      "[33]\tvalid_0's binary_logloss: 0.182297\n",
      "[34]\tvalid_0's binary_logloss: 0.182507\n",
      "[35]\tvalid_0's binary_logloss: 0.182751\n",
      "[36]\tvalid_0's binary_logloss: 0.18292\n",
      "[37]\tvalid_0's binary_logloss: 0.183592\n",
      "[38]\tvalid_0's binary_logloss: 0.183924\n",
      "[39]\tvalid_0's binary_logloss: 0.184088\n",
      "[40]\tvalid_0's binary_logloss: 0.184599\n",
      "[41]\tvalid_0's binary_logloss: 0.185235\n",
      "[42]\tvalid_0's binary_logloss: 0.185738\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's binary_logloss: 0.182212\n",
      "val loss : 0.18221\n",
      "auc score : 0.97063\n",
      "accuracy score : 0.93819\n"
     ]
    }
   ],
   "source": [
    "model_m = get_model(merge_train_x[:, 0:128], max_train_label, merge_val_x[:, 0:128], max_val_label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.27543\n",
      "auc score : 0.93688\n",
      "accuracy score : 0.89555\n",
      "thre: 0.9819630483\n",
      "fp:  0.0008914642\n",
      "recall:  0.0395202214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9368794265841063, 0.2754346038873211, 0.03952022143626019)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p = model_m.predict(merge_test_x[:, 0:])\n",
    "y_pred = np.zeros((len(y_p), 1))\n",
    "for i in range(len(y_p)):\n",
    "    y_pred[i, 0] = y_p[i]\n",
    "\n",
    "estimate_model(y_pred, max_test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
