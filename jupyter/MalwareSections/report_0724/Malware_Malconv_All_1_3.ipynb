{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# specify which GPU will be used\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pymysql\n",
    "from warnings import filterwarnings\n",
    "\n",
    "_connection = None\n",
    "\n",
    "def get_connection(db_config):\n",
    "    \"\"\"\n",
    "    get db connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        _connection = pymysql.connect(host=db_config['host'], user=db_config['username'],\n",
    "                                      password=db_config['password'],\n",
    "                                      db=db_config['db'], charset=\"utf8\")\n",
    "        filterwarnings('ignore', category=pymysql.Warning)\n",
    "\n",
    "    return _connection\n",
    "\n",
    "\n",
    "def close():\n",
    "    \"\"\"\n",
    "    close DB connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is not None:\n",
    "        _connection.close()\n",
    "    _connection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {\n",
    "    'host': '172.26.187.242',\n",
    "    'username': 'malware_r',\n",
    "    'password': 'GEg22v2O7jbfWhb3',\n",
    "    'db': 'malware'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields\n",
    "\n",
    "- mw_file_suffix: file name after hash value\n",
    "- mw_file_prefix: directory\n",
    "- mw_em_f: features of ember, splitted by \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# the base function which can query sql and return dict data\n",
    "def get_specific_data(table_suffix, sql=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        raise Exception(\"please init db connect first\")\n",
    "\n",
    "    cursor = _connection.cursor()\n",
    "    cursor.execute(\"SET NAMES utf8mb4\")\n",
    "\n",
    "    ret = []\n",
    "        \n",
    "    cursor.execute(sql)\n",
    "\n",
    "    field_names = [i[0] for i in cursor.description]\n",
    "\n",
    "    for row in cursor:\n",
    "        temp = {}\n",
    "        for key in range(len(row)):\n",
    "            temp[field_names[key]] = row[key]\n",
    "        ret.append(temp)\n",
    "     \n",
    "    cursor.close()\n",
    "    # _connection.close()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8.054723501205444 seconds ---\n",
      "--- 6.807900428771973 seconds ---\n",
      "--- 5.526529550552368 seconds ---\n",
      "58357\n"
     ]
    }
   ],
   "source": [
    "close()\n",
    "res1 = []\n",
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "table_suffix = [\"0\",\"8\",\"F\"]\n",
    "# Iterate all partitions of databases\n",
    "for suffix in table_suffix:\n",
    "    sql = \"\"\" \n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  a.section_name,\n",
    "  c.mw_file_suffix as mw_file_size,\n",
    "  c.mw_file_prefix as mw_file_directory,\n",
    "  c.mw_num_engines,\n",
    "  a.pointerto_raw_data,\n",
    "  a.virtual_size,\n",
    "  d.mw_em_f\n",
    "from mw_index_2017_section_%s as a\n",
    "  inner join mw_index_2017_%s c on a.mw_file_hash = c.mw_file_hash\n",
    "  inner join mw_index_2017_feature_%s d on a.mw_file_hash = d.mw_file_hash\n",
    "where CNT_CODE = 1 and MEM_EXECUTE = 1 and c.mw_num_engines <> -1 and (c.mw_num_engines >= 4 or c.mw_num_engines = 0) and\n",
    "      c.mw_file_prefix in ('201701')\n",
    "    \"\"\" % (suffix, suffix, suffix)\n",
    "    res1.extend(get_specific_data(suffix, sql))\n",
    "close()\n",
    "print(len(res1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.533724069595337 seconds ---\n",
      "--- 2.8871686458587646 seconds ---\n",
      "--- 2.8159091472625732 seconds ---\n",
      "24465\n"
     ]
    }
   ],
   "source": [
    "close()\n",
    "res2 = []\n",
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "table_suffix = [\"0\",\"8\",\"F\"]\n",
    "# Iterate all partitions of databases\n",
    "for suffix in table_suffix:\n",
    "    sql = \"\"\" \n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  a.section_name,\n",
    "  c.mw_file_suffix as mw_file_size,\n",
    "  c.mw_file_prefix as mw_file_directory,\n",
    "  c.mw_num_engines,\n",
    "  a.pointerto_raw_data,\n",
    "  a.virtual_size,\n",
    "  d.mw_em_f\n",
    "from mw_index_2017_section_%s as a\n",
    "  inner join mw_index_2017_%s c on a.mw_file_hash = c.mw_file_hash\n",
    "  inner join mw_index_2017_feature_%s d on a.mw_file_hash = d.mw_file_hash\n",
    "where CNT_CODE = 1 and MEM_EXECUTE = 1 and c.mw_num_engines <> -1 and (c.mw_num_engines >= 4 or c.mw_num_engines = 0) and\n",
    "      c.mw_file_prefix in ('201705')\n",
    "    \"\"\" % (suffix, suffix, suffix)\n",
    "    res2.extend(get_specific_data(suffix, sql))\n",
    "close()\n",
    "print(len(res2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_length = 300000\n",
    "\n",
    "train_data = pd.DataFrame(res1)\n",
    "# train_data = train_data.loc[train_data.virtual_size <= max_length]\n",
    "# train_data = train_data.reset_index(drop=True)\n",
    "train_data.mw_num_engines[train_data.mw_num_engines == 0 ] = 0\n",
    "train_data.mw_num_engines[train_data.mw_num_engines >= 4 ] = 1\n",
    "train_label = train_data.mw_num_engines.ravel()\n",
    "\n",
    "test_data = pd.DataFrame(res2)\n",
    "# test_data = test_data.loc[test_data.virtual_size <= max_length]\n",
    "# test_data = test_data.reset_index(drop=True)\n",
    "test_data.mw_num_engines[test_data.mw_num_engines == 0 ] = 0\n",
    "test_data.mw_num_engines[test_data.mw_num_engines >= 4 ] = 1\n",
    "test_label = test_data.mw_num_engines.ravel()\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data, train_label, test_size=0.1, random_state=2345)\n",
    "x_test = test_data\n",
    "y_test = test_label\n",
    "\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "x_val = x_val.reset_index(drop=True)\n",
    "x_test = x_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ember_feature(data):\n",
    "    ember_f = np.zeros((len(data.mw_em_f), 2351), dtype=float)\n",
    "    for index, item in data.iterrows():\n",
    "        ember_f[index, :] = item['mw_em_f'].split(';')\n",
    "    return ember_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(x_train, y_train, x_val, y_val):\n",
    "    params = {'application': 'binary'}\n",
    "    lgbm_dataset = lgb.Dataset(x_train, y_train.ravel())\n",
    "    valid_sets = lgb.Dataset(x_val, y_val.ravel())\n",
    "\n",
    "    model = lgb.train(params, lgbm_dataset, valid_sets=valid_sets, early_stopping_rounds=10)\n",
    "    y_pred = model.predict(x_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    loss = log_loss(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    acc = accuracy_score(y_val, (y_pred > 0.5).astype(int))\n",
    "#     model.save_model(file_path + \"-%04d-%.5f-%.5f.h5\" % (model.best_iteration, loss, acc),\n",
    "#                      num_iteration=model.best_iteration)\n",
    "    print(\"val loss : %.5f\" % loss)\n",
    "    print(\"auc score : %.5f\" % auc)\n",
    "    print(\"accuracy score : %.5f\" % acc)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_etrain = get_ember_feature(x_train)\n",
    "x_eval = get_ember_feature(x_val)\n",
    "x_etest = get_ember_feature(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.542036\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.483055\n",
      "[3]\tvalid_0's binary_logloss: 0.434861\n",
      "[4]\tvalid_0's binary_logloss: 0.395395\n",
      "[5]\tvalid_0's binary_logloss: 0.360363\n",
      "[6]\tvalid_0's binary_logloss: 0.330792\n",
      "[7]\tvalid_0's binary_logloss: 0.303965\n",
      "[8]\tvalid_0's binary_logloss: 0.280771\n",
      "[9]\tvalid_0's binary_logloss: 0.260945\n",
      "[10]\tvalid_0's binary_logloss: 0.242621\n",
      "[11]\tvalid_0's binary_logloss: 0.226366\n",
      "[12]\tvalid_0's binary_logloss: 0.212169\n",
      "[13]\tvalid_0's binary_logloss: 0.199134\n",
      "[14]\tvalid_0's binary_logloss: 0.186978\n",
      "[15]\tvalid_0's binary_logloss: 0.176701\n",
      "[16]\tvalid_0's binary_logloss: 0.167368\n",
      "[17]\tvalid_0's binary_logloss: 0.159069\n",
      "[18]\tvalid_0's binary_logloss: 0.1514\n",
      "[19]\tvalid_0's binary_logloss: 0.14439\n",
      "[20]\tvalid_0's binary_logloss: 0.137662\n",
      "[21]\tvalid_0's binary_logloss: 0.131836\n",
      "[22]\tvalid_0's binary_logloss: 0.126896\n",
      "[23]\tvalid_0's binary_logloss: 0.121235\n",
      "[24]\tvalid_0's binary_logloss: 0.116522\n",
      "[25]\tvalid_0's binary_logloss: 0.112013\n",
      "[26]\tvalid_0's binary_logloss: 0.107616\n",
      "[27]\tvalid_0's binary_logloss: 0.10388\n",
      "[28]\tvalid_0's binary_logloss: 0.100331\n",
      "[29]\tvalid_0's binary_logloss: 0.0969455\n",
      "[30]\tvalid_0's binary_logloss: 0.0939897\n",
      "[31]\tvalid_0's binary_logloss: 0.0907057\n",
      "[32]\tvalid_0's binary_logloss: 0.0880098\n",
      "[33]\tvalid_0's binary_logloss: 0.0856085\n",
      "[34]\tvalid_0's binary_logloss: 0.0835521\n",
      "[35]\tvalid_0's binary_logloss: 0.08137\n",
      "[36]\tvalid_0's binary_logloss: 0.0795934\n",
      "[37]\tvalid_0's binary_logloss: 0.0778703\n",
      "[38]\tvalid_0's binary_logloss: 0.0761878\n",
      "[39]\tvalid_0's binary_logloss: 0.0748154\n",
      "[40]\tvalid_0's binary_logloss: 0.0733306\n",
      "[41]\tvalid_0's binary_logloss: 0.0718083\n",
      "[42]\tvalid_0's binary_logloss: 0.07062\n",
      "[43]\tvalid_0's binary_logloss: 0.0692182\n",
      "[44]\tvalid_0's binary_logloss: 0.0678469\n",
      "[45]\tvalid_0's binary_logloss: 0.0667091\n",
      "[46]\tvalid_0's binary_logloss: 0.0654251\n",
      "[47]\tvalid_0's binary_logloss: 0.0642251\n",
      "[48]\tvalid_0's binary_logloss: 0.0634638\n",
      "[49]\tvalid_0's binary_logloss: 0.0625865\n",
      "[50]\tvalid_0's binary_logloss: 0.0616854\n",
      "[51]\tvalid_0's binary_logloss: 0.0608643\n",
      "[52]\tvalid_0's binary_logloss: 0.0600328\n",
      "[53]\tvalid_0's binary_logloss: 0.0593077\n",
      "[54]\tvalid_0's binary_logloss: 0.0584938\n",
      "[55]\tvalid_0's binary_logloss: 0.0577747\n",
      "[56]\tvalid_0's binary_logloss: 0.0570116\n",
      "[57]\tvalid_0's binary_logloss: 0.0563728\n",
      "[58]\tvalid_0's binary_logloss: 0.0557264\n",
      "[59]\tvalid_0's binary_logloss: 0.0549764\n",
      "[60]\tvalid_0's binary_logloss: 0.0544891\n",
      "[61]\tvalid_0's binary_logloss: 0.0541135\n",
      "[62]\tvalid_0's binary_logloss: 0.053547\n",
      "[63]\tvalid_0's binary_logloss: 0.0531017\n",
      "[64]\tvalid_0's binary_logloss: 0.0526629\n",
      "[65]\tvalid_0's binary_logloss: 0.0519994\n",
      "[66]\tvalid_0's binary_logloss: 0.051623\n",
      "[67]\tvalid_0's binary_logloss: 0.0511541\n",
      "[68]\tvalid_0's binary_logloss: 0.0507336\n",
      "[69]\tvalid_0's binary_logloss: 0.0501704\n",
      "[70]\tvalid_0's binary_logloss: 0.04955\n",
      "[71]\tvalid_0's binary_logloss: 0.0490684\n",
      "[72]\tvalid_0's binary_logloss: 0.0485975\n",
      "[73]\tvalid_0's binary_logloss: 0.0481908\n",
      "[74]\tvalid_0's binary_logloss: 0.0478334\n",
      "[75]\tvalid_0's binary_logloss: 0.0473185\n",
      "[76]\tvalid_0's binary_logloss: 0.0470553\n",
      "[77]\tvalid_0's binary_logloss: 0.0468109\n",
      "[78]\tvalid_0's binary_logloss: 0.046377\n",
      "[79]\tvalid_0's binary_logloss: 0.0459941\n",
      "[80]\tvalid_0's binary_logloss: 0.0456136\n",
      "[81]\tvalid_0's binary_logloss: 0.0452009\n",
      "[82]\tvalid_0's binary_logloss: 0.0447107\n",
      "[83]\tvalid_0's binary_logloss: 0.044269\n",
      "[84]\tvalid_0's binary_logloss: 0.0440011\n",
      "[85]\tvalid_0's binary_logloss: 0.0436255\n",
      "[86]\tvalid_0's binary_logloss: 0.0433181\n",
      "[87]\tvalid_0's binary_logloss: 0.0429787\n",
      "[88]\tvalid_0's binary_logloss: 0.0428149\n",
      "[89]\tvalid_0's binary_logloss: 0.0424759\n",
      "[90]\tvalid_0's binary_logloss: 0.0420564\n",
      "[91]\tvalid_0's binary_logloss: 0.041762\n",
      "[92]\tvalid_0's binary_logloss: 0.041551\n",
      "[93]\tvalid_0's binary_logloss: 0.0413492\n",
      "[94]\tvalid_0's binary_logloss: 0.0410704\n",
      "[95]\tvalid_0's binary_logloss: 0.0408004\n",
      "[96]\tvalid_0's binary_logloss: 0.0404441\n",
      "[97]\tvalid_0's binary_logloss: 0.0402848\n",
      "[98]\tvalid_0's binary_logloss: 0.0401354\n",
      "[99]\tvalid_0's binary_logloss: 0.039806\n",
      "[100]\tvalid_0's binary_logloss: 0.0396429\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.0396429\n",
      "val loss : 0.03964\n",
      "auc score : 0.99914\n",
      "accuracy score : 0.98681\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = get_model(x_etrain, y_train, x_eval, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model(y_pred, test_y):\n",
    "    \n",
    "    loss = log_loss(test_y, y_pred)\n",
    "    auc = roc_auc_score(test_y, y_pred)\n",
    "    acc = accuracy_score(test_y, (y_pred > 0.5).astype(int))\n",
    "    print(\"loss : %.5f\" % loss)\n",
    "    print(\"auc score : %.5f\" % auc)\n",
    "    print(\"accuracy score : %.5f\" % acc)\n",
    "\n",
    "    fp_np_index = np.where(test_y == 0)\n",
    "    fp_np = y_pred[fp_np_index].shape[0]\n",
    "    thre_index = int(np.ceil(fp_np - fp_np * 0.001))\n",
    "\n",
    "    sorted_pred_prob = np.sort(y_pred[fp_np_index], axis=0)\n",
    "    thre = sorted_pred_prob[thre_index]\n",
    "    if thre == 1:\n",
    "        thre = max(sorted_pred_prob[np.where(sorted_pred_prob != 1)])\n",
    "\n",
    "    y_pred_prob = np.vstack((y_pred.transpose(), (1 - y_pred).transpose())).transpose()\n",
    "    y_pred_prob[:, 1] = thre\n",
    "    y_pred_label = np.argmin(y_pred_prob, axis=-1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(test_y, y_pred_label).ravel()\n",
    "    fp_rate = fp / (fp + tn)\n",
    "    recall_rate = tp / (tp + fn)\n",
    "\n",
    "    print(\"thre: %.10f\"%  thre)\n",
    "    print(\"fp:  %.10f\"%  fp_rate)\n",
    "    print(\"recall:  %.10f\"%  recall_rate)\n",
    "    \n",
    "    return auc, loss, recall_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.16559\n",
      "auc score : 0.98690\n",
      "accuracy score : 0.93767\n",
      "thre: 0.9960937584\n",
      "fp:  0.0008351893\n",
      "recall:  0.4213681783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9869039063663552, 0.1655862394596109, 0.42136817832436585)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p = model.predict(x_etest)\n",
    "y_pred_e = np.zeros((len(y_p), 1))\n",
    "for i in range(len(y_p)):\n",
    "    y_pred_e[i, 0] = y_p[i]\n",
    "\n",
    "estimate_model(y_pred_e, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "\n",
    "    def __init__(self, list_IDs, datasets, labels, batch_size=32, dim=8192, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.datasets = datasets\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size, self.dim), dtype=float)\n",
    "        y = np.zeros(self.batch_size, dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            base_path = \"/ssd/2017/{0}/{1}{2}\"\n",
    "            item = self.datasets.loc[ID]\n",
    "            file_path = base_path.format(item[\"mw_file_directory\"], item[\"mw_file_hash\"], item[\"mw_file_size\"])\n",
    "            in_file = open(file_path, 'rb')\n",
    "            in_file.seek(item['pointerto_raw_data'])\n",
    "            if item['virtual_size'] > max_length:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(max_length)]\n",
    "            else:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(item['virtual_size'])]\n",
    "            X[i, 0:len(bytes_data)] = bytes_data\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras import Input\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, Conv1D, Multiply, GlobalMaxPooling1D, Dropout, Activation\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TMalConv(object):\n",
    "    \"\"\"\n",
    "    train of mal conv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.max_len = max_length\n",
    "        self.history = None\n",
    "        self.model = None\n",
    "        self.p_md5 = None\n",
    "        self.time = time.time()\n",
    "        self.summary = {\n",
    "            'time':time.time(),\n",
    "            'batch_size': 32,\n",
    "            'epochs': 64,\n",
    "            'g_c_filter': 128,\n",
    "            'g_c_kernel_size': 500,\n",
    "            'g_c_stride': 500,\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        \n",
    "    def get_p(self, key):\n",
    "        \"\"\"\n",
    "        get the parameter from the summary\n",
    "        :param key:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.summary[key]\n",
    "\n",
    "    def gate_cnn(self, gate_cnn_input):\n",
    "        \"\"\"\n",
    "        construct a gated cnn by the specific kernel size\n",
    "        :param gate_cnn_input:\n",
    "        :param kernel_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        gate_cnn_input = Activation('relu')(gate_cnn_input)\n",
    "        \n",
    "        conv1_out = Conv1D(self.get_p(\"g_c_filter\"), self.get_p(\"g_c_kernel_size\"), strides=self.get_p(\"g_c_stride\"))(\n",
    "            gate_cnn_input)\n",
    "        conv2_out = Conv1D(self.get_p(\"g_c_filter\"), self.get_p(\"g_c_kernel_size\"), strides=self.get_p(\"g_c_stride\"),\n",
    "                           activation=\"sigmoid\")(gate_cnn_input)\n",
    "        \n",
    "        merged = Multiply()([conv1_out, conv2_out])\n",
    "        gate_cnn_output = GlobalMaxPooling1D()(merged)\n",
    "        return gate_cnn_output\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        get a model\n",
    "        :param max_len:\n",
    "        :param kernel_sizes:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        net_input = Input(shape=(self.max_len,))\n",
    "\n",
    "        embedding_out = Embedding(256, 8, input_length=self.max_len)(net_input)\n",
    "        merged = self.gate_cnn(embedding_out)\n",
    "\n",
    "        dense_out = Dense(128)(merged)\n",
    "        \n",
    "        net_output = Dense(1, activation='sigmoid')(dense_out)\n",
    "\n",
    "        model = keras.models.Model(inputs=net_input, outputs=net_output)\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        batch_size = self.get_p(\"batch_size\")\n",
    "        epochs = self.get_p(\"epochs\")\n",
    "\n",
    "        self.model = self.get_model()\n",
    "\n",
    "        print('Length of the train: ', len(x_train))\n",
    "        print('Length of the validation: ', len(x_val))\n",
    "        \n",
    "#         tensor_board = TensorBoard(log_dir='./logs/', batch_size=batch_size)\n",
    "        file_path = \"/home/zhaoqi/BaseTrain/models/\"+ str(self.time) +\"-{epoch:04d}-{val_loss:.5f}-{val_acc:.5f}.h5\"\n",
    "        early_stopping = EarlyStopping(\"val_loss\", patience=3, verbose=0, mode='auto')\n",
    "        check_point = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "        callbacks_list = [check_point, early_stopping]\n",
    "\n",
    "        # Generators\n",
    "        training_generator = DataGenerator(range(len(x_train)), x_train, y_train, batch_size, self.max_len)\n",
    "        validation_generator = DataGenerator(range(len(x_val)), x_val, y_val, batch_size, self.max_len)\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                           optimizer='adam',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        self.model.fit_generator(generator=training_generator,\n",
    "                                 validation_data=validation_generator,\n",
    "                                 use_multiprocessing=True,\n",
    "                                 epochs=epochs,\n",
    "                                 workers=6,\n",
    "                                 callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 300000)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300000, 8)    2048        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300000, 8)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 600, 128)     512128      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 600, 128)     512128      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 600, 128)     0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,042,945\n",
      "Trainable params: 1,042,945\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Length of the train:  52521\n",
      "Length of the validation:  5836\n",
      "Epoch 1/64\n",
      "1641/1641 [==============================] - 678s 413ms/step - loss: 0.2198 - acc: 0.9070 - val_loss: 0.2171 - val_acc: 0.9050\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.21715, saving model to /home/zhaoqi/BaseTrain/models/1533004328.78999-0001-0.21715-0.90505.h5\n",
      "Epoch 2/64\n",
      "1641/1641 [==============================] - 1076s 656ms/step - loss: 0.1198 - acc: 0.9576 - val_loss: 0.1390 - val_acc: 0.9483\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.21715 to 0.13903, saving model to /home/zhaoqi/BaseTrain/models/1533004328.78999-0002-0.13903-0.94832.h5\n",
      "Epoch 3/64\n",
      "1641/1641 [==============================] - 1064s 648ms/step - loss: 0.0685 - acc: 0.9822 - val_loss: 0.1624 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13903\n",
      "Epoch 4/64\n",
      " 271/1641 [===>..........................] - ETA: 13:06 - loss: 0.0426 - acc: 0.9918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-38:\n",
      "Process ForkPoolWorker-32:\n",
      "Process ForkPoolWorker-37:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-40:\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 347, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/connection.py\", line 398, in _send_bytes\n",
      "    self._send(buf)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-22-16e7f329ed3a>\", line 29, in __getitem__\n",
      "    X, y = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-22-16e7f329ed3a>\", line 56, in __data_generation\n",
      "    X[i, 0:len(bytes_data)] = bytes_data\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-22-16e7f329ed3a>\", line 29, in __getitem__\n",
      "    X, y = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-22-16e7f329ed3a>\", line 56, in __data_generation\n",
      "    X[i, 0:len(bytes_data)] = bytes_data\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-35:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 341, in put\n",
      "    obj = ForkingPickler.dumps(obj)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/reduction.py\", line 50, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-42acf2777b3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTMalConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-5907fc887a49>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-5907fc887a49>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                                  \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                                  callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-47:\n",
      "Process ForkPoolWorker-46:\n",
      "Process ForkPoolWorker-48:\n",
      "Process ForkPoolWorker-45:\n",
      "Process ForkPoolWorker-44:\n",
      "Process ForkPoolWorker-43:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "t_instance = TMalConv()\n",
    "t_instance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 486s 637ms/step\n"
     ]
    }
   ],
   "source": [
    "model_dir = '/home/zhaoqi/BaseTrain/models/'\n",
    "f_name = '1533004328.78999-0002-0.13903-0.94832.h5'\n",
    "c_model = load_model(model_dir + f_name)\n",
    "\n",
    "test_generator = DataGenerator(range(len(x_test)), x_test, y_test, 32, max_length, False)\n",
    "y_pred = c_model.predict_generator(generator=test_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.31448\n",
      "auc score : 0.94071\n",
      "accuracy score : 0.87688\n",
      "thre: 0.9978906512\n",
      "fp:  0.0008914642\n",
      "recall:  0.3143076923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9407144657214859, 0.3144842622549249, 0.3143076923076923)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_model(y_pred, y_test[0:len(y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Malconv and Ember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.32620\n",
      "auc score : 0.91627\n",
      "accuracy score : 0.86765\n",
      "thre: 0.9968934059\n",
      "fp:  0.0005619732\n",
      "recall:  0.1928884559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9162662287349643, 0.32620220864521776, 0.19288845591816853)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_x_test = x_test.iloc[0:len(y_pred)]\n",
    "f_x_test['pred'] = y_pred\n",
    "f_x_test = f_x_test[['mw_file_hash','mw_num_engines','pred']]\n",
    "f_x_test = f_x_test.groupby('mw_file_hash').min().reset_index(drop=True)\n",
    "estimate_model(f_x_test['pred'].values, f_x_test['mw_num_engines'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1641/1641 [==============================] - 991s 604ms/step\n",
      "182/182 [==============================] - 113s 620ms/step\n",
      "764/764 [==============================] - 479s 627ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model_f = Model(c_model.input, c_model.layers[-2].output)\n",
    "\n",
    "train_generator = DataGenerator(range(len(x_train)), x_train, y_train, 32, max_length, False)\n",
    "malcon_train_x = model_f.predict_generator(generator=train_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)\n",
    "\n",
    "val_generator = DataGenerator(range(len(x_val)), x_val, y_val, 32, max_length, False)\n",
    "malcon_val_x = model_f.predict_generator(generator=val_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)\n",
    "\n",
    "test_generator = DataGenerator(range(len(x_test)), x_test, y_test, 32, max_length, False)\n",
    "malcon_test_x = model_f.predict_generator(generator=test_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_feature(origin_data, feature_data):\n",
    "    feature_data = pd.DataFrame(feature_data)\n",
    "    feature_data['mw_file_hash'] = origin_data.iloc[0:len(feature_data)][['mw_file_hash']]\n",
    "    feature_data = feature_data.groupby('mw_file_hash').first().merge(origin_data[['mw_file_hash','mw_em_f','mw_num_engines']], how='inner', on='mw_file_hash')\n",
    "    tmp_ember = get_ember_feature(feature_data[['mw_em_f']])\n",
    "    tmp_label = feature_data[['mw_num_engines']].mw_num_engines\n",
    "    tmp_data = feature_data.drop(['mw_num_engines', 'mw_em_f', 'mw_file_hash'], axis=1)\n",
    "    return tmp_data, tmp_label, tmp_ember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_data, max_train_label, max_train_ember = get_max_feature(x_train, malcon_train_x)\n",
    "max_val_data, max_val_label, max_val_ember = get_max_feature(x_val, malcon_val_x)\n",
    "max_test_data, max_test_label, max_test_ember = get_max_feature(x_test, malcon_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_feature(m_data, e_data):\n",
    "    num = len(m_data)\n",
    "    m_x = np.zeros((num, 128+2351), dtype=float)\n",
    "    \n",
    "    for index in range(num):\n",
    "        m_x[index, 0:128] = m_data.iloc[index]\n",
    "        m_x[index, 128:128+2351] = e_data[index]  \n",
    "    return m_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_train_x = merge_feature(max_train_data, max_train_ember)\n",
    "merge_val_x = merge_feature(max_val_data, max_val_ember)\n",
    "merge_test_x = merge_feature(max_test_data, max_test_ember)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.536386\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.474561\n",
      "[3]\tvalid_0's binary_logloss: 0.425101\n",
      "[4]\tvalid_0's binary_logloss: 0.384225\n",
      "[5]\tvalid_0's binary_logloss: 0.3503\n",
      "[6]\tvalid_0's binary_logloss: 0.321578\n",
      "[7]\tvalid_0's binary_logloss: 0.297083\n",
      "[8]\tvalid_0's binary_logloss: 0.276345\n",
      "[9]\tvalid_0's binary_logloss: 0.258626\n",
      "[10]\tvalid_0's binary_logloss: 0.243299\n",
      "[11]\tvalid_0's binary_logloss: 0.230444\n",
      "[12]\tvalid_0's binary_logloss: 0.219107\n",
      "[13]\tvalid_0's binary_logloss: 0.209166\n",
      "[14]\tvalid_0's binary_logloss: 0.200712\n",
      "[15]\tvalid_0's binary_logloss: 0.193389\n",
      "[16]\tvalid_0's binary_logloss: 0.187136\n",
      "[17]\tvalid_0's binary_logloss: 0.181883\n",
      "[18]\tvalid_0's binary_logloss: 0.177465\n",
      "[19]\tvalid_0's binary_logloss: 0.173696\n",
      "[20]\tvalid_0's binary_logloss: 0.170544\n",
      "[21]\tvalid_0's binary_logloss: 0.167904\n",
      "[22]\tvalid_0's binary_logloss: 0.165827\n",
      "[23]\tvalid_0's binary_logloss: 0.164114\n",
      "[24]\tvalid_0's binary_logloss: 0.162743\n",
      "[25]\tvalid_0's binary_logloss: 0.161632\n",
      "[26]\tvalid_0's binary_logloss: 0.160624\n",
      "[27]\tvalid_0's binary_logloss: 0.159905\n",
      "[28]\tvalid_0's binary_logloss: 0.159483\n",
      "[29]\tvalid_0's binary_logloss: 0.159397\n",
      "[30]\tvalid_0's binary_logloss: 0.159609\n",
      "[31]\tvalid_0's binary_logloss: 0.159659\n",
      "[32]\tvalid_0's binary_logloss: 0.159822\n",
      "[33]\tvalid_0's binary_logloss: 0.160189\n",
      "[34]\tvalid_0's binary_logloss: 0.160932\n",
      "[35]\tvalid_0's binary_logloss: 0.161401\n",
      "[36]\tvalid_0's binary_logloss: 0.161962\n",
      "[37]\tvalid_0's binary_logloss: 0.162564\n",
      "[38]\tvalid_0's binary_logloss: 0.162973\n",
      "[39]\tvalid_0's binary_logloss: 0.163804\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's binary_logloss: 0.159397\n",
      "val loss : 0.15940\n",
      "auc score : 0.98458\n",
      "accuracy score : 0.95039\n"
     ]
    }
   ],
   "source": [
    "model_m = get_model(merge_train_x[:,0:128], max_train_label, merge_val_x[:,0:128], max_val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.38485\n",
      "auc score : 0.93292\n",
      "accuracy score : 0.87485\n",
      "thre: 0.9824993241\n",
      "fp:  0.0001114330\n",
      "recall:  0.0016915270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9329205467597014, 0.38484676492734643, 0.0016915269875442103)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p = model_m.predict(merge_test_x[:,0:128])\n",
    "y_pred = np.zeros((len(y_p), 1))\n",
    "for i in range(len(y_p)):\n",
    "    y_pred[i, 0] = y_p[i]\n",
    "\n",
    "estimate_model(y_pred, max_test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
