{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pymysql\n",
    "from warnings import filterwarnings\n",
    "\n",
    "_connection = None\n",
    "\n",
    "def get_connection(db_config):\n",
    "    \"\"\"\n",
    "    get db connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        _connection = pymysql.connect(host=db_config['host'], user=db_config['username'],\n",
    "                                      password=db_config['password'],\n",
    "                                      db=db_config['db'], charset=\"utf8\")\n",
    "        filterwarnings('ignore', category=pymysql.Warning)\n",
    "\n",
    "    return _connection\n",
    "\n",
    "\n",
    "def close():\n",
    "    \"\"\"\n",
    "    close DB connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is not None:\n",
    "        _connection.close()\n",
    "    _connection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {\n",
    "    'host': '172.26.187.242',\n",
    "    'username': 'malware_r',\n",
    "    'password': 'GEg22v2O7jbfWhb3',\n",
    "    'db': 'malware'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_specific_data(table_suffix, sql=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        raise Exception(\"please init db connect first\")\n",
    "\n",
    "    cursor = _connection.cursor()\n",
    "    cursor.execute(\"SET NAMES utf8mb4\")\n",
    "\n",
    "    ret = []\n",
    "   \n",
    "    cursor.execute(sql)\n",
    "\n",
    "    field_names = [i[0] for i in cursor.description]\n",
    "\n",
    "    for row in cursor:\n",
    "        temp = {}\n",
    "        for key in range(len(row)):\n",
    "            temp[field_names[key]] = row[key]\n",
    "        ret.append(temp)\n",
    "     \n",
    "    cursor.close()\n",
    "    # _connection.close()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.70159912109375 seconds ---\n",
      "--- 0.6680092811584473 seconds ---\n",
      "--- 0.6432478427886963 seconds ---\n",
      "--- 0.6327581405639648 seconds ---\n",
      "--- 0.6401398181915283 seconds ---\n",
      "--- 0.6680278778076172 seconds ---\n",
      "--- 0.675234317779541 seconds ---\n",
      "--- 0.6311783790588379 seconds ---\n",
      "--- 0.7162714004516602 seconds ---\n",
      "--- 0.6555304527282715 seconds ---\n",
      "--- 0.5963871479034424 seconds ---\n",
      "--- 0.5353579521179199 seconds ---\n",
      "--- 0.45333290100097656 seconds ---\n",
      "--- 0.43079423904418945 seconds ---\n",
      "--- 0.4037284851074219 seconds ---\n",
      "--- 0.4111042022705078 seconds ---\n",
      "274812\n"
     ]
    }
   ],
   "source": [
    "close()\n",
    "res1 = []\n",
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "for suffix in table_suffix:\n",
    "    sql6 = \"\"\"\n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  a.mw_num_engines,\n",
    "  a.mw_file_prefix,\n",
    "  a.mw_file_suffix,\n",
    "  section_name,\n",
    "  virtual_size,\n",
    "  pointerto_raw_data\n",
    "from mw_index_2017_%s a\n",
    "  left join mw_index_2017_section_%s b on a.mw_file_hash = b.mw_file_hash\n",
    "where CNT_CODE = 1 and MEM_EXECUTE = 1 and mw_num_engines <> -1 and (mw_num_engines > 6 or mw_num_engines = 0)\n",
    "  and mw_file_prefix in ('201701')\n",
    "    \"\"\" % (suffix, suffix)\n",
    "    res1.extend(get_specific_data(suffix, sql6))\n",
    "close()\n",
    "print(len(res1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.4007298946380615 seconds ---\n",
      "--- 0.3579287528991699 seconds ---\n",
      "--- 0.3581352233886719 seconds ---\n",
      "--- 0.32486748695373535 seconds ---\n",
      "--- 0.3375849723815918 seconds ---\n",
      "--- 0.37219715118408203 seconds ---\n",
      "--- 0.40309906005859375 seconds ---\n",
      "--- 0.3790857791900635 seconds ---\n",
      "--- 0.3896019458770752 seconds ---\n",
      "--- 0.4195430278778076 seconds ---\n",
      "--- 0.4056985378265381 seconds ---\n",
      "--- 0.37197399139404297 seconds ---\n",
      "--- 0.33071446418762207 seconds ---\n",
      "--- 0.34854578971862793 seconds ---\n",
      "--- 0.38054347038269043 seconds ---\n",
      "--- 0.3898921012878418 seconds ---\n",
      "135306\n"
     ]
    }
   ],
   "source": [
    "close()\n",
    "res2 = []\n",
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "for suffix in table_suffix:\n",
    "    sql6 = \"\"\"\n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  a.mw_num_engines,\n",
    "  a.mw_file_prefix,\n",
    "  a.mw_file_suffix,\n",
    "  section_name,\n",
    "  virtual_size,\n",
    "  pointerto_raw_data\n",
    "from mw_index_2017_%s a\n",
    "  left join mw_index_2017_section_%s b on a.mw_file_hash = b.mw_file_hash\n",
    "where CNT_CODE = 1 and MEM_EXECUTE = 1 and mw_num_engines <> -1 and (mw_num_engines > 6 or mw_num_engines = 0)\n",
    "  and mw_file_prefix in ('201702')\n",
    "    \"\"\" % (suffix, suffix)\n",
    "    res2.extend(get_specific_data(suffix, sql6))\n",
    "close()\n",
    "print(len(res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_length = 10000\n",
    "\n",
    "train_data = pd.DataFrame(res1)\n",
    "# train_data = train_data.loc[train_data.virtual_size <= max_length]\n",
    "# train_data = train_data.reset_index(drop=True)\n",
    "train_data.mw_num_engines[train_data.mw_num_engines == 0 ] = 0\n",
    "train_data.mw_num_engines[train_data.mw_num_engines > 6 ] = 1\n",
    "train_label = train_data.mw_num_engines.ravel()\n",
    "\n",
    "test_data = pd.DataFrame(res2)\n",
    "# test_data = test_data.loc[test_data.virtual_size <= max_length]\n",
    "# test_data = test_data.reset_index(drop=True)\n",
    "test_data.mw_num_engines[test_data.mw_num_engines == 0 ] = 0\n",
    "test_data.mw_num_engines[test_data.mw_num_engines > 6 ] = 1\n",
    "test_label = test_data.mw_num_engines.ravel()\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data, train_label, test_size=0.1, random_state=2345)\n",
    "x_test = test_data\n",
    "y_test = test_label\n",
    "\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "x_val = x_val.reset_index(drop=True)\n",
    "x_test = x_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "\n",
    "    def __init__(self, list_IDs, datasets, labels, batch_size=32, dim=8192, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.datasets = datasets\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size, self.dim), dtype=float)\n",
    "        y = np.zeros(self.batch_size, dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            base_path = \"/ssd/2017/{0}/{1}{2}\"\n",
    "            item = self.datasets.loc[ID]\n",
    "            file_path = base_path.format(item[\"mw_file_prefix\"], item[\"mw_file_hash\"], item[\"mw_file_suffix\"])\n",
    "            in_file = open(file_path, 'rb')\n",
    "            in_file.seek(item['pointerto_raw_data'])\n",
    "            if item['virtual_size'] > max_length:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(max_length)]\n",
    "            else:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(item['virtual_size'])]\n",
    "            X[i, 0:len(bytes_data)] = bytes_data\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import keras\n",
    "import lightgbm as lgb\n",
    "import pymysql\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn import metrics as sm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class XX_DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "\n",
    "    def __init__(self, list_IDs, datasets, batch_size=32, dim=300000, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.datasets = datasets\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, X = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size, self.dim), dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            base_path = \"/ssd/2017/{0}/{1}{2}\"\n",
    "            item = self.datasets.loc[ID]\n",
    "            file_path = base_path.format(item[\"mw_file_prefix\"], item[\"mw_file_hash\"], item[\"mw_file_suffix\"])\n",
    "            in_file = open(file_path, 'rb')\n",
    "            in_file.seek(item['pointerto_raw_data'])\n",
    "            if item['virtual_size'] > max_length:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(max_length)]\n",
    "            else:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(item['virtual_size'])]\n",
    "            X[i, 0:len(bytes_data)] = bytes_data\n",
    "\n",
    "        X = X.reshape((-1, 400, 750, 1)) / 255.0\n",
    "        return X, X\n",
    "\n",
    "class Autoencoder():\n",
    "    def __init__(self, autoencoder_name, encoder_name, input_shape, conv_filter_num, conv_kernel_size, pooling_size,\n",
    "                 dense_num):\n",
    "        self.autoencoder_name = autoencoder_name\n",
    "        self.encoder_name = encoder_name\n",
    "        self.input_shape = input_shape\n",
    "        self.conv_filter_num = conv_filter_num\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.pooling_size = pooling_size\n",
    "        self.dense_num = dense_num\n",
    "        self.autoencoder = None\n",
    "        self.encoder = None\n",
    "\n",
    "    def get_model(self):\n",
    "        input_img = Input(self.input_shape)\n",
    "\n",
    "        x = Conv2D(self.conv_filter_num, self.conv_kernel_size, activation='relu', padding='same')(input_img)\n",
    "        x = MaxPooling2D(self.pooling_size, padding='same')(x)\n",
    "        encoded = Dense(self.dense_num, activation='sigmoid')(x)\n",
    "\n",
    "        x = Conv2D(self.conv_filter_num, self.conv_kernel_size, activation='relu', padding='same')(encoded)\n",
    "        x = UpSampling2D(self.pooling_size)(x)\n",
    "        decoded = Conv2D(1, self.conv_kernel_size, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "        self.autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "        self.encoder = Model(inputs=input_img, outputs=encoded)\n",
    "\n",
    "        self.autoencoder.compile(loss='mean_squared_error', optimizer=RMSprop())\n",
    "        self.autoencoder.summary()\n",
    "\n",
    "    def train(self, train_df, max_epoch, batch_size=16):\n",
    "        self.get_model()\n",
    "        partition_train, partition_validation = train_test_split(range(len(train_df)), test_size=0.05,\n",
    "                                                                 random_state=1234)\n",
    "        print('Length of the train: ', len(partition_train))\n",
    "        print('Length of the validation: ', len(partition_validation))\n",
    "\n",
    "        #         tensor_board = TensorBoard(log_dir='./logs/', batch_size=batch_size)\n",
    "        file_path = \"/home/zhaoqi/models/{epoch:04d}-{val_loss:.5f}.h5\"\n",
    "        #         early_stopping = EarlyStopping(\"val_loss\", patience=2, verbose=0, mode='auto')\n",
    "        check_point = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "        callbacks_list = [check_point]\n",
    "\n",
    "        # Generators\n",
    "        training_generator = XX_DataGenerator(partition_train, train_df, batch_size, 300000)\n",
    "        validation_generator = XX_DataGenerator(partition_validation, train_df, batch_size, 300000)\n",
    "\n",
    "        self.autoencoder.fit_generator(generator=training_generator,\n",
    "                                       validation_data=validation_generator,\n",
    "                                       use_multiprocessing=True,\n",
    "                                       epochs=max_epoch,\n",
    "                                       workers=6,\n",
    "                                       callbacks=callbacks_list)\n",
    "        self.autoencoder.save(self.autoencoder_name)\n",
    "        self.encoder.save(self.encoder_name)\n",
    "\n",
    "    def predict(self, test_df, batch_size=32):\n",
    "        testing_generator = XX_DataGenerator(range(len(test_df)), test_df, batch_size, 300000)\n",
    "        encoder_output = self.encoder.predict_generator(generator=testing_generator, max_queue_size=10, workers=6,\n",
    "                                                        use_multiprocessing=True, verbose=1)\n",
    "        return encoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400, 750, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 400, 750, 16)      272       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16, 30, 1)         17        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 30, 16)        272       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 400, 750, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 400, 750, 1)       257       \n",
      "=================================================================\n",
      "Total params: 818\n",
      "Trainable params: 818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Length of the train:  234963\n",
      "Length of the validation:  12367\n",
      "Epoch 1/1\n",
      " 4738/14685 [========>.....................] - ETA: 1:12:19 - loss: 0.0052"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"<ipython-input-10-74e84ab83b36>\", line 40, in __getitem__\n",
      "    X, X = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-10-74e84ab83b36>\", line 60, in __data_generation\n",
      "    in_file = open(file_path, 'rb')\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"<ipython-input-10-74e84ab83b36>\", line 40, in __getitem__\n",
      "    X, X = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-10-74e84ab83b36>\", line 60, in __data_generation\n",
      "    in_file = open(file_path, 'rb')\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"<ipython-input-10-74e84ab83b36>\", line 40, in __getitem__\n",
      "    X, X = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-10-74e84ab83b36>\", line 40, in __getitem__\n",
      "    X, X = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-10-74e84ab83b36>\", line 61, in __data_generation\n",
      "    in_file.seek(item['pointerto_raw_data'])\n",
      "  File \"<ipython-input-10-74e84ab83b36>\", line 40, in __getitem__\n",
      "    X, X = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-10-74e84ab83b36>\", line 63, in __data_generation\n",
      "    bytes_data = [int(single_byte) for single_byte in in_file.read(max_length)]\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-10-74e84ab83b36>\", line 60, in __data_generation\n",
      "    in_file = open(file_path, 'rb')\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/queues.py\", line 341, in put\n",
      "    obj = ForkingPickler.dumps(obj)\n",
      "  File \"/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/multiprocessing/reduction.py\", line 50, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Autoencoder('autoencoder.h5', 'encoder.h5', (400, 750, 1,), 16, 4, 25, 1)\n",
    "autoencoder.train(x_train, max_epoch=1, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
