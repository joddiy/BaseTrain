{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# specify which GPU will be used\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import os\n",
    "\n",
    "import pymysql\n",
    "from warnings import filterwarnings\n",
    "\n",
    "_connection = None\n",
    "\n",
    "def get_connection(db_config):\n",
    "    \"\"\"\n",
    "    get db connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        _connection = pymysql.connect(host=db_config['host'], user=db_config['username'],\n",
    "                                      password=db_config['password'],\n",
    "                                      db=db_config['db'], charset=\"utf8\")\n",
    "        filterwarnings('ignore', category=pymysql.Warning)\n",
    "\n",
    "    return _connection\n",
    "\n",
    "\n",
    "def close():\n",
    "    \"\"\"\n",
    "    close DB connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is not None:\n",
    "        _connection.close()\n",
    "    _connection = None\n",
    "    \n",
    "db = {\n",
    "    'host': '172.26.187.242',\n",
    "    'username': 'malware_r',\n",
    "    'password': 'GEg22v2O7jbfWhb3',\n",
    "    'db': 'malware'\n",
    "}\n",
    "\n",
    "import time\n",
    "\n",
    "# the base function which can query sql and return dict data\n",
    "def get_specific_data(table_suffix, sql=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        raise Exception(\"please init db connect first\")\n",
    "\n",
    "    cursor = _connection.cursor()\n",
    "    cursor.execute(\"SET NAMES utf8mb4\")\n",
    "\n",
    "    ret = []\n",
    "        \n",
    "    cursor.execute(sql)\n",
    "\n",
    "    field_names = [i[0] for i in cursor.description]\n",
    "\n",
    "    for row in cursor:\n",
    "        temp = {}\n",
    "        for key in range(len(row)):\n",
    "            temp[field_names[key]] = row[key]\n",
    "        ret.append(temp)\n",
    "     \n",
    "    cursor.close()\n",
    "    # _connection.close()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10.190435886383057 seconds ---\n",
      "--- 9.902132987976074 seconds ---\n",
      "--- 10.309653759002686 seconds ---\n",
      "--- 9.726117134094238 seconds ---\n",
      "--- 9.86786961555481 seconds ---\n",
      "--- 9.52732515335083 seconds ---\n",
      "--- 9.774340152740479 seconds ---\n",
      "--- 9.620240449905396 seconds ---\n",
      "--- 9.892221212387085 seconds ---\n",
      "--- 9.573340892791748 seconds ---\n",
      "--- 9.217154264450073 seconds ---\n",
      "--- 9.35383939743042 seconds ---\n",
      "--- 8.708279132843018 seconds ---\n",
      "--- 8.447227716445923 seconds ---\n",
      "--- 8.636963844299316 seconds ---\n",
      "--- 8.171812295913696 seconds ---\n",
      "145496\n"
     ]
    }
   ],
   "source": [
    "close()\n",
    "res1 = []\n",
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "# Iterate all partitions of databases\n",
    "for suffix in table_suffix:\n",
    "    sql = \"\"\" \n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  a.section_name,\n",
    "  b.mw_file_suffix as mw_file_size,\n",
    "  b.mw_file_prefix as mw_file_directory,\n",
    "  b.mw_num_engines,\n",
    "  a.pointerto_raw_data,\n",
    "  a.virtual_size,\n",
    "  d.mw_em_f\n",
    "from (\n",
    "       select\n",
    "         mw_file_hash,\n",
    "         section_name,\n",
    "         pointerto_raw_data,\n",
    "         virtual_size,\n",
    "         count(1) as cnt\n",
    "       from mw_index_2017_section_%s\n",
    "       where CNT_CODE = 1 and MEM_EXECUTE = 1\n",
    "       group by mw_file_hash) a\n",
    "  inner join mw_index_2017_%s b on a.mw_file_hash = b.mw_file_hash and a.cnt = 1 and b.mw_num_engines <> -1 and\n",
    "                                  (b.mw_num_engines >= 4 or b.mw_num_engines = 0) and\n",
    "                                  b.mw_file_prefix in ('201701')\n",
    "  inner join mw_index_2017_feature_%s d on a.mw_file_hash = d.mw_file_hash\n",
    "    \"\"\" % (suffix, suffix, suffix)\n",
    "    res1.extend(get_specific_data(suffix, sql))\n",
    "close()\n",
    "print(len(res1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 9.833410739898682 seconds ---\n",
      "--- 9.480260372161865 seconds ---\n",
      "--- 9.266990184783936 seconds ---\n",
      "--- 9.381102800369263 seconds ---\n",
      "--- 9.636274099349976 seconds ---\n",
      "--- 9.324761629104614 seconds ---\n",
      "--- 9.366666793823242 seconds ---\n",
      "--- 9.214855432510376 seconds ---\n",
      "--- 9.628302335739136 seconds ---\n",
      "--- 9.39831018447876 seconds ---\n",
      "--- 8.771495580673218 seconds ---\n",
      "--- 9.058997631072998 seconds ---\n",
      "--- 8.663572549819946 seconds ---\n",
      "--- 8.181586265563965 seconds ---\n",
      "--- 8.561010837554932 seconds ---\n",
      "--- 8.02829122543335 seconds ---\n",
      "138446\n"
     ]
    }
   ],
   "source": [
    "close()\n",
    "res2 = []\n",
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "# Iterate all partitions of databases\n",
    "for suffix in table_suffix:\n",
    "    sql = \"\"\" \n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  a.section_name,\n",
    "  b.mw_file_suffix as mw_file_size,\n",
    "  b.mw_file_prefix as mw_file_directory,\n",
    "  b.mw_num_engines,\n",
    "  a.pointerto_raw_data,\n",
    "  a.virtual_size,\n",
    "  d.mw_em_f\n",
    "from (\n",
    "       select\n",
    "         mw_file_hash,\n",
    "         section_name,\n",
    "         pointerto_raw_data,\n",
    "         virtual_size,\n",
    "         count(1) as cnt\n",
    "       from mw_index_2017_section_%s\n",
    "       where CNT_CODE = 1 and MEM_EXECUTE = 1\n",
    "       group by mw_file_hash) a\n",
    "  inner join mw_index_2017_%s b on a.mw_file_hash = b.mw_file_hash and a.cnt = 1 and b.mw_num_engines <> -1 and\n",
    "                                  (b.mw_num_engines >= 4 or b.mw_num_engines = 0) and\n",
    "                                  b.mw_file_prefix in ('201703')\n",
    "  inner join mw_index_2017_feature_%s d on a.mw_file_hash = d.mw_file_hash\n",
    "    \"\"\" % (suffix, suffix, suffix)\n",
    "    res2.extend(get_specific_data(suffix, sql))\n",
    "close()\n",
    "print(len(res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_length = 300000\n",
    "\n",
    "train_data = pd.DataFrame(res1)\n",
    "# train_data = train_data.loc[train_data.virtual_size <= max_length]\n",
    "# train_data = train_data.reset_index(drop=True)\n",
    "train_data.mw_num_engines[train_data.mw_num_engines == 0 ] = 0\n",
    "train_data.mw_num_engines[train_data.mw_num_engines >= 4 ] = 1\n",
    "train_label = train_data.mw_num_engines.ravel()\n",
    "\n",
    "test_data = pd.DataFrame(res2)\n",
    "# test_data = test_data.loc[test_data.virtual_size <= max_length]\n",
    "# test_data = test_data.reset_index(drop=True)\n",
    "test_data.mw_num_engines[test_data.mw_num_engines == 0 ] = 0\n",
    "test_data.mw_num_engines[test_data.mw_num_engines >= 4 ] = 1\n",
    "test_label = test_data.mw_num_engines.ravel()\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data, train_label, test_size=0.1, random_state=2345)\n",
    "x_test = test_data\n",
    "y_test = test_label\n",
    "\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "x_val = x_val.reset_index(drop=True)\n",
    "x_test = x_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "\n",
    "    def __init__(self, list_IDs, datasets, labels, batch_size=32, dim=8192, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.datasets = datasets\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size, self.dim), dtype=float)\n",
    "        y = np.zeros(self.batch_size, dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            base_path = \"/ssd/2017/{0}/{1}{2}\"\n",
    "            item = self.datasets.loc[ID]\n",
    "            file_path = base_path.format(item[\"mw_file_directory\"], item[\"mw_file_hash\"], item[\"mw_file_size\"])\n",
    "            in_file = open(file_path, 'rb')\n",
    "            in_file.seek(item['pointerto_raw_data'])\n",
    "            if item['virtual_size'] - max_length - 1 > 0:\n",
    "                start_index = np.random.randint(0, item['virtual_size'] - max_length - 1)\n",
    "                in_file.seek(start_index)\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(max_length)]\n",
    "            else:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(item['virtual_size'])]\n",
    "            X[i, 0:len(bytes_data)] = bytes_data\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 10240)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 10240, 8)     1024        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 10240, 8)     0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 10240, 32)    36896       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 10240, 32)    36896       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_17 (Multiply)          (None, 10240, 32)    0           conv1d_33[0][0]                  \n",
      "                                                                 conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 10240, 32)    0           multiply_17[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 10240, 32)    131104      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 10240, 32)    131104      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_18 (Multiply)          (None, 10240, 32)    0           conv1d_35[0][0]                  \n",
      "                                                                 conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_15 (Maximum)            (None, 10240, 32)    0           multiply_18[0][0]                \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 10240, 32)    0           maximum_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 10240, 32)    114720      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 10240, 32)    114720      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, 10240, 32)    0           conv1d_37[0][0]                  \n",
      "                                                                 conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_16 (Maximum)            (None, 10240, 32)    0           multiply_19[0][0]                \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 10240, 32)    0           maximum_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 10240, 32)    98336       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 10240, 32)    98336       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_20 (Multiply)          (None, 10240, 32)    0           conv1d_39[0][0]                  \n",
      "                                                                 conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_17 (Maximum)            (None, 10240, 32)    0           multiply_20[0][0]                \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 10240, 32)    0           maximum_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 10240, 32)    81952       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 10240, 32)    81952       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_21 (Multiply)          (None, 10240, 32)    0           conv1d_41[0][0]                  \n",
      "                                                                 conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_18 (Maximum)            (None, 10240, 32)    0           multiply_21[0][0]                \n",
      "                                                                 activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 10240, 32)    0           maximum_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 10240, 32)    65568       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 10240, 32)    65568       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_22 (Multiply)          (None, 10240, 32)    0           conv1d_43[0][0]                  \n",
      "                                                                 conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_19 (Maximum)            (None, 10240, 32)    0           multiply_22[0][0]                \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 10240, 32)    0           maximum_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 10240, 32)    49184       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 10240, 32)    49184       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_23 (Multiply)          (None, 10240, 32)    0           conv1d_45[0][0]                  \n",
      "                                                                 conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_20 (Maximum)            (None, 10240, 32)    0           multiply_23[0][0]                \n",
      "                                                                 activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 10240, 32)    0           maximum_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 10240, 32)    32800       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 10240, 32)    32800       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_24 (Multiply)          (None, 10240, 32)    0           conv1d_47[0][0]                  \n",
      "                                                                 conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_21 (Maximum)            (None, 10240, 32)    0           multiply_24[0][0]                \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 32)           0           maximum_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          global_max_pooling1d_3[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 1,222,177\n",
      "Trainable params: 1,222,177\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Length of the train:  130946\n",
      "Length of the validation:  14550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ProgbarLogger' object has no attribute 'log_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4aae410ec3b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m                              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                              \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                              callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    227\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ProgbarLogger' object has no attribute 'log_values'"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras import Input\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, Conv1D, Multiply, GlobalMaxPooling1D, concatenate, Dropout, Maximum, Flatten, Activation\n",
    "from keras.models import load_model\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "max_len = max_length\n",
    "time = time.time()\n",
    "\n",
    "summary = {\n",
    "    'batch_size': 16,\n",
    "    'epochs': 100,\n",
    "}\n",
    "\n",
    "def get_p(key):\n",
    "    return summary[key]\n",
    "\n",
    "def res_dilated_block_1(filters, dilation_rate, filter_size):\n",
    "    def layer(input_):\n",
    "        \n",
    "        input_ = Activation('relu')(input_)\n",
    "        conv_1 = Conv1D(filters, filter_size, padding='same')(input_)\n",
    "        conv_2 = Conv1D(filters, filter_size, dilation_rate=dilation_rate, padding='causal',\n",
    "                        activation='sigmoid')(input_)\n",
    "        \n",
    "        merged = Multiply()([conv_1, conv_2])\n",
    "        if dilation_rate > 1:\n",
    "            out = Maximum()([merged, input_])\n",
    "        else:\n",
    "            out = merged\n",
    "        return out, conv_1\n",
    "    return layer\n",
    "\n",
    "def get_model(max_features):\n",
    "\n",
    "    input = Input(shape=(max_len,))\n",
    "    seq = Embedding(128, 8, input_length=max_len)(input)\n",
    "    res_layers = []\n",
    "    for i in range(8):\n",
    "        seq, curr_layer = res_dilated_block_1(32, 2**i, ((8-i)+1)*16)(seq)\n",
    "        res_layers.append(curr_layer)\n",
    "\n",
    "    seq = GlobalMaxPooling1D()(seq)\n",
    "#     seq = Dense(max_features)(seq)\n",
    "    seq = Dense(1, activation='sigmoid')(seq)\n",
    "    model = Model(input=input, output=seq)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "if __name__ == '__main__':\n",
    "    batch_size = get_p(\"batch_size\")\n",
    "    epochs = get_p(\"epochs\")\n",
    "\n",
    "    model = get_model(1024)\n",
    "\n",
    "    print('Length of the train: ', len(x_train))\n",
    "    print('Length of the validation: ', len(x_val))\n",
    "\n",
    "    #         tensor_board = TensorBoard(log_dir='./logs/', batch_size=batch_size)\n",
    "    file_path = \"/home/zhaoqi/BaseTrain/models/\"+ str(time) +\"-{epoch:04d}-{val_loss:.5f}-{val_acc:.5f}.h5\"\n",
    "#     early_stopping = EarlyStopping(\"val_loss\", patience=3, verbose=0, mode='auto')\n",
    "    check_point = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=False, mode='auto')\n",
    "    callbacks_list = [check_point]\n",
    "\n",
    "    # Generators\n",
    "    training_generator = DataGenerator(range(10), x_train, y_train, batch_size, max_len)\n",
    "    validation_generator = DataGenerator(range(10), x_val, y_val, batch_size, max_len)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])\n",
    "    \n",
    "    model.fit_generator(generator=training_generator,\n",
    "                             validation_data=validation_generator,\n",
    "                             use_multiprocessing=True,\n",
    "                             epochs=epochs,\n",
    "                             workers=6,\n",
    "                             callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8652/8652 [==============================] - 638s 74ms/step\n"
     ]
    }
   ],
   "source": [
    "model_dir = '/home/zhaoqi/BaseTrain/models/'\n",
    "f_name = '1532664311.622236-0004-0.20003-0.92244.h5'\n",
    "c_model = load_model(model_dir + f_name)\n",
    "\n",
    "test_generator = DataGenerator(range(len(x_test)), x_test, y_test, batch_size, max_length, False)\n",
    "y_pred = model.predict_generator(generator=test_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8652/8652 [==============================] - 740s 86ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model.predict_generator(generator=test_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8652/8652 [==============================] - 785s 91ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred3 = model.predict_generator(generator=test_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, confusion_matrix\n",
    "\n",
    "def estimate_model(y_pred, test_y):\n",
    "    \n",
    "    loss = log_loss(test_y, y_pred)\n",
    "    auc = roc_auc_score(test_y, y_pred)\n",
    "    acc = accuracy_score(test_y, (y_pred > 0.5).astype(int))\n",
    "    print(\"loss : %.5f\" % loss)\n",
    "    print(\"auc score : %.5f\" % auc)\n",
    "    print(\"accuracy score : %.5f\" % acc)\n",
    "\n",
    "    fp_np_index = np.where(test_y == 0)\n",
    "    fp_np = y_pred[fp_np_index].shape[0]\n",
    "    thre_index = int(np.ceil(fp_np - fp_np * 0.001))\n",
    "\n",
    "    sorted_pred_prob = np.sort(y_pred[fp_np_index], axis=0)\n",
    "    thre = sorted_pred_prob[thre_index]\n",
    "    if thre == 1:\n",
    "        thre = max(sorted_pred_prob[np.where(sorted_pred_prob != 1)])\n",
    "\n",
    "    y_pred_prob = np.vstack((y_pred.transpose(), (1 - y_pred).transpose())).transpose()\n",
    "    y_pred_prob[:, 1] = thre\n",
    "    y_pred_label = np.argmin(y_pred_prob, axis=-1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(test_y, y_pred_label).ravel()\n",
    "    fp_rate = fp / (fp + tn)\n",
    "    recall_rate = tp / (tp + fn)\n",
    "\n",
    "    print(\"thre: %.10f\"%  thre)\n",
    "    print(\"fp:  %.10f\"%  fp_rate)\n",
    "    print(\"recall:  %.10f\"%  recall_rate)\n",
    "    \n",
    "    return auc, loss, recall_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : nan\n",
      "auc score : 0.90337\n",
      "accuracy score : 0.86634\n",
      "thre: 0.9999341965\n",
      "fp:  0.0009204148\n",
      "recall:  0.4297299155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9033727002444281, nan, 0.42972991546972716)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_model(y_pred, y_test[0:len(y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : nan\n",
      "auc score : 0.90337\n",
      "accuracy score : 0.86634\n",
      "thre: 0.9999341965\n",
      "fp:  0.0009204148\n",
      "recall:  0.4297299155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9033727002444281, nan, 0.42972991546972716)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_model(y_pred, y_test[0:len(y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : nan\n",
      "auc score : 0.89713\n",
      "accuracy score : 0.82644\n",
      "thre: 0.9999998808\n",
      "fp:  0.0051178190\n",
      "recall:  0.0383385440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8971331337134518, nan, 0.038338543992853955)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_model(y_pred, y_test[0:len(y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.stack((y_pred, y_pred2, y_pred3))\n",
    "abc = np.amax(test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01006087],\n",
       "       [0.04471654],\n",
       "       [0.21484901],\n",
       "       ...,\n",
       "       [0.00375033],\n",
       "       [0.00635988],\n",
       "       [0.97017777]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : nan\n",
      "auc score : 0.90108\n",
      "accuracy score : 0.82321\n",
      "thre: 0.9999998808\n",
      "fp:  0.0102235104\n",
      "recall:  0.0513622153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.901083696211892, nan, 0.051362215274676194)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_model(abc, y_test[0:len(y_pred)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
