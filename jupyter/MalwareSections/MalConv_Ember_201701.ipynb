{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pymysql\n",
    "from warnings import filterwarnings\n",
    "\n",
    "_connection = None\n",
    "\n",
    "def get_connection(db_config):\n",
    "    \"\"\"\n",
    "    get db connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        _connection = pymysql.connect(host=db_config['host'], user=db_config['username'],\n",
    "                                      password=db_config['password'],\n",
    "                                      db=db_config['db'], charset=\"utf8\")\n",
    "        filterwarnings('ignore', category=pymysql.Warning)\n",
    "\n",
    "    return _connection\n",
    "\n",
    "\n",
    "def close():\n",
    "    \"\"\"\n",
    "    close DB connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is not None:\n",
    "        _connection.close()\n",
    "    _connection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {\n",
    "    'host': '172.26.187.242',\n",
    "    'username': 'malware_r',\n",
    "    'password': 'GEg22v2O7jbfWhb3',\n",
    "    'db': 'malware'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_get(table, condition, columns={\"*\"}, limit=None):\n",
    "    \"\"\"\n",
    "    common get data function\n",
    "    :param table:\n",
    "    :param condition:\n",
    "    :param columns:\n",
    "    :param limit:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        raise Exception(\"please init db connect first\")\n",
    "\n",
    "    cursor = _connection.cursor()\n",
    "    cursor.execute(\"SET NAMES utf8mb4\")\n",
    "\n",
    "    ret = []\n",
    "\n",
    "    sql = \"SELECT %s FROM {table} WHERE 1\" % \", \".join(columns)\n",
    "    for item in condition:\n",
    "        sql += \" AND \" + item\n",
    "\n",
    "    sql = sql.format(table=table)\n",
    "    if limit is not None:\n",
    "        sql += \" LIMIT \" + str(limit)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "    field_names = [i[0] for i in cursor.description]\n",
    "\n",
    "    for row in cursor:\n",
    "        temp = {}\n",
    "        for key in range(len(row)):\n",
    "            temp[field_names[key]] = row[key]\n",
    "        ret.append(temp)\n",
    "\n",
    "    cursor.close()\n",
    "    # _connection.close()\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_specific_data(table_suffix):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        raise Exception(\"please init db connect first\")\n",
    "\n",
    "    cursor = _connection.cursor()\n",
    "    cursor.execute(\"SET NAMES utf8mb4\")\n",
    "\n",
    "    ret = []\n",
    "    \n",
    "    sql = \"\"\"\n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  c.mw_file_prefix as mw_file_directory,\n",
    "  c.mw_file_suffix as mw_file_size,\n",
    "  c.mw_num_engines,\n",
    "  b.pointerto_raw_data,\n",
    "  b.virtual_size\n",
    "from (select\n",
    "        mw_file_hash,\n",
    "        section_name,\n",
    "        count(1)\n",
    "      from mw_index_2017_section_%s\n",
    "      where section_name = '.text' and pointerto_raw_data <> 0\n",
    "      group by mw_file_hash, section_name) as a right join mw_index_2017_section_%s as b\n",
    "    on a.mw_file_hash = b.mw_file_hash\n",
    "  right join mw_index_2017_%s c on a.mw_file_hash = c.mw_file_hash\n",
    "where b.section_name = '.text' and (c.mw_num_engines >= 8 or c.mw_num_engines = 0) and c.mw_file_directory = '201701';\n",
    "    \"\"\" % (table_suffix, table_suffix, table_suffix)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "    field_names = [i[0] for i in cursor.description]\n",
    "\n",
    "    for row in cursor:\n",
    "        temp = {}\n",
    "        for key in range(len(row)):\n",
    "            temp[field_names[key]] = row[key]\n",
    "        ret.append(temp)\n",
    "    \n",
    "    cursor.close()\n",
    "    # _connection.close()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "(1054, \"Unknown column 'c.mw_file_directory' in 'where clause'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6411a1dc1c8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_suffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_specific_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-25ce5f25add1>\u001b[0m in \u001b[0;36mget_specific_data\u001b[0;34m(table_suffix)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msection_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.text'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmw_num_engines\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmw_num_engines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmw_file_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'201701'\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\" % (table_suffix, table_suffix, table_suffix)\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mfield_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pymysql/cursors.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, query, args)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmogrify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pymysql/cursors.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_executed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrowcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, sql, unbuffered)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0msql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogateescape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOMMAND\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOM_QUERY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_affected_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_query_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munbuffered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munbuffered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_affected_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m_read_query_result\u001b[0;34m(self, unbuffered)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMySQLResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_status\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m             \u001b[0mfirst_packet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst_packet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ok_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m_read_packet\u001b[0;34m(self, packet_type)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mpacket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacket_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m         \u001b[0mpacket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpacket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pymysql/protocol.py\u001b[0m in \u001b[0;36mcheck_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0merrno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_uint16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"errno =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_mysql_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/pymysql/err.py\u001b[0m in \u001b[0;36mraise_mysql_exception\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0merrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0merrorclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInternalError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrorclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m: (1054, \"Unknown column 'c.mw_file_directory' in 'where clause'\")"
     ]
    }
   ],
   "source": [
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "res = []\n",
    "for suffix in table_suffix:\n",
    "    res.extend(get_specific_data(suffix))\n",
    "close()\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "\n",
    "clean_data = data.loc[data.virtual_size <= 300000]\n",
    "clean_data = clean_data.reset_index(drop=True)\n",
    "\n",
    "print(clean_data.shape)\n",
    "\n",
    "h = sorted(clean_data.virtual_size.ravel())  #sorted\n",
    "\n",
    "fit = stats.norm.pdf(h, np.mean(h), np.std(h))  #this is a fitting indeed\n",
    "\n",
    "pl.plot(h,fit,'-o')\n",
    "\n",
    "pl.hist(h,normed=True)      #use this to draw histogram of your data\n",
    "\n",
    "pl.show()                   #use may also need add this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.mw_num_engines[clean_data.mw_num_engines != 0] = 1\n",
    "label = clean_data.mw_num_engines.ravel()\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(clean_data, label, test_size=0.05,\n",
    "                                                    random_state=5242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(x_train.virtual_size.ravel())\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reset_index(drop=True)\n",
    "x_test = x_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mal Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "\n",
    "    def __init__(self, list_IDs, datasets, labels, batch_size=32, dim=8192, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.datasets = datasets\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size, self.dim), dtype=float)\n",
    "        y = np.zeros(self.batch_size, dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            base_path = \"/malware_data_2017/{0}/{1}{2}\"\n",
    "            item = self.datasets.loc[ID]\n",
    "            file_path = base_path.format(item[\"mw_file_directory\"], item[\"mw_file_hash\"], item[\"mw_file_size\"])\n",
    "            in_file = open(file_path, 'rb')\n",
    "            in_file.seek(item['pointerto_raw_data'])\n",
    "            bytes_data = [int(single_byte) for single_byte in in_file.read(item['virtual_size'])]\n",
    "            X[i, 0:len(bytes_data)] = bytes_data\n",
    "            y[i] = self.labels[ID]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras import Input\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, Conv1D, Multiply, GlobalMaxPooling1D\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TMalConv(object):\n",
    "    \"\"\"\n",
    "    train of mal conv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train_df = x_train\n",
    "        self.label_df = y_train\n",
    "        self.v_x = None\n",
    "        self.v_y = None\n",
    "        self.max_len = max_length\n",
    "        self.history = None\n",
    "        self.model = None\n",
    "        self.p_md5 = None\n",
    "        self.summary = {\n",
    "            'time':time.time(),\n",
    "            'batch_size': 16,\n",
    "            'epochs': 6,\n",
    "            's_test_size': 0.05,\n",
    "            's_random_state': 5242,\n",
    "            'g_c_filter': 128,\n",
    "            'g_c_kernel_size': 500,\n",
    "            'g_c_stride': 500,\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        \n",
    "    def get_p(self, key):\n",
    "        \"\"\"\n",
    "        get the parameter from the summary\n",
    "        :param key:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.summary[key]\n",
    "\n",
    "    def gate_cnn(self, gate_cnn_input):\n",
    "        \"\"\"\n",
    "        construct a gated cnn by the specific kernel size\n",
    "        :param gate_cnn_input:\n",
    "        :param kernel_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        conv1_out = Conv1D(self.get_p(\"g_c_filter\"), self.get_p(\"g_c_kernel_size\"), strides=self.get_p(\"g_c_stride\"))(\n",
    "            gate_cnn_input)\n",
    "        conv2_out = Conv1D(self.get_p(\"g_c_filter\"), self.get_p(\"g_c_kernel_size\"), strides=self.get_p(\"g_c_stride\"),\n",
    "                           activation=\"sigmoid\")(gate_cnn_input)\n",
    "        merged = Multiply()([conv1_out, conv2_out])\n",
    "        gate_cnn_output = GlobalMaxPooling1D()(merged)\n",
    "        return gate_cnn_output\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        get a model\n",
    "        :param max_len:\n",
    "        :param kernel_sizes:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        net_input = Input(shape=(self.max_len,))\n",
    "\n",
    "        embedding_out = Embedding(256, 8, input_length=self.max_len)(net_input)\n",
    "        merged = self.gate_cnn(embedding_out)\n",
    "\n",
    "        dense_out = Dense(128)(merged)\n",
    "        net_output = Dense(1, activation='sigmoid')(dense_out)\n",
    "\n",
    "        model = keras.models.Model(inputs=net_input, outputs=net_output)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        batch_size = self.get_p(\"batch_size\")\n",
    "        epochs = self.get_p(\"epochs\")\n",
    "\n",
    "        self.model = self.get_model()\n",
    "\n",
    "        partition_train, partition_validation = train_test_split(range(len(self.train_df)), test_size=0.05)\n",
    "        print('Length of the train: ', len(partition_train))\n",
    "        print('Length of the validation: ', len(partition_validation))\n",
    "        \n",
    "        tensor_board = TensorBoard(log_dir='./logs/', batch_size=batch_size)\n",
    "        file_path = \"./models/{epoch:04d}-{val_loss:.5f}-{val_acc:.5f}.h5\"\n",
    "        check_point = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=False, mode='auto')\n",
    "        callbacks_list = [tensor_board, check_point]\n",
    "\n",
    "        # Generators\n",
    "        training_generator = DataGenerator(partition_train, self.train_df, self.label_df, batch_size, self.max_len)\n",
    "        validation_generator = DataGenerator(partition_validation, self.train_df, self.label_df, batch_size, self.max_len)\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                           optimizer='adam',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        self.model.fit_generator(generator=training_generator,\n",
    "                                 validation_data=validation_generator,\n",
    "                                 use_multiprocessing=True,\n",
    "                                 epochs=epochs,\n",
    "                                 workers=6,\n",
    "                                 callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "t_instance = TMalConv()\n",
    "t_instance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/home/zhaoqi/BaseTrain/models/'\n",
    "f_name = '0005-0.11288-0.96264.h5'\n",
    "c_model = load_model(model_dir + f_name)\n",
    "test_generator = DataGenerator(range(len(x_test)), x_test, y_test, 16, max_length, False)\n",
    "y_pred = c_model.predict_generator(generator=test_generator, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, confusion_matrix\n",
    "\n",
    "def estimate_model(y_pred, test_y):\n",
    "    \n",
    "    loss = log_loss(test_y, y_pred)\n",
    "    auc = roc_auc_score(test_y, y_pred)\n",
    "    acc = accuracy_score(test_y, (y_pred > 0.5).astype(int))\n",
    "    print(\"loss : %.5f\" % loss)\n",
    "    print(\"auc score : %.5f\" % auc)\n",
    "    print(\"accuracy score : %.5f\" % acc)\n",
    "\n",
    "    fp_np_index = np.where(test_y == 0)\n",
    "    fp_np = y_pred[fp_np_index].shape[0]\n",
    "    thre_index = int(np.ceil(fp_np - fp_np * 0.001))\n",
    "\n",
    "    sorted_pred_prob = np.sort(y_pred[fp_np_index], axis=0)\n",
    "    thre = sorted_pred_prob[thre_index]\n",
    "    if thre == 1:\n",
    "        thre = max(sorted_pred_prob[np.where(sorted_pred_prob != 1)])\n",
    "\n",
    "    y_pred_prob = np.vstack((y_pred.transpose(), (1 - y_pred).transpose())).transpose()\n",
    "    y_pred_prob[:, 1] = thre\n",
    "    y_pred_label = np.argmin(y_pred_prob, axis=-1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(test_y, y_pred_label).ravel()\n",
    "    fp_rate = fp / (fp + tn)\n",
    "    recall_rate = tp / (tp + fn)\n",
    "\n",
    "    print(\"thre: %.5f\"%  thre)\n",
    "    print(\"fp:  %.5f\"%  fp_rate)\n",
    "    print(\"recall:  %.5f\"%  recall_rate)\n",
    "    \n",
    "    return auc, loss, recall_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_model(y_pred, y_test[0:len(y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import lief\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureType(object):\n",
    "    ''' Base class from which each feature type may inherit '''\n",
    "\n",
    "    name = ''\n",
    "    dim = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.name, self.dim)\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        ''' Generate a JSON-able representation of the file '''\n",
    "        raise (NotImplemented)\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        ''' Generate a feature vector from the raw features '''\n",
    "        raise (NotImplemented)\n",
    "\n",
    "    def feature_vector(self, bytez, lief_binary):\n",
    "        ''' Directly calculate the feature vector from the sample itself. This should only be implemented differently\n",
    "        if there are significant speedups to be gained from combining the two functions. '''\n",
    "        return self.process_raw_features(self.raw_features(bytez, lief_binary))\n",
    "\n",
    "\n",
    "class ByteHistogram(FeatureType):\n",
    "    ''' Byte histogram (count + non-normalized) over the entire binary file '''\n",
    "\n",
    "    name = 'histogram'\n",
    "    dim = 256\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        counts = np.bincount(np.frombuffer(bytez, dtype=np.uint8), minlength=256)\n",
    "        return counts.tolist()\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        counts = np.array(raw_obj, dtype=np.float32)\n",
    "        sum = counts.sum()\n",
    "        normalized = counts / sum\n",
    "        return normalized\n",
    "\n",
    "\n",
    "class ByteEntropyHistogram(FeatureType):\n",
    "    ''' 2d byte/entropy histogram based loosely on (Saxe and Berlin, 2015).\n",
    "    This roughly approximates the joint probability of byte value and local entropy.\n",
    "    See Section 2.1.1 in https://arxiv.org/pdf/1508.03096.pdf for more info.\n",
    "    '''\n",
    "\n",
    "    name = 'byteentropy'\n",
    "    dim = 256\n",
    "\n",
    "    def __init__(self, step=1024, window=2048):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self.window = window\n",
    "        self.step = step\n",
    "\n",
    "    def _entropy_bin_counts(self, block):\n",
    "        # coarse histogram, 16 bytes per bin\n",
    "        c = np.bincount(block >> 4, minlength=16)  # 16-bin histogram\n",
    "        p = c.astype(np.float32) / self.window\n",
    "        wh = np.where(c)[0]\n",
    "        H = np.sum(-p[wh] * np.log2(\n",
    "            p[wh])) * 2  # * x2 b.c. we reduced information by half: 256 bins (8 bits) to 16 bins (4 bits)\n",
    "\n",
    "        Hbin = int(H * 2)  # up to 16 bins (max entropy is 8 bits)\n",
    "        if Hbin == 16:  # handle entropy = 8.0 bits\n",
    "            Hbin = 15\n",
    "\n",
    "        return Hbin, c\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        output = np.zeros((16, 16), dtype=np.int)\n",
    "        a = np.frombuffer(bytez, dtype=np.uint8)\n",
    "        if a.shape[0] < self.window:\n",
    "            Hbin, c = self._entropy_bin_counts(a)\n",
    "            output[Hbin, :] += c\n",
    "        else:\n",
    "            # strided trick from here: http://www.rigtorp.se/2011/01/01/rolling-statistics-numpy.html\n",
    "            shape = a.shape[:-1] + (a.shape[-1] - self.window + 1, self.window)\n",
    "            strides = a.strides + (a.strides[-1],)\n",
    "            blocks = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[::self.step, :]\n",
    "\n",
    "            # from the blocks, compute histogram\n",
    "            for block in blocks:\n",
    "                Hbin, c = self._entropy_bin_counts(block)\n",
    "                output[Hbin, :] += c\n",
    "\n",
    "        return output.flatten().tolist()\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        counts = np.array(raw_obj, dtype=np.float32)\n",
    "        sum = counts.sum()\n",
    "        normalized = counts / sum\n",
    "        return normalized\n",
    "\n",
    "\n",
    "class SectionInfo(FeatureType):\n",
    "    ''' Information about section names, sizes and entropy.  Uses hashing trick\n",
    "    to summarize all this section info into a feature vector.\n",
    "    '''\n",
    "\n",
    "    name = 'section'\n",
    "    dim = 5 + 50 + 50 + 50 + 50 + 50\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def _properties(s):\n",
    "        return [str(c).split('.')[-1] for c in s.characteristics_lists]\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return {\"entry\": \"\", \"sections\": []}\n",
    "\n",
    "        # properties of entry point, or if invalid, the first executable section\n",
    "        try:\n",
    "            entry_section = lief_binary.section_from_offset(lief_binary.entrypoint).name\n",
    "        except lief.not_found:\n",
    "            # bad entry point, let's find the first executable section\n",
    "            entry_section = \"\"\n",
    "            for s in lief_binary.sections:\n",
    "                if lief.PE.SECTION_CHARACTERISTICS.MEM_EXECUTE in s.characteristics_lists:\n",
    "                    entry_section = s.name\n",
    "                    break\n",
    "\n",
    "        raw_obj = {\"entry\": entry_section}\n",
    "        raw_obj[\"sections\"] = [{\n",
    "            'name': s.name,\n",
    "            'size': s.size,\n",
    "            'entropy': s.entropy,\n",
    "            'vsize': s.virtual_size,\n",
    "            'props': self._properties(s)\n",
    "        } for s in lief_binary.sections]\n",
    "        return raw_obj\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        sections = raw_obj['sections']\n",
    "        general = [\n",
    "            len(sections),  # total number of sections\n",
    "            # number of sections with nonzero size\n",
    "            sum(1 for s in sections if s['size'] == 0),\n",
    "            # number of sections with an empty name\n",
    "            sum(1 for s in sections if s['name'] == \"\"),\n",
    "            # number of RX\n",
    "            sum(1 for s in sections if 'MEM_READ' in s['props'] and 'MEM_EXECUTE' in s['props']),\n",
    "            # number of W\n",
    "            sum(1 for s in sections if 'MEM_WRITE' in s['props'])\n",
    "        ]\n",
    "        # gross characteristics of each section\n",
    "        section_sizes = [(s['name'], s['size']) for s in sections]\n",
    "        section_sizes_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_sizes]).toarray()[0]\n",
    "        section_entropy = [(s['name'], s['entropy']) for s in sections]\n",
    "        section_entropy_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_entropy]).toarray()[0]\n",
    "        section_vsize = [(s['name'], s['vsize']) for s in sections]\n",
    "        section_vsize_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_vsize]).toarray()[0]\n",
    "        entry_name_hashed = FeatureHasher(50, input_type=\"string\").transform([raw_obj['entry']]).toarray()[0]\n",
    "        characteristics = [p for s in sections for p in s['props'] if s['name'] == raw_obj['entry']]\n",
    "        characteristics_hashed = FeatureHasher(50, input_type=\"string\").transform([characteristics]).toarray()[0]\n",
    "\n",
    "        return np.hstack([\n",
    "            general, section_sizes_hashed, section_entropy_hashed, section_vsize_hashed, entry_name_hashed,\n",
    "            characteristics_hashed\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "\n",
    "class ImportsInfo(FeatureType):\n",
    "    ''' Information about imported libraries and functions from the\n",
    "    import address table.  Note that the total number of imported\n",
    "    functions is contained in GeneralFileInfo.\n",
    "    '''\n",
    "\n",
    "    name = 'imports'\n",
    "    dim = 1280\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        imports = {}\n",
    "        if lief_binary is None:\n",
    "            return imports\n",
    "\n",
    "        for lib in lief_binary.imports:\n",
    "            if lib.name not in imports:\n",
    "                imports[lib.name] = []  # libraries can be duplicated in listing, extend instead of overwrite\n",
    "\n",
    "            # Clipping assumes there are diminishing returns on the discriminatory power of imported functions\n",
    "            #  beyond the first 10000 characters, and this will help limit the dataset size\n",
    "            imports[lib.name].extend([entry.name[:10000] for entry in lib.entries])\n",
    "\n",
    "        return imports\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        # unique libraries\n",
    "        libraries = list(set([l.lower() for l in raw_obj.keys()]))\n",
    "        libraries_hashed = FeatureHasher(256, input_type=\"string\").transform([libraries]).toarray()[0]\n",
    "\n",
    "        # A string like \"kernel32.dll:CreateFileMappingA\" for each imported function\n",
    "        imports = [lib.lower() + ':' + e for lib, elist in raw_obj.items() for e in elist]\n",
    "        imports_hashed = FeatureHasher(1024, input_type=\"string\").transform([imports]).toarray()[0]\n",
    "\n",
    "        # Two separate elements: libraries (alone) and fully-qualified names of imported functions\n",
    "        return np.hstack([libraries_hashed, imports_hashed]).astype(np.float32)\n",
    "\n",
    "\n",
    "class ExportsInfo(FeatureType):\n",
    "    ''' Information about exported functions. Note that the total number of exported\n",
    "    functions is contained in GeneralFileInfo.\n",
    "    '''\n",
    "\n",
    "    name = 'exports'\n",
    "    dim = 128\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return []\n",
    "\n",
    "        # Clipping assumes there are diminishing returns on the discriminatory power of exports beyond\n",
    "        #  the first 10000 characters, and this will help limit the dataset size\n",
    "        clipped_exports = [export[:10000] for export in lief_binary.exported_functions]\n",
    "\n",
    "        return clipped_exports\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        exports_hashed = FeatureHasher(128, input_type=\"string\").transform([raw_obj]).toarray()[0]\n",
    "        return exports_hashed.astype(np.float32)\n",
    "\n",
    "\n",
    "class GeneralFileInfo(FeatureType):\n",
    "    ''' General information about the file '''\n",
    "\n",
    "    name = 'general'\n",
    "    dim = 10\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return {\n",
    "                'size': len(bytez),\n",
    "                'vsize': 0,\n",
    "                'has_debug': 0,\n",
    "                'exports': 0,\n",
    "                'imports': 0,\n",
    "                'has_relocations': 0,\n",
    "                'has_resources': 0,\n",
    "                'has_signature': 0,\n",
    "                'has_tls': 0,\n",
    "                'symbols': 0\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'size': len(bytez),\n",
    "            'vsize': lief_binary.virtual_size,\n",
    "            'has_debug': int(lief_binary.has_debug),\n",
    "            'exports': len(lief_binary.exported_functions),\n",
    "            'imports': len(lief_binary.imported_functions),\n",
    "            'has_relocations': int(lief_binary.has_relocations),\n",
    "            'has_resources': int(lief_binary.has_resources),\n",
    "            'has_signature': int(lief_binary.has_signature),\n",
    "            'has_tls': int(lief_binary.has_tls),\n",
    "            'symbols': len(lief_binary.symbols),\n",
    "        }\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return np.asarray(\n",
    "            [\n",
    "                raw_obj['size'], raw_obj['vsize'], raw_obj['has_debug'], raw_obj['exports'], raw_obj['imports'],\n",
    "                raw_obj['has_relocations'], raw_obj['has_resources'], raw_obj['has_signature'], raw_obj['has_tls'],\n",
    "                raw_obj['symbols']\n",
    "            ],\n",
    "            dtype=np.float32)\n",
    "\n",
    "\n",
    "class HeaderFileInfo(FeatureType):\n",
    "    ''' Machine, architecure, OS, linker and other information extracted from header '''\n",
    "\n",
    "    name = 'header'\n",
    "    dim = 62\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        raw_obj = {}\n",
    "        raw_obj['coff'] = {'timestamp': 0, 'machine': \"\", 'characteristics': []}\n",
    "        raw_obj['optional'] = {\n",
    "            'subsystem': \"\",\n",
    "            'dll_characteristics': [],\n",
    "            'magic': \"\",\n",
    "            'major_image_version': 0,\n",
    "            'minor_image_version': 0,\n",
    "            'major_linker_version': 0,\n",
    "            'minor_linker_version': 0,\n",
    "            'major_operating_system_version': 0,\n",
    "            'minor_operating_system_version': 0,\n",
    "            'major_subsystem_version': 0,\n",
    "            'minor_subsystem_version': 0,\n",
    "            'sizeof_code': 0,\n",
    "            'sizeof_headers': 0,\n",
    "            'sizeof_heap_commit': 0\n",
    "        }\n",
    "        if lief_binary is None:\n",
    "            return raw_obj\n",
    "\n",
    "        raw_obj['coff']['timestamp'] = lief_binary.header.time_date_stamps\n",
    "        raw_obj['coff']['machine'] = str(lief_binary.header.machine).split('.')[-1]\n",
    "        raw_obj['coff']['characteristics'] = [str(c).split('.')[-1] for c in lief_binary.header.characteristics_list]\n",
    "        raw_obj['optional']['subsystem'] = str(lief_binary.optional_header.subsystem).split('.')[-1]\n",
    "        raw_obj['optional']['dll_characteristics'] = [\n",
    "            str(c).split('.')[-1] for c in lief_binary.optional_header.dll_characteristics_lists\n",
    "        ]\n",
    "        raw_obj['optional']['magic'] = str(lief_binary.optional_header.magic).split('.')[-1]\n",
    "        raw_obj['optional']['major_image_version'] = lief_binary.optional_header.major_image_version\n",
    "        raw_obj['optional']['minor_image_version'] = lief_binary.optional_header.minor_image_version\n",
    "        raw_obj['optional']['major_linker_version'] = lief_binary.optional_header.major_linker_version\n",
    "        raw_obj['optional']['minor_linker_version'] = lief_binary.optional_header.minor_linker_version\n",
    "        raw_obj['optional'][\n",
    "            'major_operating_system_version'] = lief_binary.optional_header.major_operating_system_version\n",
    "        raw_obj['optional'][\n",
    "            'minor_operating_system_version'] = lief_binary.optional_header.minor_operating_system_version\n",
    "        raw_obj['optional']['major_subsystem_version'] = lief_binary.optional_header.major_subsystem_version\n",
    "        raw_obj['optional']['minor_subsystem_version'] = lief_binary.optional_header.minor_subsystem_version\n",
    "        raw_obj['optional']['sizeof_code'] = lief_binary.optional_header.sizeof_code\n",
    "        raw_obj['optional']['sizeof_headers'] = lief_binary.optional_header.sizeof_headers\n",
    "        raw_obj['optional']['sizeof_heap_commit'] = lief_binary.optional_header.sizeof_heap_commit\n",
    "        return raw_obj\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return np.hstack([\n",
    "            raw_obj['coff']['timestamp'],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['coff']['machine']]]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([raw_obj['coff']['characteristics']]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['subsystem']]]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([raw_obj['optional']['dll_characteristics']]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['magic']]]).toarray()[0],\n",
    "            raw_obj['optional']['major_image_version'],\n",
    "            raw_obj['optional']['minor_image_version'],\n",
    "            raw_obj['optional']['major_linker_version'],\n",
    "            raw_obj['optional']['minor_linker_version'],\n",
    "            raw_obj['optional']['major_operating_system_version'],\n",
    "            raw_obj['optional']['minor_operating_system_version'],\n",
    "            raw_obj['optional']['major_subsystem_version'],\n",
    "            raw_obj['optional']['minor_subsystem_version'],\n",
    "            raw_obj['optional']['sizeof_code'],\n",
    "            raw_obj['optional']['sizeof_headers'],\n",
    "            raw_obj['optional']['sizeof_heap_commit'],\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "\n",
    "class StringExtractor(FeatureType):\n",
    "    ''' Extracts strings from raw byte stream '''\n",
    "\n",
    "    name = 'strings'\n",
    "    dim = 1 + 1 + 1 + 96 + 1 + 1 + 1 + 1 + 1\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        # all consecutive runs of 0x20 - 0x7f that are 5+ characters\n",
    "        self._allstrings = re.compile(b'[\\x20-\\x7f]{5,}')\n",
    "        # occurances of the string 'C:\\'.  Not actually extracting the path\n",
    "        self._paths = re.compile(b'c:\\\\\\\\', re.IGNORECASE)\n",
    "        # occurances of http:// or https://.  Not actually extracting the URLs\n",
    "        self._urls = re.compile(b'https?://', re.IGNORECASE)\n",
    "        # occurances of the string prefix HKEY_.  No actually extracting registry names\n",
    "        self._registry = re.compile(b'HKEY_')\n",
    "        # crude evidence of an MZ header (dropper?) somewhere in the byte stream\n",
    "        self._mz = re.compile(b'MZ')\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        allstrings = self._allstrings.findall(bytez)\n",
    "        if allstrings:\n",
    "            # statistics about strings:\n",
    "            string_lengths = [len(s) for s in allstrings]\n",
    "            avlength = sum(string_lengths) / len(string_lengths)\n",
    "            # map printable characters 0x20 - 0x7f to an int array consisting of 0-95, inclusive\n",
    "            as_shifted_string = [b - ord(b'\\x20') for b in b''.join(allstrings)]\n",
    "            c = np.bincount(as_shifted_string, minlength=96)  # histogram count\n",
    "            # distribution of characters in printable strings\n",
    "            csum = c.sum()\n",
    "            p = c.astype(np.float32) / csum\n",
    "            wh = np.where(c)[0]\n",
    "            H = np.sum(-p[wh] * np.log2(p[wh]))  # entropy\n",
    "        else:\n",
    "            avlength = 0\n",
    "            c = np.zeros((96,), dtype=np.float32)\n",
    "            H = 0\n",
    "            csum = 0\n",
    "\n",
    "        return {\n",
    "            'numstrings': len(allstrings),\n",
    "            'avlength': avlength,\n",
    "            'printabledist': c.tolist(),  # store non-normalized histogram\n",
    "            'printables': int(csum),\n",
    "            'entropy': float(H),\n",
    "            'paths': len(self._paths.findall(bytez)),\n",
    "            'urls': len(self._urls.findall(bytez)),\n",
    "            'registry': len(self._registry.findall(bytez)),\n",
    "            'MZ': len(self._mz.findall(bytez))\n",
    "        }\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        hist_divisor = float(raw_obj['printables']) if raw_obj['printables'] > 0 else 1.0\n",
    "        return np.hstack([\n",
    "            raw_obj['numstrings'], raw_obj['avlength'], raw_obj['printables'],\n",
    "            np.asarray(raw_obj['printabledist']) / hist_divisor, raw_obj['entropy'], raw_obj['paths'], raw_obj['urls'],\n",
    "            raw_obj['registry'], raw_obj['MZ']\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "\n",
    "class PEFeatureExtractor(object):\n",
    "    ''' Extract useful features from a PE file, and return as a vector of fixed size. '''\n",
    "\n",
    "    features = [\n",
    "        ByteHistogram(), ByteEntropyHistogram(), StringExtractor(), GeneralFileInfo(), HeaderFileInfo(), SectionInfo(),\n",
    "        ImportsInfo(), ExportsInfo()\n",
    "    ]\n",
    "    dim = sum([fe.dim for fe in features])\n",
    "\n",
    "    def raw_features(self, bytez):\n",
    "        try:\n",
    "            lief_binary = lief.PE.parse(list(bytez))\n",
    "        except (lief.bad_format, lief.bad_file, lief.pe_error, lief.parser_error, RuntimeError) as e:\n",
    "            print(\"lief error: \", str(e))\n",
    "            lief_binary = None\n",
    "        except Exception:  # everything else (KeyboardInterrupt, SystemExit, ValueError):\n",
    "            raise\n",
    "\n",
    "        features = {\"sha256\": hashlib.sha256(bytez).hexdigest()}\n",
    "        features.update({fe.name: fe.raw_features(bytez, lief_binary) for fe in self.features})\n",
    "        return features\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        feature_vectors = [fe.process_raw_features(raw_obj[fe.name]) for fe in self.features]\n",
    "        return np.hstack(feature_vectors).astype(np.float32)\n",
    "\n",
    "    def feature_vector(self, bytez):\n",
    "        return self.process_raw_features(self.raw_features(bytez))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ember_feature(file_path):\n",
    "    \"\"\"\n",
    "    int to bytes array\n",
    "    :param data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    in_file = open(file_path, \"rb\") # opening for [r]eading as [b]inary\n",
    "    bytes_data = in_file.read() # if you only wanted to read 512 bytes, do .read(512)\n",
    "    in_file.close()\n",
    "    extractor = PEFeatureExtractor()\n",
    "    features = np.array(extractor.feature_vector(bytes_data), dtype=np.float32)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ember train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num, _ = x_train.shape\n",
    "eb_X = np.zeros((num, 2351), dtype=float)\n",
    "eb_y = np.zeros(num, dtype=float)\n",
    "\n",
    "for index, item in x_train.iterrows():\n",
    "    if index % 1000 == 0:\n",
    "        print(index)\n",
    "    base_path = \"/home/zhaoqi/input/{0}_{1}\"\n",
    "    file_path = base_path.format(item[\"mw_file_hash\"], item[\"mw_file_size\"])\n",
    "    features = get_ember_feature(file_path)\n",
    "    eb_X[index, 0:len(features)] = features\n",
    "    eb_y[index] = y_train[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(data, label):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.05, random_state=5242)\n",
    "    params = {'application': 'binary'}\n",
    "    lgbm_dataset = lgb.Dataset(x_train, y_train.ravel())\n",
    "    valid_sets = lgb.Dataset(x_test, y_test.ravel())\n",
    "\n",
    "    model = lgb.train(params, lgbm_dataset, 100000, valid_sets=valid_sets, early_stopping_rounds=10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = get_model(eb_X, eb_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ember test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num, _ = x_test.shape\n",
    "ebt_X = np.zeros((num, 2351), dtype=float)\n",
    "ebt_y = np.zeros(num, dtype=float)\n",
    "\n",
    "for index, item in x_test.iterrows():\n",
    "    if index % 1000 == 0:\n",
    "        print(index)\n",
    "    base_path = \"/home/zhaoqi/input/{0}_{1}\"\n",
    "    file_path = base_path.format(item[\"mw_file_hash\"], item[\"mw_file_size\"])\n",
    "    features = get_ember_feature(file_path)\n",
    "    ebt_X[index, 0:len(features)] = features\n",
    "    ebt_y[index] = y_test[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model.predict(ebt_X)\n",
    "y_pred_e = np.zeros((len(y_p), 1))\n",
    "for i in range(len(y_p)):\n",
    "    y_pred_e[i, 0] = y_p[i]\n",
    "\n",
    "estimate_model(y_pred_e, ebt_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mal Conv + Ember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model_f = Model(c_model.input, c_model.layers[-2].output)\n",
    "\n",
    "train_generator = DataGenerator(range(len(x_train)), x_train, y_train, 16, max_length, False)\n",
    "malcon_train_x = model_f.predict_generator(generator=train_generator, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)\n",
    "\n",
    "test_generator = DataGenerator(range(len(x_test)), x_test, y_test, 16, max_length, False)\n",
    "malcon_test_x = model_f.predict_generator(generator=test_generator, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge mal conv and ember *train* data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(malcon_train_x)\n",
    "ebm_X = np.zeros((num, 2351+128), dtype=float)\n",
    "ebm_y = np.zeros(num, dtype=float)\n",
    "\n",
    "for index in range(num):\n",
    "    ebm_X[index, 0:2351] = eb_X[index]\n",
    "    ebm_X[index, 2351:2351+128] = malcon_train_x[index]\n",
    "    ebm_y[index] = eb_y[index]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "model_m = get_model(ebm_X, ebm_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge mal conv and ember *test* data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(malcon_test_x)\n",
    "ebm_X_t = np.zeros((num, 2351+128), dtype=float)\n",
    "ebm_y_t = np.zeros(num, dtype=float)\n",
    "\n",
    "for index in range(num):\n",
    "    ebm_X_t[index, 0:2351] = ebt_X[index]\n",
    "    ebm_X_t[index, 2351:2351+128] = malcon_test_x[index]\n",
    "    ebm_y_t[index] = ebt_y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model_m.predict(ebm_X_t)\n",
    "y_pred = np.zeros((len(y_p), 1))\n",
    "for i in range(len(y_p)):\n",
    "    y_pred[i, 0] = y_p[i]\n",
    "\n",
    "estimate_model(y_pred, ebm_y_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
