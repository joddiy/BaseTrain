{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# specify which GPU will be used\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pymysql\n",
    "from warnings import filterwarnings\n",
    "\n",
    "_connection = None\n",
    "\n",
    "def get_connection(db_config):\n",
    "    \"\"\"\n",
    "    get db connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        _connection = pymysql.connect(host=db_config['host'], user=db_config['username'],\n",
    "                                      password=db_config['password'],\n",
    "                                      db=db_config['db'], charset=\"utf8\")\n",
    "        filterwarnings('ignore', category=pymysql.Warning)\n",
    "\n",
    "    return _connection\n",
    "\n",
    "\n",
    "def close():\n",
    "    \"\"\"\n",
    "    close DB connection\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global _connection\n",
    "    if _connection is not None:\n",
    "        _connection.close()\n",
    "    _connection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {\n",
    "    'host': '172.26.187.242',\n",
    "    'username': 'malware_r',\n",
    "    'password': 'GEg22v2O7jbfWhb3',\n",
    "    'db': 'malware'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields\n",
    "\n",
    "- mw_file_suffix: file name after hash value\n",
    "- mw_file_prefix: directory\n",
    "- mw_em_f: features of ember, splitted by \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# the base function which can query sql and return dict data\n",
    "def get_specific_data(table_suffix, sql=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    global _connection\n",
    "    if _connection is None:\n",
    "        raise Exception(\"please init db connect first\")\n",
    "\n",
    "    cursor = _connection.cursor()\n",
    "    cursor.execute(\"SET NAMES utf8mb4\")\n",
    "\n",
    "    ret = []\n",
    "        \n",
    "    cursor.execute(sql)\n",
    "\n",
    "    field_names = [i[0] for i in cursor.description]\n",
    "\n",
    "    for row in cursor:\n",
    "        temp = {}\n",
    "        for key in range(len(row)):\n",
    "            temp[field_names[key]] = row[key]\n",
    "        ret.append(temp)\n",
    "     \n",
    "    cursor.close()\n",
    "    # _connection.close()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 6.508746385574341 seconds ---\n",
      "--- 6.439056396484375 seconds ---\n",
      "--- 6.0142199993133545 seconds ---\n",
      "--- 6.165328025817871 seconds ---\n",
      "--- 6.026252031326294 seconds ---\n",
      "--- 5.981155157089233 seconds ---\n",
      "--- 7.792041063308716 seconds ---\n",
      "--- 9.055203437805176 seconds ---\n",
      "--- 7.822920799255371 seconds ---\n",
      "--- 7.520376205444336 seconds ---\n",
      "--- 7.515448570251465 seconds ---\n",
      "--- 8.57888150215149 seconds ---\n",
      "--- 6.345381498336792 seconds ---\n",
      "--- 6.4234113693237305 seconds ---\n",
      "--- 6.526384353637695 seconds ---\n",
      "--- 6.459148168563843 seconds ---\n",
      "101030\n"
     ]
    }
   ],
   "source": [
    "close()\n",
    "res1 = []\n",
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "# Iterate all partitions of databases\n",
    "for suffix in table_suffix:\n",
    "    sql = \"\"\" \n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  a.section_name,\n",
    "  c.mw_file_suffix as mw_file_size,\n",
    "  c.mw_file_prefix as mw_file_directory,\n",
    "  c.mw_num_engines,\n",
    "  a.pointerto_raw_data,\n",
    "  a.virtual_size,\n",
    "  d.mw_em_f\n",
    "from mw_index_2017_section_%s as a\n",
    "  inner join mw_index_2017_%s c on a.mw_file_hash = c.mw_file_hash\n",
    "  inner join mw_index_2017_feature_%s d on a.mw_file_hash = d.mw_file_hash\n",
    "where CNT_CODE = 1 and MEM_EXECUTE = 1 and c.mw_num_engines <> -1 and (c.mw_num_engines > 6 or c.mw_num_engines = 0) and\n",
    "      c.mw_file_prefix in ('201702')\n",
    "group by mw_file_hash\n",
    "    \"\"\" % (suffix, suffix, suffix)\n",
    "    res1.extend(get_specific_data(suffix, sql))\n",
    "close()\n",
    "print(len(res1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 9.55379581451416 seconds ---\n",
      "--- 10.065983772277832 seconds ---\n",
      "--- 8.129196405410767 seconds ---\n",
      "--- 8.099434852600098 seconds ---\n",
      "--- 9.19154167175293 seconds ---\n",
      "--- 9.958981990814209 seconds ---\n",
      "--- 7.7313361167907715 seconds ---\n",
      "--- 8.358144283294678 seconds ---\n",
      "--- 8.372468709945679 seconds ---\n",
      "--- 9.524359464645386 seconds ---\n",
      "--- 7.730841398239136 seconds ---\n",
      "--- 7.8709635734558105 seconds ---\n",
      "--- 9.530184268951416 seconds ---\n",
      "--- 11.541909456253052 seconds ---\n",
      "--- 9.67682147026062 seconds ---\n",
      "--- 8.63088059425354 seconds ---\n",
      "154527\n"
     ]
    }
   ],
   "source": [
    "close()\n",
    "res2 = []\n",
    "get_connection(db)\n",
    "table_suffix = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "# Iterate all partitions of databases\n",
    "for suffix in table_suffix:\n",
    "    sql = \"\"\" \n",
    "select\n",
    "  a.mw_file_hash,\n",
    "  a.section_name,\n",
    "  c.mw_file_suffix as mw_file_size,\n",
    "  c.mw_file_prefix as mw_file_directory,\n",
    "  c.mw_num_engines,\n",
    "  a.pointerto_raw_data,\n",
    "  a.virtual_size,\n",
    "  d.mw_em_f\n",
    "from mw_index_2017_section_%s as a\n",
    "  inner join mw_index_2017_%s c on a.mw_file_hash = c.mw_file_hash\n",
    "  inner join mw_index_2017_feature_%s d on a.mw_file_hash = d.mw_file_hash\n",
    "where CNT_CODE = 1 and MEM_EXECUTE = 1 and c.mw_num_engines <> -1 and (c.mw_num_engines > 6 or c.mw_num_engines = 0) and\n",
    "      c.mw_file_prefix in ('201703')\n",
    "group by mw_file_hash\n",
    "    \"\"\" % (suffix, suffix, suffix)\n",
    "    res2.extend(get_specific_data(suffix, sql))\n",
    "close()\n",
    "print(len(res2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_length = 300000\n",
    "\n",
    "train_data = pd.DataFrame(res1)\n",
    "# train_data = train_data.loc[train_data.virtual_size <= max_length]\n",
    "# train_data = train_data.reset_index(drop=True)\n",
    "train_data.mw_num_engines[train_data.mw_num_engines == 0 ] = 0\n",
    "train_data.mw_num_engines[train_data.mw_num_engines > 6 ] = 1\n",
    "train_label = train_data.mw_num_engines.ravel()\n",
    "\n",
    "test_data = pd.DataFrame(res2)\n",
    "# test_data = test_data.loc[test_data.virtual_size <= max_length]\n",
    "# test_data = test_data.reset_index(drop=True)\n",
    "test_data.mw_num_engines[test_data.mw_num_engines == 0 ] = 0\n",
    "test_data.mw_num_engines[test_data.mw_num_engines > 6 ] = 1\n",
    "test_label = test_data.mw_num_engines.ravel()\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data, train_label, test_size=0.1, random_state=2345)\n",
    "x_test = test_data\n",
    "y_test = test_label\n",
    "\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "x_val = x_val.reset_index(drop=True)\n",
    "x_test = x_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ember_feature(data):\n",
    "    ember_f = np.zeros((len(data.mw_em_f), 2351), dtype=float)\n",
    "    for index, item in data.iterrows():\n",
    "        ember_f[index, :] = item['mw_em_f'].split(';')\n",
    "    return ember_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(x_train, y_train, x_val, y_val):\n",
    "    params = {'application': 'binary'}\n",
    "    lgbm_dataset = lgb.Dataset(x_train, y_train.ravel())\n",
    "    valid_sets = lgb.Dataset(x_val, y_val.ravel())\n",
    "\n",
    "    model = lgb.train(params, lgbm_dataset, 100000, valid_sets=valid_sets, early_stopping_rounds=10)\n",
    "    y_pred = model.predict(x_val)\n",
    "    \n",
    "    loss = log_loss(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    acc = accuracy_score(y_val, (y_pred > 0.5).astype(int))\n",
    "#     model.save_model(file_path + \"-%04d-%.5f-%.5f.h5\" % (model.best_iteration, loss, acc),\n",
    "#                      num_iteration=model.best_iteration)\n",
    "    print(\"val loss : %.5f\" % loss)\n",
    "    print(\"auc score : %.5f\" % auc)\n",
    "    print(\"accuracy score : %.5f\" % acc)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_etrain = get_ember_feature(x_train)\n",
    "x_eval = get_ember_feature(x_val)\n",
    "x_etest = get_ember_feature(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.446362\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.395605\n",
      "[3]\tvalid_0's binary_logloss: 0.356458\n",
      "[4]\tvalid_0's binary_logloss: 0.324558\n",
      "[5]\tvalid_0's binary_logloss: 0.29884\n",
      "[6]\tvalid_0's binary_logloss: 0.276706\n",
      "[7]\tvalid_0's binary_logloss: 0.257758\n",
      "[8]\tvalid_0's binary_logloss: 0.241391\n",
      "[9]\tvalid_0's binary_logloss: 0.227203\n",
      "[10]\tvalid_0's binary_logloss: 0.214519\n",
      "[11]\tvalid_0's binary_logloss: 0.203253\n",
      "[12]\tvalid_0's binary_logloss: 0.193508\n",
      "[13]\tvalid_0's binary_logloss: 0.184491\n",
      "[14]\tvalid_0's binary_logloss: 0.176772\n",
      "[15]\tvalid_0's binary_logloss: 0.169711\n",
      "[16]\tvalid_0's binary_logloss: 0.163527\n",
      "[17]\tvalid_0's binary_logloss: 0.157913\n",
      "[18]\tvalid_0's binary_logloss: 0.152591\n",
      "[19]\tvalid_0's binary_logloss: 0.147744\n",
      "[20]\tvalid_0's binary_logloss: 0.143751\n",
      "[21]\tvalid_0's binary_logloss: 0.139774\n",
      "[22]\tvalid_0's binary_logloss: 0.136592\n",
      "[23]\tvalid_0's binary_logloss: 0.133326\n",
      "[24]\tvalid_0's binary_logloss: 0.130831\n",
      "[25]\tvalid_0's binary_logloss: 0.127937\n",
      "[26]\tvalid_0's binary_logloss: 0.125265\n",
      "[27]\tvalid_0's binary_logloss: 0.122911\n",
      "[28]\tvalid_0's binary_logloss: 0.1206\n",
      "[29]\tvalid_0's binary_logloss: 0.11881\n",
      "[30]\tvalid_0's binary_logloss: 0.117035\n",
      "[31]\tvalid_0's binary_logloss: 0.11524\n",
      "[32]\tvalid_0's binary_logloss: 0.113622\n",
      "[33]\tvalid_0's binary_logloss: 0.112356\n",
      "[34]\tvalid_0's binary_logloss: 0.110853\n",
      "[35]\tvalid_0's binary_logloss: 0.109545\n",
      "[36]\tvalid_0's binary_logloss: 0.108467\n",
      "[37]\tvalid_0's binary_logloss: 0.107617\n",
      "[38]\tvalid_0's binary_logloss: 0.106646\n",
      "[39]\tvalid_0's binary_logloss: 0.105948\n",
      "[40]\tvalid_0's binary_logloss: 0.105053\n",
      "[41]\tvalid_0's binary_logloss: 0.104236\n",
      "[42]\tvalid_0's binary_logloss: 0.103503\n",
      "[43]\tvalid_0's binary_logloss: 0.102865\n",
      "[44]\tvalid_0's binary_logloss: 0.102138\n",
      "[45]\tvalid_0's binary_logloss: 0.101525\n",
      "[46]\tvalid_0's binary_logloss: 0.101117\n",
      "[47]\tvalid_0's binary_logloss: 0.100647\n",
      "[48]\tvalid_0's binary_logloss: 0.10014\n",
      "[49]\tvalid_0's binary_logloss: 0.0997782\n",
      "[50]\tvalid_0's binary_logloss: 0.0991943\n",
      "[51]\tvalid_0's binary_logloss: 0.0989113\n",
      "[52]\tvalid_0's binary_logloss: 0.0985259\n",
      "[53]\tvalid_0's binary_logloss: 0.0977376\n",
      "[54]\tvalid_0's binary_logloss: 0.0973272\n",
      "[55]\tvalid_0's binary_logloss: 0.0967935\n",
      "[56]\tvalid_0's binary_logloss: 0.0964994\n",
      "[57]\tvalid_0's binary_logloss: 0.0962691\n",
      "[58]\tvalid_0's binary_logloss: 0.0957956\n",
      "[59]\tvalid_0's binary_logloss: 0.0954756\n",
      "[60]\tvalid_0's binary_logloss: 0.0951073\n",
      "[61]\tvalid_0's binary_logloss: 0.0947106\n",
      "[62]\tvalid_0's binary_logloss: 0.0946312\n",
      "[63]\tvalid_0's binary_logloss: 0.0943916\n",
      "[64]\tvalid_0's binary_logloss: 0.0941379\n",
      "[65]\tvalid_0's binary_logloss: 0.0939787\n",
      "[66]\tvalid_0's binary_logloss: 0.0937439\n",
      "[67]\tvalid_0's binary_logloss: 0.0936657\n",
      "[68]\tvalid_0's binary_logloss: 0.0934429\n",
      "[69]\tvalid_0's binary_logloss: 0.0932718\n",
      "[70]\tvalid_0's binary_logloss: 0.0931363\n",
      "[71]\tvalid_0's binary_logloss: 0.0928407\n",
      "[72]\tvalid_0's binary_logloss: 0.092732\n",
      "[73]\tvalid_0's binary_logloss: 0.0927543\n",
      "[74]\tvalid_0's binary_logloss: 0.0926749\n",
      "[75]\tvalid_0's binary_logloss: 0.0925586\n",
      "[76]\tvalid_0's binary_logloss: 0.0923172\n",
      "[77]\tvalid_0's binary_logloss: 0.0921386\n",
      "[78]\tvalid_0's binary_logloss: 0.0920147\n",
      "[79]\tvalid_0's binary_logloss: 0.0917936\n",
      "[80]\tvalid_0's binary_logloss: 0.091771\n",
      "[81]\tvalid_0's binary_logloss: 0.0916142\n",
      "[82]\tvalid_0's binary_logloss: 0.0915543\n",
      "[83]\tvalid_0's binary_logloss: 0.0913615\n",
      "[84]\tvalid_0's binary_logloss: 0.0912959\n",
      "[85]\tvalid_0's binary_logloss: 0.0912074\n",
      "[86]\tvalid_0's binary_logloss: 0.091116\n",
      "[87]\tvalid_0's binary_logloss: 0.0910209\n",
      "[88]\tvalid_0's binary_logloss: 0.0908732\n",
      "[89]\tvalid_0's binary_logloss: 0.0908415\n",
      "[90]\tvalid_0's binary_logloss: 0.0906538\n",
      "[91]\tvalid_0's binary_logloss: 0.0905142\n",
      "[92]\tvalid_0's binary_logloss: 0.0904419\n",
      "[93]\tvalid_0's binary_logloss: 0.0903585\n",
      "[94]\tvalid_0's binary_logloss: 0.0901594\n",
      "[95]\tvalid_0's binary_logloss: 0.0899584\n",
      "[96]\tvalid_0's binary_logloss: 0.0899323\n",
      "[97]\tvalid_0's binary_logloss: 0.0898137\n",
      "[98]\tvalid_0's binary_logloss: 0.0896441\n",
      "[99]\tvalid_0's binary_logloss: 0.0895203\n",
      "[100]\tvalid_0's binary_logloss: 0.089436\n",
      "[101]\tvalid_0's binary_logloss: 0.0894285\n",
      "[102]\tvalid_0's binary_logloss: 0.0894619\n",
      "[103]\tvalid_0's binary_logloss: 0.0894056\n",
      "[104]\tvalid_0's binary_logloss: 0.089316\n",
      "[105]\tvalid_0's binary_logloss: 0.0891899\n",
      "[106]\tvalid_0's binary_logloss: 0.089148\n",
      "[107]\tvalid_0's binary_logloss: 0.0890371\n",
      "[108]\tvalid_0's binary_logloss: 0.0889984\n",
      "[109]\tvalid_0's binary_logloss: 0.088896\n",
      "[110]\tvalid_0's binary_logloss: 0.0888903\n",
      "[111]\tvalid_0's binary_logloss: 0.0888057\n",
      "[112]\tvalid_0's binary_logloss: 0.0888195\n",
      "[113]\tvalid_0's binary_logloss: 0.0887536\n",
      "[114]\tvalid_0's binary_logloss: 0.0886854\n",
      "[115]\tvalid_0's binary_logloss: 0.088609\n",
      "[116]\tvalid_0's binary_logloss: 0.0885849\n",
      "[117]\tvalid_0's binary_logloss: 0.0885044\n",
      "[118]\tvalid_0's binary_logloss: 0.0884601\n",
      "[119]\tvalid_0's binary_logloss: 0.08843\n",
      "[120]\tvalid_0's binary_logloss: 0.08831\n",
      "[121]\tvalid_0's binary_logloss: 0.0881774\n",
      "[122]\tvalid_0's binary_logloss: 0.0880759\n",
      "[123]\tvalid_0's binary_logloss: 0.0879446\n",
      "[124]\tvalid_0's binary_logloss: 0.0878831\n",
      "[125]\tvalid_0's binary_logloss: 0.0878763\n",
      "[126]\tvalid_0's binary_logloss: 0.0878967\n",
      "[127]\tvalid_0's binary_logloss: 0.0879141\n",
      "[128]\tvalid_0's binary_logloss: 0.087925\n",
      "[129]\tvalid_0's binary_logloss: 0.0879219\n",
      "[130]\tvalid_0's binary_logloss: 0.0879122\n",
      "[131]\tvalid_0's binary_logloss: 0.0879332\n",
      "[132]\tvalid_0's binary_logloss: 0.0878803\n",
      "[133]\tvalid_0's binary_logloss: 0.0878153\n",
      "[134]\tvalid_0's binary_logloss: 0.0877977\n",
      "[135]\tvalid_0's binary_logloss: 0.0877658\n",
      "[136]\tvalid_0's binary_logloss: 0.0878446\n",
      "[137]\tvalid_0's binary_logloss: 0.087887\n",
      "[138]\tvalid_0's binary_logloss: 0.0878975\n",
      "[139]\tvalid_0's binary_logloss: 0.0878169\n",
      "[140]\tvalid_0's binary_logloss: 0.0878323\n",
      "[141]\tvalid_0's binary_logloss: 0.0876783\n",
      "[142]\tvalid_0's binary_logloss: 0.0876702\n",
      "[143]\tvalid_0's binary_logloss: 0.0877\n",
      "[144]\tvalid_0's binary_logloss: 0.0876505\n",
      "[145]\tvalid_0's binary_logloss: 0.0876084\n",
      "[146]\tvalid_0's binary_logloss: 0.0875712\n",
      "[147]\tvalid_0's binary_logloss: 0.0875504\n",
      "[148]\tvalid_0's binary_logloss: 0.0874824\n",
      "[149]\tvalid_0's binary_logloss: 0.0875174\n",
      "[150]\tvalid_0's binary_logloss: 0.0874809\n",
      "[151]\tvalid_0's binary_logloss: 0.0874442\n",
      "[152]\tvalid_0's binary_logloss: 0.087489\n",
      "[153]\tvalid_0's binary_logloss: 0.0875182\n",
      "[154]\tvalid_0's binary_logloss: 0.0874085\n",
      "[155]\tvalid_0's binary_logloss: 0.0873519\n",
      "[156]\tvalid_0's binary_logloss: 0.087326\n",
      "[157]\tvalid_0's binary_logloss: 0.0873554\n",
      "[158]\tvalid_0's binary_logloss: 0.0873214\n",
      "[159]\tvalid_0's binary_logloss: 0.0873626\n",
      "[160]\tvalid_0's binary_logloss: 0.087402\n",
      "[161]\tvalid_0's binary_logloss: 0.08728\n",
      "[162]\tvalid_0's binary_logloss: 0.0872468\n",
      "[163]\tvalid_0's binary_logloss: 0.0873369\n",
      "[164]\tvalid_0's binary_logloss: 0.0873607\n",
      "[165]\tvalid_0's binary_logloss: 0.0874144\n",
      "[166]\tvalid_0's binary_logloss: 0.0874651\n",
      "[167]\tvalid_0's binary_logloss: 0.0874292\n",
      "[168]\tvalid_0's binary_logloss: 0.0873928\n",
      "[169]\tvalid_0's binary_logloss: 0.0874194\n",
      "[170]\tvalid_0's binary_logloss: 0.0874236\n",
      "[171]\tvalid_0's binary_logloss: 0.0872963\n",
      "[172]\tvalid_0's binary_logloss: 0.0873143\n",
      "Early stopping, best iteration is:\n",
      "[162]\tvalid_0's binary_logloss: 0.0872468\n",
      "val loss : 0.08725\n",
      "auc score : 0.99064\n",
      "accuracy score : 0.97060\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = get_model(x_etrain, y_train, x_eval, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model(y_pred, test_y):\n",
    "    \n",
    "    loss = log_loss(test_y, y_pred)\n",
    "    auc = roc_auc_score(test_y, y_pred)\n",
    "    acc = accuracy_score(test_y, (y_pred > 0.5).astype(int))\n",
    "    print(\"loss : %.5f\" % loss)\n",
    "    print(\"auc score : %.5f\" % auc)\n",
    "    print(\"accuracy score : %.5f\" % acc)\n",
    "\n",
    "    fp_np_index = np.where(test_y == 0)\n",
    "    fp_np = y_pred[fp_np_index].shape[0]\n",
    "    thre_index = int(np.ceil(fp_np - fp_np * 0.001))\n",
    "\n",
    "    sorted_pred_prob = np.sort(y_pred[fp_np_index], axis=0)\n",
    "    thre = sorted_pred_prob[thre_index]\n",
    "    if thre == 1:\n",
    "        thre = max(sorted_pred_prob[np.where(sorted_pred_prob != 1)])\n",
    "\n",
    "    y_pred_prob = np.vstack((y_pred.transpose(), (1 - y_pred).transpose())).transpose()\n",
    "    y_pred_prob[:, 1] = thre\n",
    "    y_pred_label = np.argmin(y_pred_prob, axis=-1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(test_y, y_pred_label).ravel()\n",
    "    fp_rate = fp / (fp + tn)\n",
    "    recall_rate = tp / (tp + fn)\n",
    "\n",
    "    print(\"thre: %.10f\"%  thre)\n",
    "    print(\"fp:  %.10f\"%  fp_rate)\n",
    "    print(\"recall:  %.10f\"%  recall_rate)\n",
    "    \n",
    "    return auc, loss, recall_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.20370\n",
      "auc score : 0.97656\n",
      "accuracy score : 0.92784\n",
      "thre: 0.9292815104\n",
      "fp:  0.0009850342\n",
      "recall:  0.3704709311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9765633857954636, 0.20369945163005979, 0.3704709311244056)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p = model.predict(x_etest)\n",
    "y_pred_e = np.zeros((len(y_p), 1))\n",
    "for i in range(len(y_p)):\n",
    "    y_pred_e[i, 0] = y_p[i]\n",
    "\n",
    "estimate_model(y_pred_e, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "\n",
    "    def __init__(self, list_IDs, datasets, labels, batch_size=32, dim=8192, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.datasets = datasets\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size, self.dim), dtype=float)\n",
    "        y = np.zeros(self.batch_size, dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            base_path = \"/ssd/2017/{0}/{1}{2}\"\n",
    "            item = self.datasets.loc[ID]\n",
    "            file_path = base_path.format(item[\"mw_file_directory\"], item[\"mw_file_hash\"], item[\"mw_file_size\"])\n",
    "            in_file = open(file_path, 'rb')\n",
    "            in_file.seek(item['pointerto_raw_data'])\n",
    "            if item['virtual_size'] > max_length:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(max_length)]\n",
    "            else:\n",
    "                bytes_data = [int(single_byte) for single_byte in in_file.read(item['virtual_size'])]\n",
    "            X[i, 0:len(bytes_data)] = bytes_data\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras import Input\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, Conv1D, Multiply, GlobalMaxPooling1D, Dropout\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TMalConv(object):\n",
    "    \"\"\"\n",
    "    train of mal conv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.max_len = max_length\n",
    "        self.history = None\n",
    "        self.model = None\n",
    "        self.p_md5 = None\n",
    "        self.time = time.time()\n",
    "        self.summary = {\n",
    "            'time':time.time(),\n",
    "            'batch_size': 32,\n",
    "            'epochs': 64,\n",
    "            'g_c_filter': 128,\n",
    "            'g_c_kernel_size': 500,\n",
    "            'g_c_stride': 500,\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        \n",
    "    def get_p(self, key):\n",
    "        \"\"\"\n",
    "        get the parameter from the summary\n",
    "        :param key:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.summary[key]\n",
    "\n",
    "    def gate_cnn(self, gate_cnn_input):\n",
    "        \"\"\"\n",
    "        construct a gated cnn by the specific kernel size\n",
    "        :param gate_cnn_input:\n",
    "        :param kernel_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        conv1_out = Conv1D(self.get_p(\"g_c_filter\"), self.get_p(\"g_c_kernel_size\"), strides=self.get_p(\"g_c_stride\"))(\n",
    "            gate_cnn_input)\n",
    "        conv2_out = Conv1D(self.get_p(\"g_c_filter\"), self.get_p(\"g_c_kernel_size\"), strides=self.get_p(\"g_c_stride\"),\n",
    "                           activation=\"sigmoid\")(gate_cnn_input)\n",
    "        merged = Multiply()([conv1_out, conv2_out])\n",
    "        gate_cnn_output = GlobalMaxPooling1D()(merged)\n",
    "        return gate_cnn_output\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        get a model\n",
    "        :param max_len:\n",
    "        :param kernel_sizes:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        net_input = Input(shape=(self.max_len,))\n",
    "\n",
    "        embedding_out = Embedding(256, 8, input_length=self.max_len)(net_input)\n",
    "        merged = self.gate_cnn(embedding_out)\n",
    "\n",
    "        dense_out = Dense(128)(merged)\n",
    "        \n",
    "        net_output = Dense(1, activation='sigmoid')(dense_out)\n",
    "\n",
    "        model = keras.models.Model(inputs=net_input, outputs=net_output)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        batch_size = self.get_p(\"batch_size\")\n",
    "        epochs = self.get_p(\"epochs\")\n",
    "\n",
    "        self.model = self.get_model()\n",
    "\n",
    "        print('Length of the train: ', len(x_train))\n",
    "        print('Length of the validation: ', len(x_val))\n",
    "        \n",
    "#         tensor_board = TensorBoard(log_dir='./logs/', batch_size=batch_size)\n",
    "        file_path = \"/home/zhaoqi/BaseTrain/models/\"+ str(self.time) +\"-{epoch:04d}-{val_loss:.5f}-{val_acc:.5f}.h5\"\n",
    "        early_stopping = EarlyStopping(\"val_loss\", patience=3, verbose=0, mode='auto')\n",
    "        check_point = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "        callbacks_list = [check_point, early_stopping]\n",
    "\n",
    "        # Generators\n",
    "        training_generator = DataGenerator(range(len(x_train)), x_train, y_train, batch_size, self.max_len)\n",
    "        validation_generator = DataGenerator(range(len(x_val)), x_val, y_val, batch_size, self.max_len)\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                           optimizer='adam',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        self.model.fit_generator(generator=training_generator,\n",
    "                                 validation_data=validation_generator,\n",
    "                                 use_multiprocessing=True,\n",
    "                                 epochs=epochs,\n",
    "                                 workers=6,\n",
    "                                 callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train:  90927\n",
      "Length of the validation:  10103\n",
      "Epoch 1/64\n",
      "2070/2841 [====================>.........] - ETA: 11:09 - loss: 0.1928 - acc: 0.9362"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.157023). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2346/2841 [=======================>......] - ETA: 6:54 - loss: 0.1892 - acc: 0.9377"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.178676). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2841/2841 [==============================] - 2778s 978ms/step - loss: 0.1845 - acc: 0.9392 - val_loss: 0.1729 - val_acc: 0.9478\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17285, saving model to /home/zhaoqi/BaseTrain/models/1532328542.9507692-0001-0.17285-0.94782.h5\n",
      "Epoch 2/64\n",
      " 561/2841 [====>.........................] - ETA: 37:40 - loss: 0.1337 - acc: 0.9567"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.156290). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 564/2841 [====>.........................] - ETA: 37:33 - loss: 0.1334 - acc: 0.9568"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.152284). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 565/2841 [====>.........................] - ETA: 37:36 - loss: 0.1334 - acc: 0.9568"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.151025). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1274/2841 [============>.................] - ETA: 25:27 - loss: 0.1347 - acc: 0.9562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.149817). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1662/2841 [================>.............] - ETA: 19:01 - loss: 0.1360 - acc: 0.9557"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.162348). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1763/2841 [=================>............] - ETA: 17:18 - loss: 0.1359 - acc: 0.9555"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.161088). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2841/2841 [==============================] - 3057s 1s/step - loss: 0.1397 - acc: 0.9542 - val_loss: 0.1559 - val_acc: 0.9499\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17285 to 0.15592, saving model to /home/zhaoqi/BaseTrain/models/1532328542.9507692-0002-0.15592-0.94990.h5\n",
      "Epoch 3/64\n",
      "1168/2841 [===========>..................] - ETA: 26:31 - loss: 0.1104 - acc: 0.9644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.155001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009/2841 [====================>.........] - ETA: 12:22 - loss: 0.1134 - acc: 0.9629"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.145385). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2841/2841 [==============================] - 2849s 1s/step - loss: 0.1141 - acc: 0.9625 - val_loss: 0.1564 - val_acc: 0.9545\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.15592\n",
      "Epoch 4/64\n",
      " 966/2841 [=========>....................] - ETA: 28:46 - loss: 0.0958 - acc: 0.9723"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.148440). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329/2841 [=============>................] - ETA: 22:36 - loss: 0.0952 - acc: 0.9723"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.192679). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1402/2841 [=============>................] - ETA: 21:15 - loss: 0.0949 - acc: 0.9724"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.182568). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1572/2841 [===============>..............] - ETA: 18:16 - loss: 0.0950 - acc: 0.9724"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.183200). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864/2841 [==================>...........] - ETA: 13:35 - loss: 0.0962 - acc: 0.9721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.175859). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2381/2841 [========================>.....] - ETA: 6:10 - loss: 0.0958 - acc: 0.9721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.167042). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2746/2841 [===========================>..] - ETA: 1:16 - loss: 0.0970 - acc: 0.9718"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.168072). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2841/2841 [==============================] - 2492s 877ms/step - loss: 0.0964 - acc: 0.9720 - val_loss: 0.1775 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.15592\n",
      "Epoch 5/64\n",
      " 579/2841 [=====>........................] - ETA: 25:36 - loss: 0.0846 - acc: 0.9760"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.155458). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 792/2841 [=======>......................] - ETA: 23:15 - loss: 0.0868 - acc: 0.9749"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.174961). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2696/2841 [===========================>..] - ETA: 1:37 - loss: 0.0894 - acc: 0.9745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.159532). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2788/2841 [============================>.] - ETA: 35s - loss: 0.0893 - acc: 0.9745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.162166). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2841 [============================>.] - ETA: 18s - loss: 0.0892 - acc: 0.9745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoqi/anaconda3/envs/tf/lib/python3.5/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.168777). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2841/2841 [==============================] - 2117s 745ms/step - loss: 0.0894 - acc: 0.9745 - val_loss: 0.1560 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.15592\n"
     ]
    }
   ],
   "source": [
    "t_instance = TMalConv()\n",
    "t_instance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4828/4828 [==============================] - 4339s 899ms/step\n"
     ]
    }
   ],
   "source": [
    "model_dir = '/home/zhaoqi/BaseTrain/models/'\n",
    "f_name = '1532328542.9507692-0002-0.15592-0.94990.h5'\n",
    "c_model = load_model(model_dir + f_name)\n",
    "\n",
    "test_generator = DataGenerator(range(len(x_test)), x_test, y_test, 32, max_length, False)\n",
    "y_pred = c_model.predict_generator(generator=test_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.29392\n",
      "auc score : 0.96058\n",
      "accuracy score : 0.91147\n",
      "thre: 0.9427035451\n",
      "fp:  0.0008173228\n",
      "recall:  0.2806996011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9605778832185878, 0.2939165563363736, 0.2806996011046333)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_model(y_pred, y_test[0:len(y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Malconv and Ember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2841/2841 [==============================] - 2625s 924ms/step\n",
      "315/315 [==============================] - 299s 948ms/step\n",
      " 778/4828 [===>..........................] - ETA: 1:05:20"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model_f = Model(c_model.input, c_model.layers[-2].output)\n",
    "\n",
    "train_generator = DataGenerator(range(len(x_train)), x_train, y_train, 32, max_length, False)\n",
    "malcon_train_x = model_f.predict_generator(generator=train_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)\n",
    "\n",
    "val_generator = DataGenerator(range(len(x_val)), x_val, y_val, 32, max_length, False)\n",
    "malcon_val_x = model_f.predict_generator(generator=val_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)\n",
    "\n",
    "test_generator = DataGenerator(range(len(x_test)), x_test, y_test, 32, max_length, False)\n",
    "malcon_test_x = model_f.predict_generator(generator=test_generator, max_queue_size=10, workers=6, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m = get_model(malcon_train_x, y_train[0:len(malcon_train_x)], malcon_val_x, y_val[0:len(malcon_val_x)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model_m.predict(malcon_test_x)\n",
    "y_pred = np.zeros((len(y_p), 1))\n",
    "for i in range(len(y_p)):\n",
    "    y_pred[i, 0] = y_p[i]\n",
    "\n",
    "estimate_model(y_pred, y_test[0:len(malcon_test_x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_feature(m_data, e_data):\n",
    "    num = len(m_data)\n",
    "    m_x = np.zeros((num, 128+2351), dtype=float)\n",
    "    \n",
    "    for index in range(num):\n",
    "        m_x[index, 0:128] = m_data[index]\n",
    "        m_x[index, 128:128+2351] = e_data[index]  \n",
    "    return m_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_train_x = merge_feature(malcon_train_x, x_etrain)\n",
    "merge_val_x = merge_feature(malcon_val_x, x_eval)\n",
    "merge_test_x = merge_feature(malcon_test_x, x_etest)\n",
    "\n",
    "model_m = get_model(merge_train_x, y_train[0:len(merge_train_x)], merge_val_x, y_val[0:len(merge_val_x)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model_m.predict(merge_test_x)\n",
    "y_pred = np.zeros((len(y_p), 1))\n",
    "for i in range(len(y_p)):\n",
    "    y_pred[i, 0] = y_p[i]\n",
    "\n",
    "estimate_model(y_pred, y_test[0:len(merge_test_x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2469</th>\n",
       "      <th>2470</th>\n",
       "      <th>2471</th>\n",
       "      <th>2472</th>\n",
       "      <th>2473</th>\n",
       "      <th>2474</th>\n",
       "      <th>2475</th>\n",
       "      <th>2476</th>\n",
       "      <th>2477</th>\n",
       "      <th>2478</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.705183</td>\n",
       "      <td>1.479086</td>\n",
       "      <td>-1.532787</td>\n",
       "      <td>1.188892</td>\n",
       "      <td>-1.221726</td>\n",
       "      <td>1.624470</td>\n",
       "      <td>-0.073439</td>\n",
       "      <td>-1.389202</td>\n",
       "      <td>-0.050010</td>\n",
       "      <td>1.532788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.385240</td>\n",
       "      <td>-1.328830</td>\n",
       "      <td>1.294801</td>\n",
       "      <td>-0.842054</td>\n",
       "      <td>1.094820</td>\n",
       "      <td>-1.422021</td>\n",
       "      <td>0.374867</td>\n",
       "      <td>1.221696</td>\n",
       "      <td>-0.134544</td>\n",
       "      <td>-1.071150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.394131</td>\n",
       "      <td>0.870116</td>\n",
       "      <td>-1.014822</td>\n",
       "      <td>0.788210</td>\n",
       "      <td>-0.986084</td>\n",
       "      <td>1.494880</td>\n",
       "      <td>-0.084611</td>\n",
       "      <td>-1.264098</td>\n",
       "      <td>-0.649978</td>\n",
       "      <td>1.641255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.286791</td>\n",
       "      <td>-0.833753</td>\n",
       "      <td>0.517326</td>\n",
       "      <td>-0.506641</td>\n",
       "      <td>0.325388</td>\n",
       "      <td>-0.763379</td>\n",
       "      <td>0.083963</td>\n",
       "      <td>0.569139</td>\n",
       "      <td>0.049757</td>\n",
       "      <td>-0.532283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.473867</td>\n",
       "      <td>0.949333</td>\n",
       "      <td>-1.038878</td>\n",
       "      <td>0.761097</td>\n",
       "      <td>-0.942002</td>\n",
       "      <td>1.515890</td>\n",
       "      <td>-0.161014</td>\n",
       "      <td>-1.152947</td>\n",
       "      <td>0.230228</td>\n",
       "      <td>1.190743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2479 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.705183  1.479086 -1.532787  1.188892 -1.221726  1.624470 -0.073439   \n",
       "1 -0.385240 -1.328830  1.294801 -0.842054  1.094820 -1.422021  0.374867   \n",
       "2  0.394131  0.870116 -1.014822  0.788210 -0.986084  1.494880 -0.084611   \n",
       "3 -0.286791 -0.833753  0.517326 -0.506641  0.325388 -0.763379  0.083963   \n",
       "4  0.473867  0.949333 -1.038878  0.761097 -0.942002  1.515890 -0.161014   \n",
       "\n",
       "       7         8         9     ...   2469  2470  2471  2472  2473  2474  \\\n",
       "0 -1.389202 -0.050010  1.532788  ...    0.0   0.0   1.0   0.0   1.0   0.0   \n",
       "1  1.221696 -0.134544 -1.071150  ...    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2 -1.264098 -0.649978  1.641255  ...    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3  0.569139  0.049757 -0.532283  ...    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4 -1.152947  0.230228  1.190743  ...    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   2475  2476  2477  2478  \n",
       "0  -1.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   1.0  \n",
       "4   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 2479 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame(merge_train_x)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(merge_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.606546\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.535902\n",
      "[3]\tvalid_0's binary_logloss: 0.476737\n",
      "[4]\tvalid_0's binary_logloss: 0.426738\n",
      "[5]\tvalid_0's binary_logloss: 0.384202\n",
      "[6]\tvalid_0's binary_logloss: 0.347401\n",
      "[7]\tvalid_0's binary_logloss: 0.315518\n",
      "[8]\tvalid_0's binary_logloss: 0.288056\n",
      "[9]\tvalid_0's binary_logloss: 0.263826\n",
      "[10]\tvalid_0's binary_logloss: 0.242805\n",
      "[11]\tvalid_0's binary_logloss: 0.224185\n",
      "[12]\tvalid_0's binary_logloss: 0.207686\n",
      "[13]\tvalid_0's binary_logloss: 0.193076\n",
      "[14]\tvalid_0's binary_logloss: 0.18006\n",
      "[15]\tvalid_0's binary_logloss: 0.168389\n",
      "[16]\tvalid_0's binary_logloss: 0.157996\n",
      "[17]\tvalid_0's binary_logloss: 0.148482\n",
      "[18]\tvalid_0's binary_logloss: 0.140271\n",
      "[19]\tvalid_0's binary_logloss: 0.132753\n",
      "[20]\tvalid_0's binary_logloss: 0.126112\n",
      "[21]\tvalid_0's binary_logloss: 0.120228\n",
      "[22]\tvalid_0's binary_logloss: 0.11487\n",
      "[23]\tvalid_0's binary_logloss: 0.110088\n",
      "[24]\tvalid_0's binary_logloss: 0.105356\n",
      "[25]\tvalid_0's binary_logloss: 0.101314\n",
      "[26]\tvalid_0's binary_logloss: 0.0976633\n",
      "[27]\tvalid_0's binary_logloss: 0.0943403\n",
      "[28]\tvalid_0's binary_logloss: 0.0913071\n",
      "[29]\tvalid_0's binary_logloss: 0.0884764\n",
      "[30]\tvalid_0's binary_logloss: 0.0859406\n",
      "[31]\tvalid_0's binary_logloss: 0.0836728\n",
      "[32]\tvalid_0's binary_logloss: 0.0815169\n",
      "[33]\tvalid_0's binary_logloss: 0.0797125\n",
      "[34]\tvalid_0's binary_logloss: 0.0779086\n",
      "[35]\tvalid_0's binary_logloss: 0.0761945\n",
      "[36]\tvalid_0's binary_logloss: 0.0747354\n",
      "[37]\tvalid_0's binary_logloss: 0.0733653\n",
      "[38]\tvalid_0's binary_logloss: 0.072017\n",
      "[39]\tvalid_0's binary_logloss: 0.0707748\n",
      "[40]\tvalid_0's binary_logloss: 0.0697863\n",
      "[41]\tvalid_0's binary_logloss: 0.0689857\n",
      "[42]\tvalid_0's binary_logloss: 0.067992\n",
      "[43]\tvalid_0's binary_logloss: 0.0670066\n",
      "[44]\tvalid_0's binary_logloss: 0.0661015\n",
      "[45]\tvalid_0's binary_logloss: 0.0652356\n",
      "[46]\tvalid_0's binary_logloss: 0.0644067\n",
      "[47]\tvalid_0's binary_logloss: 0.0636017\n",
      "[48]\tvalid_0's binary_logloss: 0.0628674\n",
      "[49]\tvalid_0's binary_logloss: 0.0620853\n",
      "[50]\tvalid_0's binary_logloss: 0.0614709\n",
      "[51]\tvalid_0's binary_logloss: 0.0610755\n",
      "[52]\tvalid_0's binary_logloss: 0.0606494\n",
      "[53]\tvalid_0's binary_logloss: 0.0601412\n",
      "[54]\tvalid_0's binary_logloss: 0.0596569\n",
      "[55]\tvalid_0's binary_logloss: 0.0591058\n",
      "[56]\tvalid_0's binary_logloss: 0.0586088\n",
      "[57]\tvalid_0's binary_logloss: 0.0582595\n",
      "[58]\tvalid_0's binary_logloss: 0.0579456\n",
      "[59]\tvalid_0's binary_logloss: 0.05756\n",
      "[60]\tvalid_0's binary_logloss: 0.0570082\n",
      "[61]\tvalid_0's binary_logloss: 0.0568255\n",
      "[62]\tvalid_0's binary_logloss: 0.0564749\n",
      "[63]\tvalid_0's binary_logloss: 0.056106\n",
      "[64]\tvalid_0's binary_logloss: 0.0558353\n",
      "[65]\tvalid_0's binary_logloss: 0.0557153\n",
      "[66]\tvalid_0's binary_logloss: 0.0555062\n",
      "[67]\tvalid_0's binary_logloss: 0.0553156\n",
      "[68]\tvalid_0's binary_logloss: 0.0549379\n",
      "[69]\tvalid_0's binary_logloss: 0.0547802\n",
      "[70]\tvalid_0's binary_logloss: 0.0546313\n",
      "[71]\tvalid_0's binary_logloss: 0.0543152\n",
      "[72]\tvalid_0's binary_logloss: 0.0540336\n",
      "[73]\tvalid_0's binary_logloss: 0.0538064\n",
      "[74]\tvalid_0's binary_logloss: 0.0534591\n",
      "[75]\tvalid_0's binary_logloss: 0.0533271\n",
      "[76]\tvalid_0's binary_logloss: 0.0532259\n",
      "[77]\tvalid_0's binary_logloss: 0.0530977\n",
      "[78]\tvalid_0's binary_logloss: 0.0528924\n",
      "[79]\tvalid_0's binary_logloss: 0.0525713\n",
      "[80]\tvalid_0's binary_logloss: 0.0523743\n",
      "[81]\tvalid_0's binary_logloss: 0.0521686\n",
      "[82]\tvalid_0's binary_logloss: 0.0520333\n",
      "[83]\tvalid_0's binary_logloss: 0.0519668\n",
      "[84]\tvalid_0's binary_logloss: 0.0518466\n",
      "[85]\tvalid_0's binary_logloss: 0.0517005\n",
      "[86]\tvalid_0's binary_logloss: 0.0515647\n",
      "[87]\tvalid_0's binary_logloss: 0.0513687\n",
      "[88]\tvalid_0's binary_logloss: 0.051299\n",
      "[89]\tvalid_0's binary_logloss: 0.0510803\n",
      "[90]\tvalid_0's binary_logloss: 0.0507977\n",
      "[91]\tvalid_0's binary_logloss: 0.0507291\n",
      "[92]\tvalid_0's binary_logloss: 0.0506334\n",
      "[93]\tvalid_0's binary_logloss: 0.0505004\n",
      "[94]\tvalid_0's binary_logloss: 0.0503293\n",
      "[95]\tvalid_0's binary_logloss: 0.0501937\n",
      "[96]\tvalid_0's binary_logloss: 0.0501188\n",
      "[97]\tvalid_0's binary_logloss: 0.0500768\n",
      "[98]\tvalid_0's binary_logloss: 0.0499936\n",
      "[99]\tvalid_0's binary_logloss: 0.0498052\n",
      "[100]\tvalid_0's binary_logloss: 0.0496775\n",
      "[101]\tvalid_0's binary_logloss: 0.0495321\n",
      "[102]\tvalid_0's binary_logloss: 0.0494674\n",
      "[103]\tvalid_0's binary_logloss: 0.0493654\n",
      "[104]\tvalid_0's binary_logloss: 0.0492804\n",
      "[105]\tvalid_0's binary_logloss: 0.0492292\n",
      "[106]\tvalid_0's binary_logloss: 0.0491256\n",
      "[107]\tvalid_0's binary_logloss: 0.0489611\n",
      "[108]\tvalid_0's binary_logloss: 0.048822\n",
      "[109]\tvalid_0's binary_logloss: 0.0488296\n",
      "[110]\tvalid_0's binary_logloss: 0.0486414\n",
      "[111]\tvalid_0's binary_logloss: 0.0486226\n",
      "[112]\tvalid_0's binary_logloss: 0.048532\n",
      "[113]\tvalid_0's binary_logloss: 0.0485001\n",
      "[114]\tvalid_0's binary_logloss: 0.0483726\n",
      "[115]\tvalid_0's binary_logloss: 0.048309\n",
      "[116]\tvalid_0's binary_logloss: 0.0481979\n",
      "[117]\tvalid_0's binary_logloss: 0.0481646\n",
      "[118]\tvalid_0's binary_logloss: 0.0480551\n",
      "[119]\tvalid_0's binary_logloss: 0.0481086\n",
      "[120]\tvalid_0's binary_logloss: 0.0481883\n",
      "[121]\tvalid_0's binary_logloss: 0.0481176\n",
      "[122]\tvalid_0's binary_logloss: 0.0480806\n",
      "[123]\tvalid_0's binary_logloss: 0.0479731\n",
      "[124]\tvalid_0's binary_logloss: 0.0479422\n",
      "[125]\tvalid_0's binary_logloss: 0.0478627\n",
      "[126]\tvalid_0's binary_logloss: 0.0477884\n",
      "[127]\tvalid_0's binary_logloss: 0.0476951\n",
      "[128]\tvalid_0's binary_logloss: 0.0476097\n",
      "[129]\tvalid_0's binary_logloss: 0.0475873\n",
      "[130]\tvalid_0's binary_logloss: 0.0475648\n",
      "[131]\tvalid_0's binary_logloss: 0.0475117\n",
      "[132]\tvalid_0's binary_logloss: 0.0474792\n",
      "[133]\tvalid_0's binary_logloss: 0.0474449\n",
      "[134]\tvalid_0's binary_logloss: 0.0474426\n",
      "[135]\tvalid_0's binary_logloss: 0.0474997\n",
      "[136]\tvalid_0's binary_logloss: 0.0474711\n",
      "[137]\tvalid_0's binary_logloss: 0.0474653\n",
      "[138]\tvalid_0's binary_logloss: 0.0473843\n",
      "[139]\tvalid_0's binary_logloss: 0.0473856\n",
      "[140]\tvalid_0's binary_logloss: 0.04737\n",
      "[141]\tvalid_0's binary_logloss: 0.0473614\n",
      "[142]\tvalid_0's binary_logloss: 0.0473722\n",
      "[143]\tvalid_0's binary_logloss: 0.0473082\n",
      "[144]\tvalid_0's binary_logloss: 0.0473042\n",
      "[145]\tvalid_0's binary_logloss: 0.047267\n",
      "[146]\tvalid_0's binary_logloss: 0.0471414\n",
      "[147]\tvalid_0's binary_logloss: 0.0471675\n",
      "[148]\tvalid_0's binary_logloss: 0.0471626\n",
      "[149]\tvalid_0's binary_logloss: 0.0471192\n",
      "[150]\tvalid_0's binary_logloss: 0.0471023\n",
      "[151]\tvalid_0's binary_logloss: 0.0470659\n",
      "[152]\tvalid_0's binary_logloss: 0.0469801\n",
      "[153]\tvalid_0's binary_logloss: 0.0469054\n",
      "[154]\tvalid_0's binary_logloss: 0.0469373\n",
      "[155]\tvalid_0's binary_logloss: 0.0468748\n",
      "[156]\tvalid_0's binary_logloss: 0.0469162\n",
      "[157]\tvalid_0's binary_logloss: 0.046858\n",
      "[158]\tvalid_0's binary_logloss: 0.0467513\n",
      "[159]\tvalid_0's binary_logloss: 0.0466986\n",
      "[160]\tvalid_0's binary_logloss: 0.0467195\n",
      "[161]\tvalid_0's binary_logloss: 0.0467283\n",
      "[162]\tvalid_0's binary_logloss: 0.0467153\n",
      "[163]\tvalid_0's binary_logloss: 0.0467289\n",
      "[164]\tvalid_0's binary_logloss: 0.0466918\n",
      "[165]\tvalid_0's binary_logloss: 0.0466654\n",
      "[166]\tvalid_0's binary_logloss: 0.0466729\n",
      "[167]\tvalid_0's binary_logloss: 0.0466831\n",
      "[168]\tvalid_0's binary_logloss: 0.0467371\n",
      "[169]\tvalid_0's binary_logloss: 0.0467615\n",
      "[170]\tvalid_0's binary_logloss: 0.0467216\n",
      "[171]\tvalid_0's binary_logloss: 0.0466341\n",
      "[172]\tvalid_0's binary_logloss: 0.0466575\n",
      "[173]\tvalid_0's binary_logloss: 0.046592\n",
      "[174]\tvalid_0's binary_logloss: 0.0465562\n",
      "[175]\tvalid_0's binary_logloss: 0.0465309\n",
      "[176]\tvalid_0's binary_logloss: 0.0465212\n",
      "[177]\tvalid_0's binary_logloss: 0.0465304\n",
      "[178]\tvalid_0's binary_logloss: 0.0465145\n",
      "[179]\tvalid_0's binary_logloss: 0.0465426\n",
      "[180]\tvalid_0's binary_logloss: 0.0465521\n",
      "[181]\tvalid_0's binary_logloss: 0.0464793\n",
      "[182]\tvalid_0's binary_logloss: 0.0465143\n",
      "[183]\tvalid_0's binary_logloss: 0.0465408\n",
      "[184]\tvalid_0's binary_logloss: 0.046521\n",
      "[185]\tvalid_0's binary_logloss: 0.046432\n",
      "[186]\tvalid_0's binary_logloss: 0.0463955\n",
      "[187]\tvalid_0's binary_logloss: 0.0463876\n",
      "[188]\tvalid_0's binary_logloss: 0.0464796\n",
      "[189]\tvalid_0's binary_logloss: 0.0464142\n",
      "[190]\tvalid_0's binary_logloss: 0.0463601\n",
      "[191]\tvalid_0's binary_logloss: 0.046312\n",
      "[192]\tvalid_0's binary_logloss: 0.046327\n",
      "[193]\tvalid_0's binary_logloss: 0.046315\n",
      "[194]\tvalid_0's binary_logloss: 0.0462683\n",
      "[195]\tvalid_0's binary_logloss: 0.046312\n",
      "[196]\tvalid_0's binary_logloss: 0.0462236\n",
      "[197]\tvalid_0's binary_logloss: 0.0462153\n",
      "[198]\tvalid_0's binary_logloss: 0.0462395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199]\tvalid_0's binary_logloss: 0.0462829\n",
      "[200]\tvalid_0's binary_logloss: 0.0462199\n",
      "[201]\tvalid_0's binary_logloss: 0.0462193\n",
      "[202]\tvalid_0's binary_logloss: 0.0461826\n",
      "[203]\tvalid_0's binary_logloss: 0.0461578\n",
      "[204]\tvalid_0's binary_logloss: 0.0461028\n",
      "[205]\tvalid_0's binary_logloss: 0.046082\n",
      "[206]\tvalid_0's binary_logloss: 0.0460718\n",
      "[207]\tvalid_0's binary_logloss: 0.0461296\n",
      "[208]\tvalid_0's binary_logloss: 0.0461759\n",
      "[209]\tvalid_0's binary_logloss: 0.0462082\n",
      "[210]\tvalid_0's binary_logloss: 0.0462406\n",
      "[211]\tvalid_0's binary_logloss: 0.0462647\n",
      "[212]\tvalid_0's binary_logloss: 0.0462464\n",
      "[213]\tvalid_0's binary_logloss: 0.0461945\n",
      "[214]\tvalid_0's binary_logloss: 0.0461967\n",
      "[215]\tvalid_0's binary_logloss: 0.0461552\n",
      "[216]\tvalid_0's binary_logloss: 0.0461221\n",
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's binary_logloss: 0.0460718\n",
      "val loss : 0.04607\n",
      "auc score : 0.99867\n",
      "accuracy score : 0.98304\n"
     ]
    }
   ],
   "source": [
    "s_merge_train_x = scaler.transform(merge_train_x)\n",
    "s_merge_val_x = scaler.transform(merge_val_x)\n",
    "s_merge_test_x = scaler.transform(merge_test_x)\n",
    "\n",
    "model_m = get_model(s_merge_train_x, y_train[0:len(s_merge_train_x)], s_merge_val_x, y_val[0:len(s_merge_val_x)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.04807\n",
      "auc score : 0.99855\n",
      "accuracy score : 0.98192\n",
      "thre: 0.9962235704\n",
      "fp:  0.0008132561\n",
      "recall:  0.8711143241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.998552209126074, 0.048069058625024294, 0.8711143240731178)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p = model_m.predict(s_merge_test_x)\n",
    "y_pred = np.zeros((len(y_p), 1))\n",
    "for i in range(len(y_p)):\n",
    "    y_pred[i, 0] = y_p[i]\n",
    "\n",
    "estimate_model(y_pred, y_test[0:len(s_merge_test_x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
